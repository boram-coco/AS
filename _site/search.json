[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AS",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\nAS HW4_5(2)\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW4\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW4_5\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n편의추정 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n로지스틱 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n다항회귀 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\nGLS 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n변수변환 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW3_3(야구 투수)\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW3\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n변수선택 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n회귀진단 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\n가변수 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW2\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\nMLR실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW1_\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nalpha\n\n\n\n\n\n\n\nalpha\n\n\npvalue\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\nSLR실습_simulation\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\nSLR실습\n\n\n\n\n\n\n\nApplied statistics\n\n\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n\nSLR(Simple Linear Regression)\n\n\n\n\n\n\n\nApplied statistics\n\n\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nAS HW1\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\n김보람\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "posts/AS2.html",
    "href": "posts/AS2.html",
    "title": "AS HW2",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "posts/AS2.html#section",
    "href": "posts/AS2.html#section",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n이 데이터의 산점도 행렬을 그리시오.\n\ndt &lt;- read_csv('dt.csv')\nhead(dt)\n\nRows: 400 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (8): Sales, CompPrice, Income, Advertising, Population, Price, Age, Educ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nA tibble: 6 × 8\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nAge\nEducation\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n9.50\n138\n73\n11\n276\n120\n42\n17\n\n\n11.22\n111\n48\n16\n260\n83\n65\n10\n\n\n10.06\n113\n35\n10\n269\n80\n59\n12\n\n\n7.40\n117\n100\n4\n466\n97\n55\n14\n\n\n4.15\n141\n64\n3\n340\n128\n38\n13\n\n\n10.81\n124\n113\n13\n501\n72\n78\n16\n\n\n\n\n\n\npairs(dt, pch=16, col='darkorange')\n\n\n\n\n\n선형관계가 있어보이는 데이터는, “sales와 price”,“compprice와 porice”,\n\n\npairs(dt[,which(names(dt) %in% \n                      c('Sales', 'CompPrice', 'Price'))], \n      pch=16, col='darkorange')\n\n\n\n\n\ncor(dt[,which(names(dt) %in% \n                      c('Sales', 'CompPrice', 'Price'))])\n\n\nA matrix: 3 × 3 of type dbl\n\n\n\nSales\nCompPrice\nPrice\n\n\n\n\nSales\n1.00000000\n0.06407873\n-0.4449507\n\n\nCompPrice\n0.06407873\n1.00000000\n0.5848478\n\n\nPrice\n-0.44495073\n0.58484777\n1.0000000\n\n\n\n\n\n\n2,3번 문제에서 축소모형 하니까.. 1번 문제에서는 걍 전체로 돌리자"
  },
  {
    "objectID": "posts/AS2.html#section-1",
    "href": "posts/AS2.html#section-1",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\nSales를 예측하기 위한 중회귀분석을 하려고 한다. 이를 위한 모형을 설정하시오.\n\nfit_dt&lt;-lm(Sales~., data=dt)\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  &lt; 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  &lt; 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  &lt; 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\widehat {Sales} = 7.7076934 + 0.0939149 \\widehat {CompPrice} + 0.012871 \\widehat {Income} +0.1308637 \\widehat {Advertising} -0.0001239 \\widehat {Population} -0.0925226 \\widehat {Price} -0.0449743 \\widehat {Age} -0.0399844 \\widehat {Education}\\)\n\nfit__&lt;-lm(Sales~CompPrice+Price, data=dt)\nsummary(fit__)\n\n\nCall:\nlm(formula = Sales ~ CompPrice + Price, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.5285 -1.6207 -0.2404  1.5269  6.2437 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.278692   0.932774   6.731 5.91e-11 ***\nCompPrice    0.090777   0.009132   9.941  &lt; 2e-16 ***\nPrice       -0.087458   0.005914 -14.788  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.269 on 397 degrees of freedom\nMultiple R-squared:  0.3578,    Adjusted R-squared:  0.3546 \nF-statistic: 110.6 on 2 and 397 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\widehat {Sales} = 6.278692 + 0.090777 \\widehat {CompPrice} -0.087458 \\widehat {Price}\\)"
  },
  {
    "objectID": "posts/AS2.html#section-2",
    "href": "posts/AS2.html#section-2",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n최소제곱법의 의한 회귀직선을 적합시키시키고, 모형 적합 결과를 설명하시오."
  },
  {
    "objectID": "posts/AS2.html#section-3",
    "href": "posts/AS2.html#section-3",
    "title": "AS HW2",
    "section": "(4)",
    "text": "(4)\n회귀직선의 유의성 검정을 위한 가설을 설정하고, 분산분석표를 이용하여 가설 검정을 수행하시오.\n\\(H_0:\\beta_0=\\dots=\\beta_7=0\\) vs. \\(H_1:not H_0\\)\n\nanova(fit_dt)\n\n\nA anova: 8 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nCompPrice\n1\n13.0666859\n13.0666859\n3.5117751\n6.167778e-02\n\n\nIncome\n1\n79.0733616\n79.0733616\n21.2515906\n5.458487e-06\n\n\nAdvertising\n1\n219.3512681\n219.3512681\n58.9523862\n1.300900e-13\n\n\nPopulation\n1\n0.3824026\n0.3824026\n0.1027737\n7.486970e-01\n\n\nPrice\n1\n1198.8668836\n1198.8668836\n322.2049460\n5.144277e-53\n\n\nAge\n1\n208.6564283\n208.6564283\n56.0780635\n4.652175e-13\n\n\nEducation\n1\n4.3158913\n4.3158913\n1.1599299\n2.821424e-01\n\n\nResiduals\n392\n1458.5617763\n3.7208209\nNA\nNA\n\n\n\n\n\n\nnull_model &lt;- lm(Sales~1, data=dt)  \nfit_dt &lt;- lm(Sales~., data=dt) \n\nanova(null_model, fit_dt) \n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n399\n3182.275\nNA\nNA\nNA\nNA\n\n\n2\n392\n1458.562\n7\n1723.713\n66.18021\n1.413772e-62\n\n\n\n\n\n\n회귀직선은 유의하다.\n\n\n(1723.713/7)/(1458.562/392)\n\n66.1802021443038"
  },
  {
    "objectID": "posts/AS2.html#section-4",
    "href": "posts/AS2.html#section-4",
    "title": "AS HW2",
    "section": "(5)",
    "text": "(5)\n오차의 분산에 대한 추정량을 구하시오.\n\nmatrix\n\nn = nrow(dt)\nX = cbind(rep(1,n), dt$CompPrice, dt$Income, dt$Advertising, dt$Population, dt$Price, dt$Age, dt$Education)\ny = dt$Sales\n\n\nbeta_hat = solve(t(X)%*%X) %*% t(X) %*% y   # t(X): X^T를 의미함 \nbeta_hat\ncoef(fit_dt)\n\n\nA matrix: 8 × 1 of type dbl\n\n\n7.7076934384\n\n\n0.0939149066\n\n\n0.0128717129\n\n\n0.1308636707\n\n\n-0.0001239252\n\n\n-0.0925226099\n\n\n-0.0449743402\n\n\n-0.0399844437\n\n\n\n\n\n(Intercept)7.70769343844283CompPrice0.0939149066067561Income0.0128717128971186Advertising0.130863670692396Population-0.000123925156795604Price-0.0925226098938757Age-0.0449743402082049Education-0.0399844437382063\n\n\n\ny_hat = X %*% beta_hat\ny_hat[1:5]\nfitted(fit_dt)[1:5]\n\n\n9.34151160644639.809135309058769.510780487498658.440550277355868.05222459125309\n\n\n19.3415116064467129.8091353090589339.5107804874987748.4405502773558958.05222459125317\n\n\n\nsse &lt;- sum((y - y_hat)^2) ##SSE\nsqrt(sse/(n-7-1)) ##RMSE\nsummary(fit_dt)$sigma\n\n1.92894293798154\n\n\n1.92894293798154\n\n\n\nmse &lt;- sse/(n-7-1)\nmse\n\n3.72082085798886"
  },
  {
    "objectID": "posts/AS2.html#section-5",
    "href": "posts/AS2.html#section-5",
    "title": "AS HW2",
    "section": "(6)",
    "text": "(6)\n결정계수와 수정된 결정계수를 구하시오.\n\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  &lt; 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  &lt; 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  &lt; 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(R^2:0.5417, R^2_{adj}: 0.5335\\)"
  },
  {
    "objectID": "posts/AS2.html#section-6",
    "href": "posts/AS2.html#section-6",
    "title": "AS HW2",
    "section": "(7)",
    "text": "(7)\n개별 회귀계수의 유의성검정을 수행하시오.\n\nsummary(fit_dt)$coef\n\n\nA matrix: 8 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n7.7076934384\n1.1176259965\n6.8964873\n2.145154e-11\n\n\nCompPrice\n0.0939149066\n0.0078395225\n11.9796718\n2.153866e-28\n\n\nIncome\n0.0128717129\n0.0034756701\n3.7033759\n2.432641e-04\n\n\nAdvertising\n0.1308636707\n0.0151219066\n8.6539134\n1.302560e-16\n\n\nPopulation\n-0.0001239252\n0.0006877272\n-0.1801952\n8.570924e-01\n\n\nPrice\n-0.0925226099\n0.0050520870\n-18.3137403\n1.409811e-54\n\n\nAge\n-0.0449743402\n0.0060082977\n-7.4853715\n4.751078e-13\n\n\nEducation\n-0.0399844437\n0.0371257460\n-1.0770004\n2.821424e-01"
  },
  {
    "objectID": "posts/AS2.html#section-7",
    "href": "posts/AS2.html#section-7",
    "title": "AS HW2",
    "section": "(8)",
    "text": "(8)\n회귀계수에 대한 90% 신뢰구간을 구하시오.\n\nconfint(fit_dt, level = 0.90)\n\n\nA matrix: 8 × 2 of type dbl\n\n\n\n5 %\n95 %\n\n\n\n\n(Intercept)\n5.865007519\n9.550379358\n\n\nCompPrice\n0.080989493\n0.106840320\n\n\nIncome\n0.007141202\n0.018602224\n\n\nAdvertising\n0.105931426\n0.155795915\n\n\nPopulation\n-0.001257815\n0.001009965\n\n\nPrice\n-0.100852239\n-0.084192981\n\n\nAge\n-0.054880521\n-0.035068159\n\n\nEducation\n-0.101195519\n0.021226632"
  },
  {
    "objectID": "posts/AS2.html#section-8",
    "href": "posts/AS2.html#section-8",
    "title": "AS HW2",
    "section": "(9)",
    "text": "(9)\nCompPrice = 100, Income = 70, Advertising = 20, Population = 300, Price = 80, Education = 12인 지역에 위치한 매장의 평균 판매액을 예측하고, 95% 신뢰구간을 구하시오.\n\nnew_dt &lt;- data.frame(CompPrice=100, Income=70, Advertising=20, Population=300, Price=80, Age=53, Education=12)\n\n\npredict(fit_dt, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  ##평균반응\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n10.31504\n9.746147\n10.88393\n\n\n\n\n\n\n문제에서 Age에 대한 값이 명시되지 않아서.. 일단 age는 평균 값 넣어서 계산함"
  },
  {
    "objectID": "posts/AS2.html#section-9",
    "href": "posts/AS2.html#section-9",
    "title": "AS HW2",
    "section": "(10)",
    "text": "(10)\n위 매장에 대하여 개별 판매액 예측하고, 95% 신뢰구간을 구하시오.\n\npredict(fit_dt, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  ## 개별 y\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n10.31504\n6.480238\n14.14984"
  },
  {
    "objectID": "posts/AS2.html#section-10",
    "href": "posts/AS2.html#section-10",
    "title": "AS HW2",
    "section": "(11)",
    "text": "(11)\n잔차에 대한 산점도를 그리고, 결과를 설명하여라.\n\nyhat &lt;- fitted(fit_dt)\nres &lt;- resid(fit_dt)\n\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n선형성은 없어보이고, 등분산성이 있어보인다."
  },
  {
    "objectID": "posts/AS2.html#section-11",
    "href": "posts/AS2.html#section-11",
    "title": "AS HW2",
    "section": "(12)",
    "text": "(12)\n잔차에 대한 등분산성 검정을 수행하여라.\n\\(H_0\\):등분산 VS. \\(H_1\\):이분산\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\nbptest(fit_dt)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit_dt\nBP = 1.1751, df = 7, p-value = 0.9915\n\n\n\np-valeur가 커서 \\(H_0\\)를 채택한다. 즉 등분산성이다."
  },
  {
    "objectID": "posts/AS2.html#section-12",
    "href": "posts/AS2.html#section-12",
    "title": "AS HW2",
    "section": "(13)",
    "text": "(13)\n잔차에 대한 히스토그램, QQ plot을 그리고, 정규성 검정을 수행하여라.\n\npar(mfrow=c(1,2))\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\nhist(res)\npar(mfrow=c(1,1))\n\n\n\n\n\n정규성을 만족해 보인다.\n\n\n## H0 : normal distribution  vs. H1 : not H0\nshapiro.test(res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.98602, p-value = 0.0006715"
  },
  {
    "objectID": "posts/AS2.html#section-13",
    "href": "posts/AS2.html#section-13",
    "title": "AS HW2",
    "section": "(14)",
    "text": "(14)\n잔차에 대한 독립성 검정을 수행하시오.\n\\(H_0\\):uncorrelated\n\ndwtest(fit_dt, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  fit_dt\nDW = 1.9694, p-value = 0.7622\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n독립이다."
  },
  {
    "objectID": "posts/AS2.html#section-14",
    "href": "posts/AS2.html#section-14",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n위에서 적합한 모형에서 개별 회귀계수의 유의성 검정 결과 유의하지 않은 변수는 무엇인가?\nPopulation, Education은 유의하지 않은 변수이다."
  },
  {
    "objectID": "posts/AS2.html#section-15",
    "href": "posts/AS2.html#section-15",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\n위에서 유의하지 않았던 변수를 제외한 모형을 축소모형(Reduced Model)으로 하는 부분 F검정을 수행하여라. 검정에 필요한 가설을 설정하고, 검정 결과를 설명하시오.\n\nreduced_model&lt;-lm(Sales~.-Population-Education, data=dt)\n\n\nanova(reduced_model, fit_dt)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n394\n1462.897\nNA\nNA\nNA\nNA\n\n\n2\n392\n1458.562\n2\n4.335087\n0.5825445\n0.5589583"
  },
  {
    "objectID": "posts/AS2.html#section-16",
    "href": "posts/AS2.html#section-16",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n1번에서 설정한 모형과, 축소모형 중 어느 모형이 이 데이터에 대한 설명을 잘 하고 있는지를 비교하시오.\n\nsummary(reduced_model)\nsummary(fit_dt)\n\n\nCall:\nlm(formula = Sales ~ . - Population - Education, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9071 -1.3081 -0.1892  1.1495  4.6980 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.109190   0.943940   7.531 3.46e-13 ***\nCompPrice    0.093904   0.007792  12.051  &lt; 2e-16 ***\nIncome       0.013092   0.003465   3.779 0.000182 ***\nAdvertising  0.130611   0.014572   8.963  &lt; 2e-16 ***\nPrice       -0.092543   0.005044 -18.347  &lt; 2e-16 ***\nAge         -0.044971   0.005994  -7.503 4.20e-13 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.927 on 394 degrees of freedom\nMultiple R-squared:  0.5403,    Adjusted R-squared:  0.5345 \nF-statistic: 92.62 on 5 and 394 DF,  p-value: &lt; 2.2e-16\n\n\n\nCall:\nlm(formula = Sales ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0598 -1.3515 -0.1739  1.1331  4.8304 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.7076934  1.1176260   6.896 2.15e-11 ***\nCompPrice    0.0939149  0.0078395  11.980  &lt; 2e-16 ***\nIncome       0.0128717  0.0034757   3.703 0.000243 ***\nAdvertising  0.1308637  0.0151219   8.654  &lt; 2e-16 ***\nPopulation  -0.0001239  0.0006877  -0.180 0.857092    \nPrice       -0.0925226  0.0050521 -18.314  &lt; 2e-16 ***\nAge         -0.0449743  0.0060083  -7.485 4.75e-13 ***\nEducation   -0.0399844  0.0371257  -1.077 0.282142    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.929 on 392 degrees of freedom\nMultiple R-squared:  0.5417,    Adjusted R-squared:  0.5335 \nF-statistic: 66.18 on 7 and 392 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/AS2.html#section-17",
    "href": "posts/AS2.html#section-17",
    "title": "AS HW2",
    "section": "(1)",
    "text": "(1)\n\\(H_0 : CompPrice=Income\\) vs. \\(H_1 : not H_0\\)\n\ninstall.packages(\"car\")\n\nInstalling package into ‘/home/coco/R/x86_64-pc-linux-gnu-library/4.2’\n(as ‘lib’ is unspecified)\n\n\n\nlinearHypothesis(dt_fit, c(0,1,-1,0,0,0,0,0),0)"
  },
  {
    "objectID": "posts/AS2.html#section-18",
    "href": "posts/AS2.html#section-18",
    "title": "AS HW2",
    "section": "(2)",
    "text": "(2)\n\\(H_0 : CompPrice=-Price\\) vs. \\(H_1 : not H_0\\)\nlinearHypothesis(fit_dt, c(0,1,0,0,0,1,0,0),0)"
  },
  {
    "objectID": "posts/AS2.html#section-19",
    "href": "posts/AS2.html#section-19",
    "title": "AS HW2",
    "section": "(3)",
    "text": "(3)\n\\(H_0\\)를 기각할 수 있는 제약조건을 만들어 보시오.(단 2개 이상의 변수 사용)"
  },
  {
    "objectID": "posts/09. 변수변환.html",
    "href": "posts/09. 변수변환.html",
    "title": "09. 변수변환 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/09. 변수변환.html#설명변수-변환",
    "href": "posts/09. 변수변환.html#설명변수-변환",
    "title": "09. 변수변환 실습",
    "section": "설명변수 변환",
    "text": "설명변수 변환\n\nwind &lt;- read.csv(\"wind.csv\")\nhead(wind)\n\n\nA data.frame: 6 × 3\n\n\n\ni\nx\ny\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n5.0\n1.582\n\n\n2\n2\n6.0\n1.822\n\n\n3\n3\n3.4\n1.057\n\n\n4\n4\n2.7\n0.500\n\n\n5\n5\n10.0\n2.236\n\n\n6\n6\n9.7\n2.386\n\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(y~x, wind, pch=16, main=\"scatter plot\") \nabline(wind_fit, col='darkorange', lwd=2)\n\nplot(fitted(wind_fit), rstandard(wind_fit),  pch = 16,\n     xlab = expression(hat(y)), \n     ylab = \"Standardized Residuals\",  \n     main = \"Fitted vs. Residuals\")\nabline(h = 0, col = \"grey\", lwd = 2, lty=2)\n\n\n\n\n\n산점도를 봤을때 직선보다는 곡선이 어울려 보이는 산점도\n표준화된잔차를 봐도 선형성(2차)이 보인다.\n\n\nlm1\n\\(y=\\beta_0+\\beta_1x+\\epsilon\\)\n\nwind_fit &lt;- lm(y~x, wind) \nsummary(wind_fit)\n\n\nCall:\nlm(formula = y ~ x, data = wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59869 -0.14099  0.06059  0.17262  0.32184 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.13088    0.12599   1.039     0.31    \nx            0.24115    0.01905  12.659 7.55e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.2361 on 23 degrees of freedom\nMultiple R-squared:  0.8745,    Adjusted R-squared:  0.869 \nF-statistic: 160.3 on 1 and 23 DF,  p-value: 7.546e-12\n\n\n\nprint(paste0(\"coefficient of determination : \", round(summary(wind_fit)$r.squared,3)))\n\n[1] \"coefficient of determination : 0.874\"\n\n\n\n단순선형회귀\n\n\nprint(paste0(\"RMSE : \", round(summary(wind_fit)$sigma,3)))\n\n[1] \"RMSE : 0.236\"\n\n\n\n\nlm2\n\\(y=\\beta_0+\\beta_1x+\\beta_2x^2+\\epsilon\\)\n\nwind_fit_2 &lt;- lm(y~x+I(x^2), wind) \nsummary(wind_fit_2)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26347 -0.02537  0.01264  0.03908  0.19903 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.155898   0.174650  -6.618 1.18e-06 ***\nx            0.722936   0.061425  11.769 5.77e-11 ***\nI(x^2)      -0.038121   0.004797  -7.947 6.59e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1227 on 22 degrees of freedom\nMultiple R-squared:  0.9676,    Adjusted R-squared:  0.9646 \nF-statistic: 328.3 on 2 and 22 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(x^2\\)을 넣었더니 모형도 유의하고 \\(R^2\\)의 값도 커짐\n두개 변수 모두 유의함\n\n\npar(mfrow=c(1,2))\nplot(y~x, wind, pch=16, main=\"scatter plot\")\nx_new &lt;- sort(wind$x)\nlines(x_new, predict(wind_fit_2, newdata = data.frame(x=x_new)), col='darkorange', lwd=2)\n\nplot(fitted(wind_fit_2), rstandard(wind_fit_2),  pch = 16,\n      xlab = expression(hat(y)), \n      ylab = \"Standardized Residuals\",  \n      main = \"Fitted vs. Residuals\")\nabline(h = 0, col = \"grey\", lwd = 2, lty=2)\n\n\n\n\n\nfitted line이 데이터를 잘 훑는 거 같이 보인다.\n\n\nprint(paste0(\"coefficient of determination : \", round(summary(wind_fit_2)$r.squared,3)))\n\n[1] \"coefficient of determination : 0.968\"\n\n\n\nprint(paste0(\"RMSE : \", round(summary(wind_fit_2)$sigma,3)))\n\n[1] \"RMSE : 0.123\"\n\n\n\n\nlm3\n\\(y=\\beta_0+\\beta_1 \\dfrac{1}{x}+\\epsilon=\\beta_0+\\beta_1 x^`+\\epsilon\\)\n\nwind_fit_3 &lt;- lm(y~I(1/x), wind) \nsummary(wind_fit_3)\n\n\nCall:\nlm(formula = y ~ I(1/x), data = wind)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20547 -0.04940  0.01100  0.08352  0.12204 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.9789     0.0449   66.34   &lt;2e-16 ***\nI(1/x)       -6.9345     0.2064  -33.59   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.09417 on 23 degrees of freedom\nMultiple R-squared:   0.98, Adjusted R-squared:  0.9792 \nF-statistic:  1128 on 1 and 23 DF,  p-value: &lt; 2.2e-16\n\n\n\npar(mfrow=c(1,2))\nplot(y~I(1/x), wind, pch=16, main=\"scatter plot\") \nabline(wind_fit_3, col='darkorange', lwd=2)\n\nplot(fitted(wind_fit_3), rstandard(wind_fit_3),  pch = 16,\n     xlab = expression(hat(y)), \n     ylab = \"Standardized Residuals\",  \n     main = \"Fitted vs. Residuals\")\nabline(h = 0, col = \"grey\", lwd = 2, lty=2)\n\n\n\n\n\nprint(paste0(\"coefficient of determination : \", round(summary(wind_fit_3)$r.squared,3)))\n\n[1] \"coefficient of determination : 0.98\"\n\n\n\nprint(paste0(\"RMSE : \", round(summary(wind_fit_3)$sigma,3)))\n\n[1] \"RMSE : 0.094\""
  },
  {
    "objectID": "posts/09. 변수변환.html#반응변수-변환",
    "href": "posts/09. 변수변환.html#반응변수-변환",
    "title": "09. 변수변환 실습",
    "section": "반응변수 변환",
    "text": "반응변수 변환\n\ninitech &lt;- read.csv(\"initech.csv\")\nhead(initech)\n\n\nA data.frame: 6 × 2\n\n\n\nyears\nsalary\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n1\n41504\n\n\n2\n1\n32619\n\n\n3\n1\n44322\n\n\n4\n2\n40038\n\n\n5\n2\n46147\n\n\n6\n2\n38447\n\n\n\n\n\n\npar(mfrow=c(1,1))\nplot(salary~years, initech,  pch=16, cex=1.5)\n\n\n\n\n\n곡선 관계!?\nx가 증가하면서 y값이 점점 퍼지고 있다. x가 증가할수록 salary의 분산이 커지고 있다.\n\n\nLM1\n\\(y=\\beta_0+\\beta_x+\\epsilon\\)\n\ninitech_fit &lt;- lm(salary~years, initech) \nsummary(initech_fit)\n\n\nCall:\nlm(formula = salary ~ years, data = initech)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57225 -18104    241  15589  91332 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5302       5750   0.922    0.359    \nyears           8637        389  22.200   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 27360 on 98 degrees of freedom\nMultiple R-squared:  0.8341,    Adjusted R-squared:  0.8324 \nF-statistic: 492.8 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(salary~years, initech,\n        pch=16, cex=1.5)\nabline(initech_fit, col='darkorange', lwd=2)\n\n\n\n\n\npar(mfrow=c(1,2))\n\nplot(fitted(initech_fit), rstandard(initech_fit),  pch = 16,\n     xlab = expression(hat(y)), \n     ylab = \"Standardized Residuals\",  \n     main = \"Fitted versus Residuals\")\nabline(h = 0, col = \"grey\", lwd = 2, lty=2)\n\nqqnorm(resid(initech_fit), pch=16,\n          main = \"Normal Q-Q Plot\", col = \"darkgrey\") \nqqline(resid(initech_fit), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\n잔차게 넓게 퍼져있음\n\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\ndwtest(initech_fit, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  initech_fit\nDW = 1.3313, p-value = 0.0004993\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n등분산성 만족하지 않는다.\n변수변환(log y),WLSE -&gt; GLS 를 한다.\n\n\nhist(resid(initech_fit))\n\n\n\n\n\n\nLM2(반응변수 변환)\n\ninitech_fit_log = lm(log(salary) ~ years, data = initech) \nsummary(initech_fit_log)\n\n\nCall:\nlm(formula = log(salary) ~ years, data = initech)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.57022 -0.13560  0.03048  0.14157  0.41366 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.48381    0.04108  255.18   &lt;2e-16 ***\nyears        0.07888    0.00278   28.38   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1955 on 98 degrees of freedom\nMultiple R-squared:  0.8915,    Adjusted R-squared:  0.8904 \nF-statistic: 805.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n로그변환을 한 모형이므로 RMSE가 작다고 엇 작네 하면 안돼 .y를 예측하는 것과 y`을 예측하는것은 다르다.\n\\(log \\hat y = \\hat y^` = \\hat \\beta_0 + \\hat \\beta_1 x_1\\)\n\\(\\rightarrow \\hat y = exp(\\hat y^`)\\) 역변환 해줘야함\n\n\npar(mfrow=c(1,2))\nplot(log(salary)~years, initech,\n        pch=16, cex=1.5)\nabline(initech_fit_log, col='darkorange', lwd=2)\n\nplot(salary~years, initech,pch=16, cex=1.5)\nlines(initech$years,exp(fitted(initech_fit_log)), col=\"darkorange\", lwd=2)\n\n\n\n\n\n왼쪽 그림: \\(\\hat y^` = \\hat {log y}\\)\n오른쪽 그림: \\(\\hat y = exp(\\hat \\beta_0 + \\hat \\beta_1x)\\)\n\n\npar(mfrow=c(1,2))\n\nplot(fitted(initech_fit_log), rstandard(initech_fit_log),  pch = 16,\n     xlab = expression(hat(y)), \n     ylab = \"Standardized Residuals\",  \n     main = \"Fitted versus Residuals\")\nabline(h = 0, col = \"grey\", lwd = 2, lty=2)\n\nqqnorm(resid(initech_fit_log), pch=16,\n          main = \"Normal Q-Q Plot\", col = \"darkgrey\")\nqqline(resid(initech_fit_log), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\n\n정리\n\\(y=\\beta_0+\\beta_x+\\epsilon\\)\n\nsummary(initech_fit)$sigma\n\n27355.0944284452\n\n\n\\(log y = \\beta_0+\\beta_1x+\\epsilon\\)\n\nsummary(initech_fit_log)$sigma\n\n0.195455087632113\n\n\n\n값 작게 나왔다고 더 좋은 모형인거 아님! 변환을 다시 해야함.\n\n\\(\\sqrt{MSE}=\\hat \\sigma\\) -LM1\n\nsqrt(sum((initech$salary - fitted(initech_fit)) ^ 2)/98) \n\n27355.0944284452\n\n\n\\(\\sqrt{MSE}=\\hat \\sigma\\) -LM2\n\nsqrt(sum((initech$salary - exp(fitted(initech_fit_log))) ^ 2)/98)\n\n24526.871988033\n\n\n\\(y-(exp(\\hat{log y})=\\hat y)\\)\n\nMSE를 보면 조오금 줄어들었다."
  },
  {
    "objectID": "posts/09. 변수변환.html#boxcox변환",
    "href": "posts/09. 변수변환.html#boxcox변환",
    "title": "09. 변수변환 실습",
    "section": "boxcox변환",
    "text": "boxcox변환\n\\(y'=log y\\)\nlog변환은 앞쪽은 작게 압축시키고 뒤쪽은 확 압축시킴\nlog변환과 다르게 덜 압축하는 방법??? sqrt로 압축..\n지수변환 \\(\\lambda\\)파라미터를 이용해서 변환.. (일반화..)\n\\[y'=g_{\\lambda}(y)= \\begin{cases} \\dfrac{y^{\\lambda}-1}{\\lambda} & \\lambda \\neq 0 \\\\ log y & \\lambda=0 \\end{cases}\\]\n\nlibrary(MASS)\nboxcox(initech_fit, plotit = TRUE)\n\n\n\n\n\n(-2,2)가 기본으로 보여줌\n0을 좌우로 점선은 \\(\\lambda\\)의 95% 신뢰구간\n\n\nboxcox(initech_fit, plotit = TRUE, lambda = seq(-0.5,0.5, by = 0.1))\n\n\n\n\n\nlog-Likeilihood를 이용하여 가장 큰 \\(\\lambda\\)를 찾아준다.\n\\(\\lambda\\)를 결정해주는 함수-&gt;boxcox\n가장 큰 \\(\\hat \\lambda\\) 은 0.08… 근데 의미가 없음.. 신뢰구간에 0이 들어가있네? 0은 로그변환이니까 0도 괜찮겠다!"
  },
  {
    "objectID": "posts/05. 가변수 실습.html",
    "href": "posts/05. 가변수 실습.html",
    "title": "05. 가변수 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/05. 가변수 실습.html#example",
    "href": "posts/05. 가변수 실습.html#example",
    "title": "05. 가변수 실습",
    "section": "Example",
    "text": "Example\n\ndt &lt;- data.frame(\n  y = c(17,26,21,30,22,1,12,19,4,16,\n        28,15,11,38,31,21,20,13,30,14),\n  x1 = c(151,92,175,31,104,277,210,120,290,238,\n         164,272,295,68,85,224,166,305,124,246),\n  x2 = rep(c('M','F'), each=10)\n)\n\n\nhead(dt)\n\n\nA data.frame: 6 × 3\n\n\n\ny\nx1\nx2\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\n1\n17\n151\nM\n\n\n2\n26\n92\nM\n\n\n3\n21\n175\nM\n\n\n4\n30\n31\nM\n\n\n5\n22\n104\nM\n\n\n6\n1\n277\nM\n\n\n\n\n\n\n모든 데이터 퉁으로\n\nmodel_1 &lt;- lm(y~x1, dt)\nsummary(model_1)\n\n\nCall:\nlm(formula = y ~ x1, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.579 -4.737  0.721  4.224  7.936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.40361    2.78580  13.068 1.26e-10 ***\nx1          -0.09323    0.01396  -6.677 2.91e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.124 on 18 degrees of freedom\nMultiple R-squared:  0.7124,    Adjusted R-squared:  0.6964 \nF-statistic: 44.58 on 1 and 18 DF,  p-value: 2.906e-06\n\n\n\n모형은 유의하다.\nMSE=5.124\n\n\nggplot(dt, aes(x1, y)) + \n  geom_point() + \n  geom_abline(slope = coef(model_1)[2], \n              intercept = coef(model_1)[1], col= 'darkorange')+\n  theme_bw()\n\n\n\n\n\nM,F상관없이 모든 데이터 퉁으로!!\n\\(\\widehat y = 36.40361-0.09323 x_1\\)\n시험성적 1점 올라갈때마다 시간이 0.0932 감소한다.\n\n\n\n성별 M,F\n\nmodel_2 : \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\epsilon\\)\n\\(x_2=0 \\ if F, x_2=1 \\ if M\\)\n\\(E(y|F) : \\beta_0 + \\beta_1x_1\\) 여자가 기준\n\\(E(y|M) : \\beta_0+\\beta_1x_1 + \\beta_2 = (\\beta_0+\\beta_2)+\\beta_1x_1\\)\n\\(\\beta_2 = E(y|M)-E(y|F) = \\beta_0 + \\beta_2 + \\beta_1x_1 - {\\beta_0+\\beta_1x_1}\\)\n즉, \\(\\beta_2\\)는 시험성적이 동일할 때 여자와 남자의 소요시간의 평균의 차이\n\n\ncontrasts(factor(dt$x2))\n\n\nA matrix: 2 × 1 of type dbl\n\n\n\nM\n\n\n\n\nF\n0\n\n\nM\n1\n\n\n\n\n\n\n\\(x_2\\)를 factor로 인식했을 때 무엇이 0이고 무엇이 1인지 알려준다.\n\\(H_0: \\beta_1=\\beta_2=0\\)\n\n\n################################\n# x2 = factor(rep(c('M','F'), each=10)) 로 입력한 경우 \n#y = b0 + b1x1 + b2x2 \n# x2 = 0,  F\n# x2 = 1,  M\n#E(y|M) : b0 + b1x1 + b2 = (b0 + b2) + b1x1\n#E(y|F) : b0 + b1x1\n\n# x2 = factor(rep(c(0,1), each=10))로 입력한 경우 \n# y = b0 + b1x1 + b2x2 \n# x2 = 0,  M\n# x2 = 1,  F\n#E(y|M) : b0 + b1x1\n#E(y|F) : b0 + b1x1+ b2 = = (b0 + b2) + b1x1\n\n\nmodel_2 &lt;- lm(y~x1+x2, dt)\nsummary(model_2)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.768865   1.948930  21.432 9.64e-14 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx2M         -7.933953   1.414702  -5.608 3.13e-05 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\np-value가 유의하다.\nmodel1보다 \\(R^2\\)값이 많이 올랐다.\nmodel1보다 MSE보다 감소했다.\nx2M: x2가 남자 그룹에 있는 계수, F=0이고 M=1이라는 것을 알려준다.\n\\(\\beta_2=-7.933953\\) 값이 나오는데 강의록에는 F=1,M=0이여서 강의록과는 부호가 바뀐것.\n\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(model_2)[2], \n              intercept = coef(model_2)[1], col= 'darkorange')+\n  geom_abline(slope = coef(model_2)[2], \n              intercept = coef(model_2)[1]+coef(model_2)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"여자\", \"남자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\\(H_0:\\beta_2=0 \\ vs. \\ H_1:\\beta_2 \\neq 0\\)\n\n\nsummary(model_2)$coefficients\n\n\nA matrix: 3 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n41.7688646\n1.948929636\n21.431694\n9.640000e-14\n\n\nx1\n-0.1009177\n0.008620641\n-11.706522\n1.468240e-09\n\n\nx2M\n-7.9339526\n1.414702366\n-5.608213\n3.134533e-05\n\n\n\n\n\n\n개별회귀계수에 대한 유의성검정 , 유의확률 값이 작으므로 \\(\\beta_2=0\\)이라고 할 수 있다.\nPr(&gt;|t|)은 양측검정에 대한 유의확률 값이다.\n\\(H_0:\\beta_2=0 \\ vs. \\ H_1:\\beta_2 &lt; 0\\)\n이 때의 유의확률? -&gt; 위의 표와 t-value는 똑같다. -5.608213\n\\(t=\\dfrac{\\widehat \\beta_2}{\\widehat{s.e}(\\widehat \\beta_2)}\\)\nH_1:\\beta_2 &lt; 0 단측 검정에 대한 유의확률 값은 Pr(&gt;|t|)/2\nt-value의 -5.608213 값을 제곱하면 아래 표의 F값 31.45206 이 나온다. Pr(&gt;F)값은 똑같음\n\n\n-5.608213^2\n\n-31.452053053369\n\n\n\n자유도가 1개일때 t-value와 F검정의 값은 동일하다\n\n\nanova(model_1, model_2)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n18\n472.5913\nNA\nNA\nNA\nNA\n\n\n2\n17\n165.8145\n1\n306.7768\n31.45206\n3.134533e-05\n\n\n\n\n\n\nRM: model_1\nFM: model_2\n472.5913 : \\(SSE_{RM}\\)\n165.8145 : \\(SSE_{FM}\\)\n\n\n\n교호작용\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\epsilon\\)\n\\(x_2 = 0 if F, x_2 = 1 if M\\)\n\\(E(y|F): \\beta_0 + \\beta_1 x_1\\)\n\\(E(y|M): \\beta_0 + \\beta_1x_1 + \\beta_2 + \\beta_3 x_1 = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)x_1\\)\n\nmodel_3 &lt;- lm(y~x1*x2, dt) #교호작용 보고 싶을 떈 x1*x2 곱하기\n# 혹은 lm(y~x1+x2+x1:x2,dt)\nsummary(model_3)\n\n\nCall:\nlm(formula = y ~ x1 * x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0463 -1.7591 -0.6232  1.9311  6.1102 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.969620   2.635580  15.924 3.11e-11 ***\nx1          -0.101948   0.012474  -8.173 4.20e-07 ***\nx2M         -8.313516   3.541379  -2.348   0.0321 *  \nx1:x2M       0.002089   0.017766   0.118   0.9078    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.218 on 16 degrees of freedom\nMultiple R-squared:  0.8992,    Adjusted R-squared:  0.8803 \nF-statistic: 47.56 on 3 and 16 DF,  p-value: 3.405e-08\n\n\n\n모형 자첸는 유의하고,\nmodel2에 비하면 \\(R^2\\)가 더 감소했다.\n\\(H_0: \\beta_3=0 \\ vs. \\ H_1:\\beta_3 \\neq 0\\)에서 \\(H_0\\)기각 못함\n\n\n## y = b0 + b1x1 + b2x2 + b3x1x2\n## M : x2=0 =&gt; E(y|M) = b0+b1x1\n## F : x2=1 =&gt; E(y|F) = b0 + b1x1 + b2 + b3x1 \n##                    = (b0+b2) + (b1+b3)x1\n\nggplot(dt, aes(x1, y, col=x2)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(model_3)[2], \n              intercept = coef(model_3)[1], col= 'darkorange')+\n  geom_abline(slope = coef(model_3)[2]+coef(model_3)[4], \n              intercept = coef(model_3)[1]+coef(model_3)[3], col= 'steelblue')+\n  guides(col=guide_legend(title=\"성별\")) +\n  scale_color_manual(labels = c(\"여자\", \"남자\"), values = c(\"darkorange\", \"steelblue\"))\n\n\n\n\n\n\\(H_0: \\beta_3=0 \\ vs. \\ H_1:\\beta_3 \\neq 0\\)\n\n\nsummary(model_3)$coefficients\n\n\nA matrix: 4 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n41.96961960\n2.63558045\n15.9242415\n3.106803e-11\n\n\nx1\n-0.10194777\n0.01247420\n-8.1726893\n4.198832e-07\n\n\nx2M\n-8.31351564\n3.54137909\n-2.3475362\n3.209176e-02\n\n\nx1:x2M\n0.00208933\n0.01776597\n0.1176029\n9.078460e-01\n\n\n\n\n\n\nanova(model_2, model_3)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n17\n165.8145\nNA\nNA\nNA\nNA\n\n\n2\n16\n165.6713\n1\n0.1432067\n0.01383045\n0.907846\n\n\n\n\n\n\\(H_0: \\beta_2=\\beta_3=0 \\ vs. \\ H_1: not H_0\\) 에서\nRM: model_1 (x1), FM: model_3 (x1*x2) 아래표 보자\n\nanova(model_1, model_3)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n18\n472.5913\nNA\nNA\nNA\nNA\n\n\n2\n16\n165.6713\n2\n306.92\n14.82068\n0.0002280824\n\n\n\n\n\n472.5913=SSE_RM\n165.6713=SSE_FM\n2=3-1\n\n\n가변수가 아닌 one-hot encodeing\n\ndt2 &lt;- data.frame(y=dt$y,\n                  x1=dt$x1,\n                  x2=as.numeric(dt$x2=='M'),\n                  x3=as.numeric(dt$x2=='F'))\nhead(dt2)\n                                    \n\n\nA data.frame: 6 × 4\n\n\n\ny\nx1\nx2\nx3\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n17\n151\n1\n0\n\n\n2\n26\n92\n1\n0\n\n\n3\n21\n175\n1\n0\n\n\n4\n30\n31\n1\n0\n\n\n5\n22\n104\n1\n0\n\n\n6\n1\n277\n1\n0\n\n\n\n\n\n\nmodel_4 &lt;-lm(y~., dt2)\nsummary(model_4)\n\n\nCall:\nlm(formula = y ~ ., data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 41.768865   1.948930  21.432 9.64e-14 ***\nx1          -0.100918   0.008621 -11.707 1.47e-09 ***\nx2          -7.933953   1.414702  -5.608 3.13e-05 ***\nx3                 NA         NA      NA       NA    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.8991,    Adjusted R-squared:  0.8872 \nF-statistic: 75.72 on 2 and 17 DF,  p-value: 3.42e-09\n\n\n\n\\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\epsilon\\)\nfull Rank가 아니여서 구할 수 없다..\n1 = x2(M)+x3(F) 가 되서 full rank가 안되는데 이중 하나를 날리면 된다.\n아래 model_5는 절편이 없는 모델을 만들어서 돌려보자\n\n\nmodel_5 &lt;- lm(y~0+x1+x2+x3,dt2)\nsummary(model_5)\n\n\nCall:\nlm(formula = y ~ 0 + x1 + x2 + x3, data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0165 -1.7450 -0.6055  1.8803  6.1835 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nx1 -0.100918   0.008621  -11.71 1.47e-09 ***\nx2 33.834912   1.758659   19.24 5.64e-13 ***\nx3 41.768865   1.948930   21.43 9.64e-14 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.123 on 17 degrees of freedom\nMultiple R-squared:  0.982, Adjusted R-squared:  0.9788 \nF-statistic:   309 on 3 and 17 DF,  p-value: 5.047e-15\n\n\n\nmodle5 : \\(y=\\beta_1x_1+\\beta_2x_2 + \\beta_3x_3 + \\epsilon\\)\n\\(x_2 = 1 if M, x_2=0 if F\\)\n\\(x_3 = 0 if M, x_3=1 if F\\)\n\\(E(y|M) = \\beta_1x_1 + \\beta_2\\) - (*)\n\\(E(y|F) = \\beta_1 + \\beta_3\\)\n기울기는 동일한데, 절편이 다르다. 잎에서 쓴 모형과 비교해 본다면,\n\\(E(y|M) = (\\beta_0 + \\beta_2) + \\beta_1x_1\\)에서 (*)의 \\(\\beta_2 = \\beta_0+\\beta_2\\)"
  },
  {
    "objectID": "posts/05. 가변수 실습.html#carseats-예시",
    "href": "posts/05. 가변수 실습.html#carseats-예시",
    "title": "05. 가변수 실습",
    "section": "Carseats 예시",
    "text": "Carseats 예시\n\ninstall.packages(\"ISLR\")\n\nInstalling package into ‘/home/coco/R/x86_64-pc-linux-gnu-library/4.2’\n(as ‘lib’ is unspecified)\n\n\n\n\nlibrary(ISLR)\n\n\nhead(Carseats)\ndim(Carseats)\n\n\nA data.frame: 6 × 11\n\n\n\nSales\nCompPrice\nIncome\nAdvertising\nPopulation\nPrice\nShelveLoc\nAge\nEducation\nUrban\nUS\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n\n\n\n\n1\n9.50\n138\n73\n11\n276\n120\nBad\n42\n17\nYes\nYes\n\n\n2\n11.22\n111\n48\n16\n260\n83\nGood\n65\n10\nYes\nYes\n\n\n3\n10.06\n113\n35\n10\n269\n80\nMedium\n59\n12\nYes\nYes\n\n\n4\n7.40\n117\n100\n4\n466\n97\nMedium\n55\n14\nYes\nYes\n\n\n5\n4.15\n141\n64\n3\n340\n128\nBad\n38\n13\nYes\nNo\n\n\n6\n10.81\n124\n113\n13\n501\n72\nBad\n78\n16\nNo\nYes\n\n\n\n\n\n\n40011\n\n\n• Sales : 판매량 (단위: 1,000)\n• Price : 각 지점에서의 카시트 가격\n• ShelveLoc : 진열대의 등급 (Bad, Medium, Good)\n• Urban :도시 여부 (Yes, No)\n• US: 미국 여부 (Yes, No)\n\n판매량을 예측하자.\n\\(y=\\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)\n\\(x_1\\):Price, \\(x_2,x_3\\)는 가변수\n\\(x_2 = 1\\), if ShelveLoc = Good, \\(x_2=0\\), if o.w.\n\\(x_3 = 1\\), if ShelveLoc = Medium, \\(x_3=0\\), if o.w.\n\\(E(y|Bad) = \\beta_0+\\beta_1x_1\\) &lt;- base\n\\(E(y|Med) = \\beta_0 + \\beta_1x_1+ \\beta_3 = (\\beta_0+\\beta_3)+\\beta_1x_1\\)\n\\(E(y|Good) = \\beta_0 + \\beta_1x_1 + \\beta_2 = (\\beta_0+\\beta_2) + \\beta_1x_1\\)\n\n\nfit &lt;- lm(fit&lt;-lm(Sales~Price+ShelveLoc, \n                  data=Carseats))\nsummary(fit)  \n\n\nCall:\nlm(formula = fit &lt;- lm(Sales ~ Price + ShelveLoc, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8229 -1.3930 -0.0179  1.3868  5.0780 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     12.001802   0.503447  23.839  &lt; 2e-16 ***\nPrice           -0.056698   0.004059 -13.967  &lt; 2e-16 ***\nShelveLocGood    4.895848   0.285921  17.123  &lt; 2e-16 ***\nShelveLocMedium  1.862022   0.234748   7.932 2.23e-14 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.917 on 396 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5391 \nF-statistic: 156.6 on 3 and 396 DF,  p-value: &lt; 2.2e-16\n\n\n\n교호작용은 보지 않겠따.\n\n\ncontrasts(Carseats$ShelveLoc)\n\n\nA matrix: 3 × 2 of type dbl\n\n\n\nGood\nMedium\n\n\n\n\nBad\n0\n0\n\n\nGood\n1\n0\n\n\nMedium\n0\n1\n\n\n\n\n\n\nggplot(Carseats, aes(Price, Sales, col=ShelveLoc)) + \n  geom_point() + \n  theme_bw() + \n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1], col= 'darkorange')+\n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1]+coef(fit)[3], col= 'steelblue')+\n  geom_abline(slope = coef(fit)[2], \n              intercept = coef(fit)[1]+coef(fit)[4], col= 'darkgreen')+\n  guides(col=guide_legend(title=\"ShelveLoc\")) +\n  scale_color_manual(labels = c(\"Bad\", \"Good\", \"Medium\"), \n                     values = c(\"darkorange\", \"steelblue\",\"darkgreen\"))\n\n\n\n\n\n\\(y=\\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 \\epsilon\\)\n\\(x_2 = 1\\), if ShelveLoc = Good, \\(x_2=0\\), if o.w.\n\\(x_3 = 1\\), if ShelveLoc = Medium, \\(x_3=0\\), if o.w.\n\\(x_4 = 1\\), if US=yes, \\(x_4=0\\), if US=no\n\n\ncontrasts(Carseats$US)\n\n\nA matrix: 2 × 1 of type dbl\n\n\n\nYes\n\n\n\n\nNo\n0\n\n\nYes\n1\n\n\n\n\n\n\nfit1 &lt;- lm(fit&lt;-lm(Sales~Price+ShelveLoc+US, \n                  data=Carseats))\nsummary(fit1)  \n\n\nCall:\nlm(formula = fit &lt;- lm(Sales ~ Price + ShelveLoc + US, data = Carseats))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1720 -1.2587 -0.0056  1.2815  4.7462 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     11.476347   0.498083  23.041  &lt; 2e-16 ***\nPrice           -0.057825   0.003938 -14.683  &lt; 2e-16 ***\nShelveLocGood    4.827167   0.277294  17.408  &lt; 2e-16 ***\nShelveLocMedium  1.893360   0.227486   8.323 1.42e-15 ***\nUSYes            1.013071   0.195034   5.194 3.30e-07 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.857 on 395 degrees of freedom\nMultiple R-squared:  0.5718,    Adjusted R-squared:  0.5675 \nF-statistic: 131.9 on 4 and 395 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/05. 가변수 실습.html#구간별-회귀분석",
    "href": "posts/05. 가변수 실습.html#구간별-회귀분석",
    "title": "05. 가변수 실습",
    "section": "구간별 회귀분석",
    "text": "구간별 회귀분석\n\ndt &lt;- data.frame(\n  y = c(377,249,355,475,139,452,440,257),\n  x1 = c(480,720,570,300,800,400,340,650)\n)\n\n\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') + \n  theme_bw()\n\n\n\n\n\n\n\n### threshould = 500\n## x2(x1-xw)=x2(x1-500) = (x1 - 500)+ := x2\n\ndt$x2 = sapply(dt$x1, function(x) max(0, x-500))\n\n\nm &lt;- lm(y ~ x1+x2, dt)\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-22.765  29.765  18.068   4.068 -17.463  20.605 -15.117 -17.160 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 589.5447    60.4213   9.757 0.000192 ***\nx1           -0.3954     0.1492  -2.650 0.045432 *  \nx2           -0.3893     0.2310  -1.685 0.152774    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 24.49 on 5 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.9571 \nF-statistic: 79.06 on 2 and 5 DF,  p-value: 0.0001645\n\n\n\n\ndt2 &lt;- rbind(dt[,2:3], c(500,0))\ndt2$y &lt;- predict(m, newdata = dt2)\n\n\n# this is the predicted line of multiple linear regression\nggplot(data = dt, aes(x = x1, y = y)) + \n  geom_point(color='steelblue') +\n  geom_line(color='darkorange',\n            data = dt2, aes(x=x1, y=y))+\n  geom_vline(xintercept = 500, lty=2, col='red')+\n  theme_bw()"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html",
    "href": "posts/03. CH0304_simulation.html",
    "title": "03. SLR실습_simulation",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#the-function-to-generate-simulation-data",
    "href": "posts/03. CH0304_simulation.html#the-function-to-generate-simulation-data",
    "title": "03. SLR실습_simulation",
    "section": "The function to generate simulation data",
    "text": "The function to generate simulation data\n\ngenerating_slr = function(x, beta0, beta1, sigma) {\n  set.seed(1)\n  n = length(x)\n  epsilon = rnorm(n, mean = 0, sd = sigma)\n  y = beta0 + beta1 * x + epsilon\n  data.frame(x,y)\n}\n\n\npar(mfrow=c(1,2))\nx &lt;- 1:10 \nbeta0 &lt;- 10\nbeta1 &lt;- -3\nsigma &lt;- 3\n\n\nsim_dat &lt;- generating_slr(x, beta0, beta1, sigma)\nsim_fit &lt;- lm(y~x, data = sim_dat)\n\n\nplot(y ~ x, data = sim_dat, \n     pch=16, col='darkorange',\n     main =bquote(hat(beta)[0] ~ \"=\"~.(beta0) ~ \", \" ~\n                    hat(beta)[1] ~ \"=\"~.(beta1)~ \", \" ~\n                    sigma ~\"=\"~ .(sigma)))\nabline(sim_fit, col='steelblue', lwd=2)\nabline(10, -3, col='red', lwd=2)\n\n\n\n\n\nsummary(sim_fit)\n\n\nCall:\nlm(formula = y ~ x, data = sim_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9401 -1.9230  0.7015  0.8036  4.6355 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.4935     1.6581   5.726 0.000441 ***\nx            -2.8358     0.2672 -10.612 5.44e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.427 on 8 degrees of freedom\nMultiple R-squared:  0.9337,    Adjusted R-squared:  0.9254 \nF-statistic: 112.6 on 1 and 8 DF,  p-value: 5.438e-06"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#회귀계수",
    "href": "posts/03. CH0304_simulation.html#회귀계수",
    "title": "03. SLR실습_simulation",
    "section": "회귀계수",
    "text": "회귀계수\nhat beta0 ~ N(beta0, Var(hat beta0))\nhat beta1 ~ N(beta1, Var(hat beta1))\n\nnum_samples = 10000\n\n\nbeta0_hats = rep(0, num_samples)\nbeta1_hats = rep(0, num_samples)\n\n\nx1 &lt;- seq(3,7, lenth.out=20)\nx2 &lt;- seq(1,10, lenth.out=20)\nbeta0 &lt;- 10\nbeta1 &lt;- -3\nsigma &lt;- 3\n\nWarning message:\n“In seq.default(3, 7, lenth.out = 20) :\n extra argument ‘lenth.out’ will be disregarded”\nWarning message:\n“In seq.default(1, 10, lenth.out = 20) :\n extra argument ‘lenth.out’ will be disregarded”\n\n\n\ntmp_dt1 &lt;- generating_slr(x1, beta0, beta1, sigma)\ntmp_dt2 &lt;- generating_slr(x2, beta0, beta1, sigma)\n\n\npar(mfrow=c(1,2))\nplot(y~x, tmp_dt1)\n\n\n\n\n\nm1 &lt;- lm(y~x, tmp_dt1)\nm2 &lt;- lm(y~x, tmp_dt2)\n\n\n\nsummary(m1)$coef\nsummary(m2)$coef\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n5.402469\n4.5800907\n1.179555\n0.3231998\n\n\nx\n-2.002932\n0.8814389\n-2.272343\n0.1076921\n\n\n\n\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n9.493529\n1.6580904\n5.72558\n4.411935e-04\n\n\nx\n-2.835804\n0.2672255\n-10.61203\n5.438402e-06\n\n\n\n\n\n\n\nnum_samples = 10000\n\nbeta0_hats = rep(0, num_samples)\nbeta1_hats = rep(0, num_samples)\n\n\n\n\nx &lt;- 1:10 \nbeta0 &lt;- 10\nbeta1 &lt;- -3\nsigma &lt;- 3\n\nset.seed(1004)\n\n\n\nfor (i in 1:num_samples) {\n  tmp_dt &lt;- generating_slr(x, beta0, beta1, sigma)\n  \n  sim_fit = lm(y ~ x, tmp_dt)\n  \n  beta0_hats[i] = coef(sim_fit)[1]\n  beta1_hats[i] = coef(sim_fit)[2]\n}\n\n\n\n## beta1\nhead(beta1_hats)\n\n\n-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312-2.83580381478312\n\n\n\n\nmean(beta1_hats)  ## empirical mean of beta1_hat\n\n-2.83580381478312\n\n\n\nbeta1 ## true mean of beta1_hat\n\n-3\n\n\n\n\nvar(beta1_hats)  ## empirical variance of beta1_hat\n\n0\n\n\n\nvar_beta1_hat &lt;- sigma^2/ sum((x - mean(x))^2)  ## true variance of beta1_hat\nvar_beta1_hat\n\n0.109090909090909\n\n\n\n\nhist(beta1_hats, prob = TRUE, breaks = 20, \n     xlab = expression(hat(beta)[1]), main = \"\", border = \"steelblue\")  ## empirical distribution of beta1_hat\ncurve(dnorm(x, mean = beta1, sd = sqrt(var_beta1_hat)), \n      col = \"darkorange\", add = TRUE, lwd = 3)  ## true distribution of beta1_hat"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#model",
    "href": "posts/03. CH0304_simulation.html#model",
    "title": "03. SLR실습_simulation",
    "section": "model",
    "text": "model\n\n## Model1 : y = 3 + 5x + epsilon,  epsilon~N(0,1)\n## Model2 : y = 3 + 5x + epsilon,  epsilon~N(0,x^2)\n## Model3 : y = 3 + 5x^2 + epsilon,  epsilon~N(0,25)\n\n\ngenerating function\n\n\n\nsim_1 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x + rnorm(n = n, mean = 0, sd = 1)\n  data.frame(x, y)\n}\n\n\nsim_2 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x + rnorm(n = n, mean = 0, sd = x)\n  data.frame(x, y)\n}\n\n\nsim_3 = function(n) {\n  x = runif(n = n) * 5\n  y = 3 + 5 * x ^ 2 + rnorm(n = n, mean = 0, sd = 5)\n  data.frame(x, y)\n}\n\n\n\nn &lt;- 200\nset.seed(1004)\ndt1 &lt;- sim_1(n)\ndt2 &lt;- sim_2(n)\ndt3 &lt;- sim_3(n)\n\n\n\n\nhead(dt1)\n\n\nA data.frame: 6 × 2\n\n\n\nx\ny\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.358233\n9.348186\n\n\n2\n1.229829\n8.768066\n\n\n3\n3.893231\n21.079976\n\n\n4\n4.895058\n28.165052\n\n\n5\n2.173793\n11.262647\n\n\n6\n4.570661\n25.031025\n\n\n\n\n\n\nhead(dt2)\n\n\nA data.frame: 6 × 2\n\n\n\nx\ny\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3.622810\n18.645942\n\n\n2\n1.045306\n10.137068\n\n\n3\n3.168525\n17.546195\n\n\n4\n3.585000\n23.911858\n\n\n5\n3.989679\n18.895470\n\n\n6\n1.211359\n8.320489\n\n\n\n\n\n\n\nhead(dt3)\n\n\nA data.frame: 6 × 2\n\n\n\nx\ny\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4.016845\n82.23803\n\n\n2\n3.874216\n84.05972\n\n\n3\n2.675354\n31.63082\n\n\n4\n4.220042\n85.95100\n\n\n5\n3.426507\n57.40688\n\n\n6\n2.881320\n41.75378\n\n\n\n\n\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt1, col='grey', pch=16, main = \"Model1\")\nplot(y~x, dt2, col='grey', pch=16, main = \"Model2\")\nplot(y~x, dt3, col='grey', pch=16, main = \"Model3\")\n\n\n\n\n\n\n##### model fitting\n\nfit1 &lt;- lm(y~x, dt1)\nfit2 &lt;- lm(y~x, dt2)\nfit3 &lt;- lm(y~x, dt3)\nfit4 &lt;- lm(y~x+I(x^2), dt3)\n\n\nfit4의 경우는 x^2을 추가 해서.. 사용한다.\nfit4 &lt;- lm(y~x+I(x^2), dt3) 로 하면 된다.ㅇI 안쓰면 안된ㅁ\n\n\nsummary(fit3)\n\n\nCall:\nlm(formula = y ~ x, data = dt3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.526  -8.086  -1.802   6.021  27.172 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -18.396      1.550  -11.87   &lt;2e-16 ***\nx             25.177      0.518   48.60   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 10.3 on 198 degrees of freedom\nMultiple R-squared:  0.9227,    Adjusted R-squared:  0.9223 \nF-statistic:  2362 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt1, col='grey', pch=16, main = \"Model1\")\nabline(fit1, col='darkorange', lwd=2)\nplot(y~x, dt2, col='grey', pch=16, main = \"Model2\")\nabline(fit2, col='darkorange', lwd=2)\nplot(y~x, dt3, col='grey', pch=16, main = \"Model3\")\nabline(fit3, col='darkorange', lwd=2)\n\n\n\n\n\n\n##### residual plot\npar(mfrow=c(1,3))\nplot(fitted(fit1),resid(fit1), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model1\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\n# 0에 대해서 대칭인지 확인하자!\n\nplot(fitted(fit2),resid(fit2), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model2\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\nplot(fitted(fit3),resid(fit3), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model3\")\nabline(h=0, col='darkorange', lty=2, lwd=2)"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#등분산성homoscedasticity",
    "href": "posts/03. CH0304_simulation.html#등분산성homoscedasticity",
    "title": "03. SLR실습_simulation",
    "section": "등분산성(homoscedasticity)",
    "text": "등분산성(homoscedasticity)\n\n\n##### \n\n### Breusch-Pagan Test\n## H0 : 등분산  vs.  H1 : 이분산 (Heteroscedasticity)\nlibrary(lmtest)\nbptest(fit1) # 0.05보다 큰값이므로 H0를 기각할 수 없다. 등분산\nbptest(fit2) # H0기각 이분산이다. (오차에다가 가중치를 사용해서 분산을 안정화시켜줌. x에 비례하도록 가중치를.. 가중최소제곱추정량(WLSE))\nbptest(fit3) # H0채택\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit1\nBP = 0.11269, df = 1, p-value = 0.7371\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit2\nBP = 26.728, df = 1, p-value = 2.342e-07\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit3\nBP = 0.19028, df = 1, p-value = 0.6627"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#정규성-normality",
    "href": "posts/03. CH0304_simulation.html#정규성-normality",
    "title": "03. SLR실습_simulation",
    "section": "정규성 (Normality)",
    "text": "정규성 (Normality)\n\n\n### Histogram of residuals\npar(mfrow = c(1, 3))\nhist(resid(fit1),\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit1\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\nhist(resid(fit2),\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit2\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\nhist(resid(fit3),  #오른쪽으로 꼬리가 길다.\n     xlab   = \"Residuals\",\n     main   = \"Histogram of Residuals, fit3\",\n     col    = \"darkorange\",\n     border = \"steelblue\",\n     breaks = 10)\n\n\n\n\n\n\n### QQplot\nNormal&lt;- rnorm(500)\nChisquare &lt;- rchisq(500, 3)\nhist(Normal)\nhist(Chisquare)\n\n\n\n\n\n\n\n\n\n# 빨간색 직선에 붙어있으면 정규분포다.\nt2 &lt;- rt(500, 3)\n\n\npar(mfrow=c(1,3))\nqqnorm(Normal, pch=16)\nqqline(Normal, col = 2, lwd=2)\n\n\nqqnorm(Chisquare, pch=16)\nqqline(Chisquare, col = 2)\n\n\n\nqqnorm(t2, pch=16)\nqqline(t2, col = 2)\n\n\n\n\n\n\npar(mfrow=c(1,3))\nhist(Normal, breaks = 10)\nhist(Chisquare, breaks = 10)\nhist(t2, breaks = 10)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n##\npar(mfrow=c(1,3))\nqqnorm(resid(fit1), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit1), col = \"steelblue\", lwd = 2)\n\nqqnorm(resid(fit2), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit2), col = \"steelblue\", lwd = 2)\n\nqqnorm(resid(fit3), main = \"Normal Q-Q Plot, fit1\", col = \"darkgrey\")\nqqline(resid(fit3), col = \"steelblue\", lwd = 2)\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#shapiro-wilk-test",
    "href": "posts/03. CH0304_simulation.html#shapiro-wilk-test",
    "title": "03. SLR실습_simulation",
    "section": "Shapiro-Wilk Test",
    "text": "Shapiro-Wilk Test\n\n## H0 : normal distribution  vs. H1 : not H0\n\nshapiro.test(resid(fit1))\nshapiro.test(resid(fit2))\nshapiro.test(resid(fit3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit1)\nW = 0.99577, p-value = 0.8555\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit2)\nW = 0.96659, p-value = 0.0001095\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit3)\nW = 0.96354, p-value = 4.875e-05"
  },
  {
    "objectID": "posts/03. CH0304_simulation.html#독립성-dw-test",
    "href": "posts/03. CH0304_simulation.html#독립성-dw-test",
    "title": "03. SLR실습_simulation",
    "section": "독립성 : DW test",
    "text": "독립성 : DW test\n\nlibrary(lmtest)\n\n\nModel4 : y = 3 + 5x + epsilon,\nepsilon~N(0, var),correlated\nepsilon_i = 0.8 * epsilon_(i-1) + v_i, v_i ~ N(0,1)\n\n\n\nsim_4 = function(n) {\n  e &lt;- rep(0,n); e[1] &lt;- rnorm(1)\n  \n  for(k in 2:n) {\n    e[k] &lt;- e[(k-1)]*(0.8) + rnorm(1,0,1)\n  } # 오차들이 앞의 오차에 영향을 받고 있어!\n  \n  x = runif(n = n) * 5\n  y = 3 + 5 * x  + e\n  data.frame(x, y)\n}\n\n\n\nn &lt;- 100\ndt4 &lt;- sim_4(n)\nfit4 &lt;- lm(y~x, dt4)\n\n\npar(mfrow=c(1,3))\nplot(y~x, dt4, col='grey', pch=16, main = \"Model4\")\nabline(fit4, col='darkorange', lwd=2)\n\nplot(fitted(fit4),resid(fit4), col = 'grey', pch=16, \n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model4\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\n\n\nplot(1:n,resid(fit4), col = 'grey', pch=16, type='l',\n     xlab = expression(hat(y)),\n     ylab = \"Residual\",\n     main = \"Residual plot in Model4\")\nabline(h=0, col='darkorange', lty=2, lwd=2)\npar(mfrow=c(1,1))\n\n\n\n\n\n\n## DWtest\n#H0 : 오차들은 독립이다. \ndwtest(fit1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit2, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit3, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit4, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit1\nDW = 2.2176, p-value = 0.1241\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit2\nDW = 2.3155, p-value = 0.02485\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit3\nDW = 2.1464, p-value = 0.2966\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 3.234e-13\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n\ndwtest(fit4, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\ndwtest(fit4, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho &gt; 0 양의상관관계\ndwtest(fit4, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho &lt; 0 음의상관관계\n\n\n# DW가 4에 가까울수록 음의상관관계가 크고 0에 가까울수록 양의 상관관계가 크다.\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 3.234e-13\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 1.617e-13\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n    Durbin-Watson test\n\ndata:  fit4\nDW = 0.55926, p-value = 1\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html",
    "href": "posts/04. 선형회귀분석 CH0607.html",
    "title": "04. MLR실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#산점도",
    "href": "posts/04. 선형회귀분석 CH0607.html#산점도",
    "title": "04. MLR실습",
    "section": "산점도",
    "text": "산점도\n\npairs(Boston[,which(names(Boston) %in% \n                      c('medv', 'rm', 'lstat'))], \n      pch=16, col='darkorange')\n\n\n\n\n\n\n마지막 행을 봐보자. 맨아래 왼쪽의 X축은 rm, Y축은 medv를 의미하고, 맨아래의 중간의 x축은 lstat를 의미한다.\nrm이 클수록 집값이 증가하고(직선관계처럼 보인다)\nlstat가 높아질수록 집값은 떨어지는 경향이 있다(곡선관계처럼 보인다. 처음엔 뚝 떨어지다가 천천히 감소)\nrm과 lstat의 다중공산성을 봐야해! &gt; 맨위 가운데 그림을 봐보자.."
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#상관관계",
    "href": "posts/04. 선형회귀분석 CH0607.html#상관관계",
    "title": "04. MLR실습",
    "section": "상관관계",
    "text": "상관관계\n\n# pairs(Boston, pch=16, col='darkorange')\ncor(Boston[,which(names(Boston) %in% \n                    c('medv', 'rm', 'lstat'))])\n\n\nA matrix: 3 × 3 of type dbl\n\n\n\nrm\nlstat\nmedv\n\n\n\n\nrm\n1.0000000\n-0.6138083\n0.6953599\n\n\nlstat\n-0.6138083\n1.0000000\n-0.7376627\n\n\nmedv\n0.6953599\n-0.7376627\n1.0000000\n\n\n\n\n\n\nrm과 medv는 양의 상관관계\nlstat와 mdev는 음의 상관관계\nrm과 lstat는 음의 상관관계"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#회귀모형-적합",
    "href": "posts/04. 선형회귀분석 CH0607.html#회귀모형-적합",
    "title": "04. MLR실습",
    "section": "회귀모형 적합",
    "text": "회귀모형 적합\n\nfit_Boston&lt;-lm(medv~rm+lstat, data=Boston)\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   &lt;2e-16 ***\nlstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon , \\  \\epsilon\\) ~ \\(N(0,\\sigma^2)\\)\n모형을 적합하라는 것은, \\(\\hat \\beta_0, \\hat \\beta_1, \\hat \\beta_2\\)를 구해서 \\(\\hat y=\\)꼴로 적어주기\n\n- summary 결과 해석\n\n회귀직선의 유의성에 대한 가설검정이다.\n\\(H_0: \\beta_1=\\beta_2=0\\) vs \\(H_1: \\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\)\nF-statistic: 444.3, F=MSR/MSE, p-value: &lt; 2.2e-16:p-value가 아주 작으므로 \\(H_0\\)를 기각할 수 있다. 즉 회귀직선은 유의하다.\nResidual standard error: 5.54 = \\(\\sqrt{MSE} = \\sqrt{\\widehat \\sigma^2} = \\widehat \\sigma^2\\)\nPr(&gt;|t|):양측 검정에 대한 유의성 검정\nEstimate 추정량, \\(\\hat \\beta_0=-1.35827, \\hat \\beta_1=5.09479, \\hat \\beta_2=-0.64236\\)\nStd. Error = \\(s.e(\\widehat \\beta_i) = \\dfrac{\\sigma^2}{c_{ii}}\\) 이고, \\(\\widehat {s.e}(\\widehat \\beta_i)\\sqrt{\\dfrac{\\widehat \\sigma^2}{c_{ii}}}\\)\nt value는 \\(t_0 = \\dfrac{\\widehat \\beta_i - 0}{\\widehat {s.e}(\\widehat \\beta_i)}\\)\n절편은 유의하지 않다.\nResiduals: 잔차해석. 0을 기준으로 대칭인가? 봤는데 max가 훨씬 더 커서 오른쪽으로 꼬리가 더 길 수 있겠네? 생각 가능"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#matrix",
    "href": "posts/04. 선형회귀분석 CH0607.html#matrix",
    "title": "04. MLR실습",
    "section": "matrix",
    "text": "matrix\n\n\nn = nrow(Boston)\nX = cbind(rep(1,n), Boston$rm, Boston$lstat)\ny = Boston$medv\n\n\nhead(X) # 506X3 행렬\n\n\nA matrix: 6 × 3 of type dbl\n\n\n1\n6.575\n4.98\n\n\n1\n6.421\n9.14\n\n\n1\n7.185\n4.03\n\n\n1\n6.998\n2.94\n\n\n1\n7.147\n5.33\n\n\n1\n6.430\n5.21\n\n\n\n\n\n\nhead(y) # medv값, 506x1행렬\n\n\n2421.634.733.436.228.7\n\n\n\n# beta = 3x1 행렬"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#y-xbeta-epsilon-rightarrow-widehat-beta-xtx-1xty",
    "href": "posts/04. 선형회귀분석 CH0607.html#y-xbeta-epsilon-rightarrow-widehat-beta-xtx-1xty",
    "title": "04. MLR실습",
    "section": "y = X\\(\\beta\\) + \\(\\epsilon \\rightarrow \\widehat \\beta = (X^TX)^{-1}X^Ty\\)",
    "text": "y = X\\(\\beta\\) + \\(\\epsilon \\rightarrow \\widehat \\beta = (X^TX)^{-1}X^Ty\\)\n\nbeta_hat = solve(t(X)%*%X) %*% t(X) %*% y   # t(X): X^T를 의미함 \nbeta_hat\ncoef(fit_Boston)\n\n\nA matrix: 3 × 1 of type dbl\n\n\n-1.3582728\n\n\n5.0947880\n\n\n-0.6423583\n\n\n\n\n\n(Intercept)-1.35827281187452rm5.09478798433654lstat-0.642358334244129\n\n\n\nt(X): \\(X^T\\)를 의미\n%*% : 행렬곱의미\nsolve() : 역행렬 구하는 함수\n\\(\\widehat y = X \\widehat \\beta\\)\n\n\ny_hat = X %*% beta_hat\ny_hat[1:5]\nfitted(fit_Boston)[1:5]\n\n\n28.941013680602525.484205660559132.659074768579832.406519999834931.6304069906576\n\n\n128.941013680603225.4842056605591332.6590747685798432.4065199998349531.6304069906576\n\n\n\ny_hat[1:5] 과 fitted(fit_Boston)[1:5] 값이 동일한 것을 확인 가능\n\n\nsse &lt;- sum((y - y_hat)^2) ##SSE\nsqrt(sse/(n-2-1)) ##RMSE\nsummary(fit_Boston)$sigma\n\n5.54025736698867\n\n\n5.54025736698867\n\n\n\n\\(SSE = \\sum (y_i - \\widehat y_i)^2\\)\n\\(RMSE = \\sqrt{SSE/(n-p-1)} = \\widehat \\sigma\\)"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#lm사용",
    "href": "posts/04. 선형회귀분석 CH0607.html#lm사용",
    "title": "04. MLR실습",
    "section": "lm사용",
    "text": "lm사용\n\ndt &lt;- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]\nhead(dt)\n\n\nA data.frame: 6 × 3\n\n\n\nrm\nlstat\nmedv\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n6.575\n4.98\n24.0\n\n\n2\n6.421\n9.14\n21.6\n\n\n3\n7.185\n4.03\n34.7\n\n\n4\n6.998\n2.94\n33.4\n\n\n5\n7.147\n5.33\n36.2\n\n\n6\n6.430\n5.21\n28.7\n\n\n\n\n\n\nfit_Boston&lt;-lm(medv~., data=dt)           # boston의 설명변수 13개 모두 다 사용하고 싶을때 (물결 뒤에 점을 찍어주기!!)\nfit_Boston&lt;-lm(medv~rm+lstat, data=dt)    # 설명변수 중 원하는 것만 사용하고 싶을때\n\n\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   &lt;2e-16 ***\nlstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\nhat y = -1.3583 + 5.0948rm - 0.6424lstat"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#분산분석회귀직선의-유의성-검정",
    "href": "posts/04. 선형회귀분석 CH0607.html#분산분석회귀직선의-유의성-검정",
    "title": "04. MLR실습",
    "section": "분산분석:회귀직선의 유의성 검정",
    "text": "분산분석:회귀직선의 유의성 검정\n\n\\(H_0: \\beta_1=\\beta_2=0\\) vs \\(H_1: \\beta_1 \\neq 0\\) or \\(\\beta_2 \\neq 0\\)\n\\(H_0\\): 귀무가설, null hypothesis, 영가설 -&gt; \\(y=\\beta_0\\)\n\\(H_1\\): 대립가설 -&gt; \\(y= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)\n\n\nanova(fit_Boston) ## XXX\n\n\nA anova: 3 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nrm\n1\n20654.42\n20654.41622\n672.9039\n8.266887e-95\n\n\nlstat\n1\n6622.57\n6622.56999\n215.7579\n6.669365e-41\n\n\nResiduals\n503\n15439.31\n30.69445\nNA\nNA\n\n\n\n\n\n\n설명변수가 두개로 쪼개져서 나온다.\n\n\nnull_model &lt;- lm(medv~1, data=dt)  #H0\nfit_Boston &lt;- lm(medv~., data=dt)  #H1\n\nanova(null_model, fit_Boston) ##***\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n505\n42716.30\nNA\nNA\nNA\nNA\n\n\n2\n503\n15439.31\n2\n27276.99\n444.3309\n7.008455e-112\n\n\n\n\n\n\nnull_model : 설명모델을 안쓰고 절편만 가져가는 모델. 1만 쓴다.(절편만)\nRSS의 \\(15439.31=SSE=\\sum(y_i - \\widehat y_i)^2\\)이고 \\(42716.30 = \\sum(y_i - \\bar y)^2=SST\\)\nSST와 SSE를 비교해서 적합이 잘 되어있는지 확인\nSum of Sq의 \\(27276.99=SSR\\)\nF = \\(\\dfrac{SSR/P}{SSE/(n-p-1)}\\)"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#beta의-신뢰구간",
    "href": "posts/04. 선형회귀분석 CH0607.html#beta의-신뢰구간",
    "title": "04. MLR실습",
    "section": "\\(\\beta\\)의 신뢰구간",
    "text": "\\(\\beta\\)의 신뢰구간\n\\(\\widehat \\beta_i \\pm t_{a/2} (n-p-1) \\widehat{s.e}(\\widehat \\beta_i)\\)\n\nvcov(fit_Boston)  ##var(hat beta) = (X^TX)^-1 \\sigma^2\n\n\nA matrix: 3 × 3 of type dbl\n\n\n\n(Intercept)\nrm\nlstat\n\n\n\n\n(Intercept)\n10.06683612\n-1.39248641\n-0.099178133\n\n\nrm\n-1.39248641\n0.19754958\n0.011930670\n\n\nlstat\n-0.09917813\n0.01193067\n0.001912441\n\n\n\n\n\n\n공분산 행렬 값\n\\(Var(\\widehat \\beta_1)=0.19754958, Var(\\widehat \\beta_2)= 0.001912441\\)\n\n\n#코드\nconfint(fit_Boston, level = 0.90)\n\n#수식(직접계산)\ncoef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]\ncoef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n\n\nA matrix: 3 × 2 of type dbl\n\n\n\n5 %\n95 %\n\n\n\n\n(Intercept)\n-6.5867396\n3.8701939\n\n\nrm\n4.3623583\n5.8272176\n\n\nlstat\n-0.7144229\n-0.5702938\n\n\n\n\n\n(Intercept)4.87535465808391rm5.9680255329079lstat-0.556439501179164\n\n\n(Intercept)-7.59190028183295rm4.22155043576519lstat-0.728277167309094\n\n\n\n\\(n=506, p=2\\)\nsummary(fit_Boston)$coef[,2] : s.e"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#평균반응-개별-y-추정",
    "href": "posts/04. 선형회귀분석 CH0607.html#평균반응-개별-y-추정",
    "title": "04. MLR실습",
    "section": "평균반응, 개별 y 추정",
    "text": "평균반응, 개별 y 추정\n\nE(Y|x0), y = E(Y|x0) + epsilon\n\n\nnew_dt &lt;- data.frame(rm=7, lstat=10)\n\n\npredict(fit_Boston, newdata = new_dt)\nc(1,7,10)%*%beta_hat  # hat y0 = -1.3583 + 5.0948*7 - 0.6424*10\n\n1: 27.88165973604\n\n\n\nA matrix: 1 × 1 of type dbl\n\n\n27.88166\n\n\n\n\n\n\n\\(x_{0}=\\begin{pmatrix} 1 \\\\ 7 \\\\ 10 \\end{pmatrix}, \\widehat{y}_{0}=x_{0}^{T}\\beta\\)\n\n\npredict(fit_Boston, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  ##평균반응\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n27.88166\n27.17347\n28.58985\n\n\n\n\n\n\npredict(fit_Boston, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  ## 개별 y\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n27.88166\n16.97375\n38.78957"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#절편을-포함하지-않는-회귀직선-원점을-지나는-회귀직선",
    "href": "posts/04. 선형회귀분석 CH0607.html#절편을-포함하지-않는-회귀직선-원점을-지나는-회귀직선",
    "title": "04. MLR실습",
    "section": "절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)",
    "text": "절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)\n\n\\(y=\\beta_1 x_1 + \\beta_2 x_2\\)\n\n\nfit_Boston0 &lt;- lm(medv ~ 0 + rm + lstat, dt)\nsummary(fit_Boston0)\nsummary(fit_Boston)\n\n\nCall:\nlm(formula = medv ~ 0 + rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.714  -3.498  -1.075   1.877  27.750 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nrm     4.90691    0.07019   69.91   &lt;2e-16 ***\nlstat -0.65574    0.03056  -21.46   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.536 on 504 degrees of freedom\nMultiple R-squared:  0.9485,    Adjusted R-squared:  0.9482 \nF-statistic:  4637 on 2 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   &lt;2e-16 ***\nlstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(Adjusted R-squared: 0.9482\\) vs \\(0.6371\\)\n\\(R^2=\\dfrac{SSR}{SST}=\\dfrac{\\sum(\\widehat y_i - \\bar y)^2}{\\sum(y_i-\\bar y)^2}\\)=\\(\\dfrac{설명변수 없을때와 있을때의 차이}{y의변동이 평균으로부터 얼마나 떨어져 있는지}\\)\n절편이 없는 모형의 \\(R^2=\\dfrac{\\sum(\\widehat y_i - 0)^2}{\\sum(y_i-0)^2}\\): 원점으로부터 얼마나 떨어져있는가. 기본적으로 엄청 큰 값을 가지게 된다.\n\n\n절편이 있다 vs 절편이 없다 에서는 \\(R^2\\)과 \\(RMSE=\\widehat \\sigma\\)를 확인해주는 게 좋다.\nRMSE비교 \\(5.536\\) vs \\(5.54\\) -&gt; 별로 차이가 없네?"
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#잔차분석",
    "href": "posts/04. 선형회귀분석 CH0607.html#잔차분석",
    "title": "04. MLR실습",
    "section": "잔차분석",
    "text": "잔차분석\n\nepsilon : 선형성, 등분산성, 정규성, 독립성\n\n\nyhat &lt;- fitted(fit_Boston)\nres &lt;- resid(fit_Boston)\n\n\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\n선형성 애매함\n\n\n등분산성\n\nH0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\n\n\nbptest(fit_Boston)\n\n\n    studentized Breusch-Pagan test\n\ndata:  fit_Boston\nBP = 1.5297, df = 2, p-value = 0.4654\n\n\n\np-value가 커서 기각을 못했다. 즉 등분산이다.\n\n\n\n정규성\n\n잔차의 QQ plot\n\n\npar(mfrow=c(1,2))\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\nhist(res)\npar(mfrow=c(1,1))\n\n\n\n\n\n이상치가 있는 듯 하다ㅡ\nShapiro-Wilk Test\n\n\n\n## H0 : normal distribution  vs. H1 : not H0\nshapiro.test(res)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.9098, p-value &lt; 2.2e-16\n\n\n\n정규분포 아니다\n\n\n\n독립성 검정 DW test\n\nH0 : uncorrelated vs H1 : rho != 0\n\n\ndwtest(fit_Boston, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  fit_Boston\nDW = 0.83421, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n독립이라고 할 수 없다."
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#가설검정-fm-rm",
    "href": "posts/04. 선형회귀분석 CH0607.html#가설검정-fm-rm",
    "title": "04. MLR실습",
    "section": "가설검정: FM, RM",
    "text": "가설검정: FM, RM\n\nreduced_model = lm(medv ~ rm+lstat, data = Boston) #(q=2) \nfull_model = lm(medv ~ ., data=Boston) #(P=13) full model\n\n# r=p-q=11\n\n\nsummary(full_model)\nsummary(reduced_model)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\n\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   &lt;2e-16 ***\nlstat       -0.64236    0.04373 -14.689   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,    Adjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\n\nreduced_model보다는 full_model이 더 좋아 보인다. (R^2와 RMSE확인햇을떄)\n13개 중 11개 변수가 유의함을 확인 가능\n\n\n가설검정\nRM : \\(H_0: \\beta_1= \\dots = \\beta_5 = \\beta_7 = \\dots = \\beta_{12} = 0\\)\n\nanova(reduced_model, full_model)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n503\n15439.31\nNA\nNA\nNA\nNA\n\n\n2\n492\n11078.78\n11\n4360.525\n17.60431\n1.425983e-29\n\n\n\n\n\n\\(F = \\dfrac{(SSE_{RM}-SSE_{RM})/r}{SSE_{FM}/(n-p-1)}=\\dfrac{(SSR_{FM}-SSR_{RM})/r}{SSE_{FM}/(n-p-1)}=17.60431\\)\n\nRSS : SSE를 의미. RM의 \\(SSE=15439.31\\), FM의 \\(SSE=11078.78\\)\nRes.Df: \\(n-q-1=503, n-p-1=492\\)\n\n\n#강의록에 있는 수식\np &lt;- full_model$rank-1\nq &lt;- reduced_model$rank-1\nSSE_FM &lt;- anova(full_model)$Sum[p+1] #SSE_FM\nSSE_RM &lt;- anova(reduced_model)$Sum[q+1]  #SSE_RM\n\nF0 &lt;- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(nrow(Boston)-p-1))\nF0\n\n17.60431143783\n\n\n\n#기각역 F_{0.05}(p-q,n-p-1)\nqf(0.95,p-q,nrow(Boston)-p-1)\n# p-value -&gt; 해당강의 20분쯤.. 어렵\n1-pf(F0, p-q,nrow(Boston)-p-1)\n\n1.80811652913556\n\n\n0\n\n\n\n#################################\nreduced_model = lm(medv ~ .-age-indus, data = Boston)  # 유의하지 않은 2개 변수 제거(-age-indus)\nfull_model = lm(medv ~ ., data=Boston) # 13개 변수\n\nanova(reduced_model, full_model)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n494\n11081.36\nNA\nNA\nNA\nNA\n\n\n2\n492\n11078.78\n2\n2.579374\n0.05727398\n0.9443416\n\n\n\n\n\n\n\\(H_0: \\beta_{indus} = \\beta_{age} = 0\\) 이고 H0기각 못하므로 빼도 된다."
  },
  {
    "objectID": "posts/04. 선형회귀분석 CH0607.html#general-linear-hypothesis",
    "href": "posts/04. 선형회귀분석 CH0607.html#general-linear-hypothesis",
    "title": "04. MLR실습",
    "section": "General linear hypothesis",
    "text": "General linear hypothesis\n\n선형 가설 검정\n\n\nx1&lt;-c(4,8,9,8,8,12,6,10,6,9)\nx2&lt;-c(4,10,8,5,10,15,8,13,5,12)\ny&lt;-c(9,20,22,15,17,30,18,25,10,20)\nfit&lt;-lm(y~x1+x2)  ##FM\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4575 -1.9100  0.3314  0.6388  3.2628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  -0.6507     2.9075  -0.224   0.8293  \nx1            1.5515     0.6462   2.401   0.0474 *\nx2            0.7599     0.3968   1.915   0.0970 .\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.278 on 7 degrees of freedom\nMultiple R-squared:  0.9014,    Adjusted R-squared:  0.8732 \nF-statistic:    32 on 2 and 7 DF,  p-value: 0.0003011\n\n\n\n\\(H_0 : T\\beta = c\\)\n\n\n# install.packages(\"car\")\n\n\n\n\\(H_0 : \\beta_1 = 1\\)\n\n\\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\) 에서\n\\(y=\\beta_0 + x_1 + \\beta_2 x_2 + \\epsilon\\) 이 된다.\n즉, \\(y-x_1 = \\beta_0 + \\beta_2 x_2 + \\epsilon\\)\n단순성형회귀모형이 되는것..(z)\n\n\nlibrary(car)\nlinearHypothesis(fit, c(0,1,0), 1)\n\nERROR: Error in library(car): there is no package called ‘car’\n\n\n\n오류가 나넹.. Rstudio에서 돌린거\n\n\n\n기각 못한다. beta1=1\n\\(H_0 : \\beta_1 = \\beta_2\\)\n\n#b1-b2=0 =&gt; (0,1,-1) *beta \n#H_0 : beta_1 = beta2\nlinearHypothesis(fit, c(0,1,-1), 0)\n\n\n\\(H_0 : \\beta_1 = \\beta_2+1\\)\n\n#H_0 : beta_1 = beta2 + 1\nlinearHypothesis(fit, c(0,1,-1), 1)\n\n\n\\(H_0 : \\beta_1 = \\beta_2+5\\)\n\n#H_0 : beta_1 = beta2 + 5\nlinearHypothesis(fit, c(0,1,-1), 5)\n\n\n5로 바꾸고 나니까 기각할 수 있다.\n\n- 강의노트 코드\n\n##H_0 : beta_1 = beta2 + 1\n#y=b0 + b1x1 + b2x2 + e = b0+x1 + b2(x1+x2)+e\n#y-x1 = b0+b2(x1+x2)+e :   RM\n\n\ny1 &lt;- y-x1\nz1 &lt;- x1 + x2\n\n\nfit2 &lt;- lm(y1~z1)\nsummary(fit2)\nanova(fit2)\n\n\nCall:\nlm(formula = y1 ~ z1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5054 -1.9294  0.4236  0.6821  3.4473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -1.0014     2.2175  -0.452 0.663574    \nz1            0.6824     0.1242   5.493 0.000578 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.137 on 8 degrees of freedom\nMultiple R-squared:  0.7904,    Adjusted R-squared:  0.7642 \nF-statistic: 30.17 on 1 and 8 DF,  p-value: 0.0005785\n\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nz1\n1\n137.85135\n137.851351\n30.17378\n0.0005784583\n\n\nResiduals\n8\n36.54865\n4.568581\nNA\nNA\n\n\n\n\n\n\nanova(fit)  ##FM\nanova(fit2)  #RM\n\n\nA anova: 3 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n313.04348\n313.043478\n60.323103\n0.0001100467\n\n\nx2\n1\n19.03040\n19.030400\n3.667135\n0.0970444465\n\n\nResiduals\n7\n36.32612\n5.189446\nNA\nNA\n\n\n\n\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nz1\n1\n137.85135\n137.851351\n30.17378\n0.0005784583\n\n\nResiduals\n8\n36.54865\n4.568581\nNA\nNA\n\n\n\n\n\n\n# F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}\np &lt;- fit$rank-1\nq &lt;- fit2$rank-1\nSSE_FM &lt;- anova(fit)$Sum[p+1] #SSE_FM\nSSE_RM &lt;- anova(fit2)$Sum[q+1]  #SSE_RM\nF0 &lt;- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(length(y)-p-1))\nF0\n\n0.0428807391894599\n\n\n\n#기각역 F_{0.05}(p-q,n-p-1)\nqf(0.95,p-q,length(y)-p-1)\n# p-value\npf(F0, p-q,length(y)-p-1,lower.tail = F)\n\n5.59144785122073\n\n\n0.841844969417543"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html",
    "href": "posts/06. 회귀진단 실습.html",
    "title": "06. 회귀진단 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "href": "posts/06. 회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "title": "06. 회귀진단 실습",
    "section": "Leverage vs. Outlier vs. Influence",
    "text": "Leverage vs. Outlier vs. Influence\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\ndt &lt;- data.frame(x = c(15,26,10,9,15,20,18,11,\n 8,20,7,9,10,11,11,10,12,42,17,11,10),\n y = c(95,71,83,91,102,87,93,100,\n 104,94,113,96,83,84,102,100,\n 105,57,121,86,100))\n\n\n######## 산점도\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\n\n\n\n\n\n27이하로 normal하게 분포되어있는 데이터"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "href": "posts/06. 회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "title": "06. 회귀진단 실습",
    "section": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)",
    "text": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)\n\n######## 회귀적합\nmodel_reg &lt;- lm(y~x, dt)\nsummary(model_reg)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.604  -8.731   1.396   4.523  30.285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 109.8738     5.0678  21.681 7.31e-15 ***\nx            -1.1270     0.3102  -3.633  0.00177 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.02 on 19 degrees of freedom\nMultiple R-squared:   0.41, Adjusted R-squared:  0.3789 \nF-statistic:  13.2 on 1 and 19 DF,  p-value: 0.001769\n\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#hat-matrix",
    "href": "posts/06. 회귀진단 실습.html#hat-matrix",
    "title": "06. 회귀진단 실습",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\nX = cbind(rep(1, nrow(dt)), dt$x)\nH = X %*% solve(t(X) %*% X) %*% t(X)\ndiag(H)\n\n\n0.04792247945102180.1545132342960560.06281577558253530.07054520775205490.04792247945102180.07261895784631620.05798959354498150.05666993439408790.07985823090264690.07261895784631620.09075484503431120.07054520775205490.06281577558253530.05666993439408790.05666993439408790.06281577558253530.05210768418671290.651609984164090.05305029786592260.05666993439408790.0628157755825353\n\n\n\n21x21행렬\n0.0628157755825353 18번쨰가 큰값을 가지고 있음. leverage일 확률이 크다.\n\n\nsum(diag(H))\n\n2\n\n\n\n\\(\\sum h_{ii} = p+1 = 2\\)\n\n\nhatvalues(model_reg)\n\n10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n\n\nhatvalues는 \\(h_{ii}\\)값 바로 나오게 하는 함수\n\n\nwhich.max(hatvalues(model_reg))  # 가장 큰 값의 번호\nhatvalues(model_reg)[which.max(hatvalues(model_reg))] ##h_{18,18}값\n\n18: 18\n\n\n18: 0.65160998416409\n\n\n\n2*(1+1)/nrow(dt)\n\n0.19047619047619\n\n\n\n\\(h_{18,18} &gt; 2 \\bar h=2 \\dfrac{p+1}{n}=2\\dfrac{2}{21}=0.19047619047619\\)이므로 18번째 관측값이 leverage point 로 고려 가능\n\n\nhatvalues(model_reg)&gt;0.19047619047619\n\n1FALSE2FALSE3FALSE4FALSE5FALSE6FALSE7FALSE8FALSE9FALSE10FALSE11FALSE12FALSE13FALSE14FALSE15FALSE16FALSE17FALSE18TRUE19FALSE20FALSE21FALSE\n\n\n\n0.19047619047619보다 큰값이 있다면 leverage point\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[18,],\"18\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#이상치",
    "href": "posts/06. 회귀진단 실습.html#이상치",
    "title": "06. 회귀진단 실습",
    "section": "이상치",
    "text": "이상치\n\n잔차: \\(e_i = y_i - \\hat y_i\\)\n\nresidual &lt;- model_reg$residuals\nhead(residual)\n\n12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.334062287911925\n\n\n\nhist(residual)\n\n\n\n\n\n1개의 관측값이 30~40 사이에 있다. 혼자 동떨어져있는 값\n\n\n\n내적으로 표준화된 잔차 ((internally) standardized residual)\n\n\\[r_i = \\dfrac{e_i}{\\hat \\sigma \\sqrt{1-h_{ii}}}\\]\n\n아마도\\(N(0,1)\\)을 따를거야. 하지만 \\(\\hat \\sigma\\)이므로 완벽한 정규분포는 아니고 그렇다면 t분포냐? 독립이 아니므로 t분포가 아니다. N이 충분히 크다면 대략적으로 정규분포를 따른다고 볼 수 있다.\n\n\n\\(e_i\\) ~ \\(N(0,\\sigma^2(1-h_{ii}))\\)\n\n\\(|r_i| \\geq 2\\) OR \\(|r_i| \\geq 3\\)이면 이상치\n\n\ns_residual &lt;- rstandard(model_reg)\nhead(s_residual)\n\n10.1888322174200252-0.9444063949896653-1.462264369447094-0.82158155071330550.8396593902729546-0.0314703908632008\n\n\n\n# 또는\ns_xx &lt;- sum((dt$x-mean(dt$x))^2) #S_xx\nh_ii &lt;- 1/21 + (dt$x- mean(dt$x))^2/s_xx #hatvalues로 구해도 됨. 혹은 X(X^TX)-1X^T로 해도 됨\n### h_ii &lt;- hatvalues(model_reg)\n### h_ii &lt;- influence(model_reg)$hat\nhat_sigma &lt;- summary(model_reg)$sigma #hat sigma\ns_residual &lt;- resid(model_reg)/(hat_sigma*sqrt(1-h_ii)) ## 내적\n\n\n\\(h_{ii}=\\dfrac{1}{n}+\\dfrac{(x_i-\\bar x)^2}{S_{xx}}\\)\n\n\nhist(s_residual)\n\n\n\n\n\n똑같은 모양인데 scaleing시켜서 0근처에서 움직이도록. 2나 3보다 크면 이상치라고 했으니 맨오른쪽에 있는 관측값이 이상치일 듯. 근데 약간 몰려있어서 애매해.\n\n\n\n외적으로 표준화된 잔차 ((externally) standardized residual)\n\n\\[r_i^* = \\dfrac{e_i}{\\hat \\sigma_i \\sqrt{1-h_{ii}}}\\]\n\n\\(t(n-p-2)=(n-1-p-1)\\)\n\n\n\\(\\hat \\sigma_i : i\\)번째 측정값 \\(y_i\\)를 제외하고 얻어진 \\(\\hat \\sigma\\)\n\\(\\hat \\sigma_i^2 = \\left[ (n-p-1) \\hat \\sigma^2 - \\dfrac{e_i^2}{1-h_{ii}} \\right] / (n-p-2)\\)\n\\(|r_i^*| \\geq t_{\\alpha/2}(n-p-2)\\)이면 이상치\n\ns_residual_i &lt;- rstudent(model_reg)\nhead(s_residual_i)\n\n10.1839684933793942-0.9415833513782013-1.510811922917994-0.81426336315943850.8328629175207956-0.030631827537088\n\n\n\n# 또는\nhat_sigma_i &lt;- sqrt(((21-1-1)*hat_sigma^2 - residual^2/(1-h_ii) )/(21-1-2))\n## hat_sigma_i &lt;- influence(model_reg)$sigma\ns_residual_i &lt;- residual/(hat_sigma_i*sqrt(1-h_ii)) ## 외적\n\n\nhist(s_residual_i)\n\n\n\n\n\n둥 떨어져있는 맨 오른쪽 관측값\n\n\nwhich.max(s_residual_i)\ns_residual_i[which.max(s_residual_i)]\n\n19: 19\n\n\n19: 3.60697972130439\n\n\n\nqt(0.975, 21-1-2)\n\n2.10092204024104\n\n\n\\(t_{0.025}(21-1-1) : \\alpha=5\\%\\)\n\\(|r_i^*| \\geq t_{\\alpha/2}(n-p-2)\\)이면, 유의수준 \\(\\alpha\\)에서, \\(i\\)번째 관측값이 이상점이라고 할 수 있다.\n따라서 19번째 관측값은 유의수준 0.05에서 이상점이라고 할 수 있다.\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[19,],\"19\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ns_residual_i[which(abs(s_residual_i)&gt;qt(0.975,21-2))]\n\n19: 3.60697972130439\n\n\n\n## 잔차그림\npar(mfrow = c(2, 2))\nplot(fitted(model_reg), residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"Residuals\",\n main = \"residual plot\")\nabline(h=0, lty=2)\nplot(fitted(model_reg), s_residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals\",\n ylim=c(min(-3, min(s_residual)),\n max(3,max(s_residual))),\n main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-3,-2,0,2,3), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,21-2),0,qt(0.975,21-2)), lty=2)\ntext (fitted(model_reg)[which(abs(s_residual_i)&gt;qt(0.975,21-2))],\n s_residual_i[which(abs(s_residual_i)&gt;qt(0.975,21-2))],\n which(abs(s_residual_i)&gt;qt(0.975,21-2)),adj = c(0,1))\n\n\n\n\n\n맨오른쪽 아래 그림은 \\(t_i^*\\)의 그림이고 점선은 위에는 \\(t_{0.025}(21-2)\\), 아래는 \\(-t_{0.025}(19)\\)"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#정규성-검정",
    "href": "posts/06. 회귀진단 실습.html#정규성-검정",
    "title": "06. 회귀진단 실습",
    "section": "정규성 검정",
    "text": "정규성 검정\n\n## 정규성 검정\npar(mfrow=c(1,2))\nhist(resid(model_reg),\n xlab = \"Residuals\",\n main = \"Histogram of Residuals\",\n col = \"darkorange\",\n border = \"dodgerblue\",\n breaks = 20)\nqqnorm(resid(model_reg),\n main = \"Normal Q-Q Plot\",\n col = \"darkgrey\",\n pch=16)\nqqline(resid(model_reg), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\nQ-Q plot에서 꼬리가 하나가 길게 나옴-&gt;이상치\n\n\n## Shapiro-Wilk Test\n## H0 : normal distribution vs. H1 : not H0\nshapiro.test(resid(model_reg))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_reg)\nW = 0.92578, p-value = 0.1133\n\n\n\n기각할 수 없다. 정규분포이다.\n\n\n## 독립성 검정\nlmtest::dwtest(model_reg)\n\n\n    Durbin-Watson test\n\ndata:  model_reg\nDW = 2.0844, p-value = 0.5716\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n### 등분산성\n## H0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\nbptest(model_reg)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_reg\nBP = 0.0014282, df = 1, p-value = 0.9699"
  },
  {
    "objectID": "posts/06. 회귀진단 실습.html#영향점",
    "href": "posts/06. 회귀진단 실습.html#영향점",
    "title": "06. 회귀진단 실습",
    "section": "영향점",
    "text": "영향점\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ninfluence(model_reg)\n\n\n    $hat\n        10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n    $coefficients\n        \n\nA matrix: 21 × 2 of type dbl\n\n\n\n(Intercept)\nx\n\n\n\n\n1\n0.086545033\n0.001045618\n\n\n2\n0.958749611\n-0.104156236\n\n\n3\n-1.623423657\n0.057755211\n\n\n4\n-1.022877601\n0.040022565\n\n\n5\n0.384830249\n0.004649436\n\n\n6\n0.005894578\n-0.001602673\n\n\n7\n0.023216186\n0.010379001\n\n\n8\n0.230329653\n-0.007159985\n\n\n9\n0.410719785\n-0.017252806\n\n\n10\n-0.117621441\n0.031980023\n\n\n11\n1.595051450\n-0.070799821\n\n\n12\n-0.437100147\n0.017102603\n\n\n13\n-1.623423657\n0.057755211\n\n\n14\n-1.230320253\n0.038245507\n\n\n15\n0.412910892\n-0.012835671\n\n\n16\n0.145243868\n-0.005167222\n\n\n17\n0.681955144\n-0.017203712\n\n\n18\n4.243970455\n-0.347768136\n\n\n19\n0.569162106\n0.066321856\n\n\n20\n-1.047739014\n0.032569820\n\n\n21\n0.145243868\n-0.005167222\n\n\n\n\n\n    $sigma\n        111.3143301900592211.0559573770349310.6687048798294411.1219769595247511.1128596831455611.3246668862836711.2946094952564811.3083981658447911.29861430792641011.20682225901661110.99278346332161211.28816820706241310.66870487982941410.84242194398621511.27164317318071611.31986011811681711.12966417084631811.1067560007425198.628196059920932010.97712792942652111.3198601181168\n\n    $wt.res\n        12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.33406228791192573.4119598823618182.5230374783198893.14207073373048106.665937712088081111.015081818867412-3.7309403514063813-15.603951436543214-13.4769625216801154.52303747831988161.39604856345675178.6500263931830218-5.540306160923011930.284970967498720-11.4769625216801211.39604856345675\n\n\n\n\n\n\ncoefficients: (Intercept)=\\(\\beta_0\\), x=\\(\\beta_1\\)\n0.001045618 = \\(\\hat \\beta_1 - \\tilde \\beta_1\\) : (포함하여)21개로 돌린거랑 (1번째 변수빼고)20개로 돌린거의 차이값\n-0.005167222 = \\(\\hat \\beta_1 - \\tilde \\beta_1\\) : (포함하여)21개로 돌린거랑 (21번째 변수빼고)20개로 돌린거의 차이값\n값이 크면 영향점으로 생각\n18번째 데이터가 살짝 커보인다.\n\n\n\n\n$sigma : \\(\\hat \\sigma_{(i)}\\)\n\n\n\n\n$wt.res : 신경쓰지말자\n\n\n\ninfluence.measures(model_reg)\n\nInfluence measures of\n     lm(formula = y ~ x, data = dt) :\n\n     dfb.1_    dfb.x    dffit cov.r   cook.d    hat inf\n1   0.01664  0.00328  0.04127 1.166 8.97e-04 0.0479    \n2   0.18862 -0.33480 -0.40252 1.197 8.15e-02 0.1545    \n3  -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n4  -0.20004  0.12788 -0.22433 1.115 2.56e-02 0.0705    \n5   0.07532  0.01487  0.18686 1.085 1.77e-02 0.0479    \n6   0.00113 -0.00503 -0.00857 1.201 3.88e-05 0.0726    \n7   0.00447  0.03266  0.07722 1.170 3.13e-03 0.0580    \n8   0.04430 -0.02250  0.05630 1.174 1.67e-03 0.0567    \n9   0.07907 -0.05427  0.08541 1.200 3.83e-03 0.0799    \n10 -0.02283  0.10141  0.17284 1.152 1.54e-02 0.0726    \n11  0.31560 -0.22889  0.33200 1.088 5.48e-02 0.0908    \n12 -0.08422  0.05384 -0.09445 1.183 4.68e-03 0.0705    \n13 -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n14 -0.24681  0.12536 -0.31367 0.992 4.76e-02 0.0567    \n15  0.07968 -0.04047  0.10126 1.159 5.36e-03 0.0567    \n16  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n17  0.13328 -0.05493  0.18717 1.096 1.79e-02 0.0521    \n18  0.83112 -1.11275 -1.15578 2.959 6.78e-01 0.6516   *\n19  0.14348  0.27317  0.85374 0.396 2.23e-01 0.0531   *\n20 -0.20761  0.10544 -0.26385 1.043 3.45e-02 0.0567    \n21  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n\n\n\ndfb.1_ ,  dfb.x : \\((\\hat \\beta_1 - \\tilde \\beta_{1(i)})/\\)뭘로나누자\ninf : 영향점인 것 같으면 *로 표시\n\n\nDFFITS\n\nDFFITS: \\(DFFITS(i) = \\dfrac{\\hat y_i - \\tilde y_i(i)}{\\hat \\sigma_{(i)} \\sqrt{h_{ii}}}\\)\n\\(|DFFITS(i)| \\geq 2 \\sqrt{\\dfrac{p+1}{n-p-1}}\\)이면 영향점\n\n\ndffits(model_reg) \n\n10.04127403575140562-0.4025206873025253-0.3911400454742154-0.22432853366080450.1868559838824216-0.0085717364067812270.077223952838937980.056303486522047690.085407472693718100.172840518129759110.33199685399425312-0.094449643042361813-0.39114004547421514-0.313673908094842150.101264129345836160.0329813827461469170.18716612805440518-1.15577873097521190.85373710713076620-0.263846244162542210.0329813827461469\n\n\n\nwhich(abs(dffits(model_reg)) &gt; 2*sqrt(2/(21-2)))\n\n18181919\n\n\n\n\nCook’s Distance\n\nCook’s Distance\n\\(D(i) = \\dfrac{\\sum_{i=1}^n (\\hat y_j - \\hat y_j(i))^2}{(p+1)\\hat \\sigma^2}=\\dfrac{(\\hat \\beta - \\hat \\beta(i))^T X^T X (\\hat \\beta - \\hat \\beta(i))}{(p+1) \\hat \\sigma^2}\\)\n\\(\\hat \\beta(i): i\\)번째 관측치를 제외하고 \\(n-1\\)개의 관측값에서 구한 \\(\\hat \\beta\\)의 최소제곱추저량\n\n\ncooks.distance(model_reg)\n\n10.00089740639287069120.081497955150763530.071658144221383340.025615958245264150.017743662633501363.87762740910137e-0570.003130574802994980.0016682085781346990.00383194880672965100.0154395158127621110.0548101351203612120.00467762256482442130.0716581442213833140.0475978118328145150.00536121617564154160.000573584529113046170.017856495213809180.678112028575845190.223288273631179200.0345188940892692210.000573584529113046\n\n\n\n\\(D(i) \\geq F_{0.5}(p+1, n-p-1)\\)이면 영향점으로 의심\n\n\nqf(0.5,2,21-2)\n\n0.719060569091733\n\n\n\nwhich(cooks.distance(model_reg) &gt;qf(0.5,2,21-2))\n\n\n\n\n없다!\n\n\nCOVRATIO\n\nCOVRATIO\n\n\\(COVRATIO(i) = \\dfrac{1}{\\left[1+\\dfrac{(r_i^*)^2-1}{n-p-1}\\right]^{p+1}(1-h_{ii})}\\)\n\\(|COVRATIO(i)-1| \\geq 3(p+1)/n\\)이면 \\(i\\)번째 관측치를 영향을 크게 주는 측정값으로 볼 수 있음\n\ncovratio(model_reg)\n\n11.1658918168321921.1969989767629630.93634739734183941.1151026899392951.0850410825772861.2013199827549771.1701575789867381.1742372676080391.19966823450598101.15209128858604111.08783960928084121.18326164825873130.936347397341839140.992331347870996151.15904532932769161.18673688685713171.09643883044992182.95868271380702190.396431612340971201.04257281407241211.18673688685713\n\n\n\nwhich(abs(covratio(model_reg)-1) &gt; 3*(1+1)/21)\n\n18181919\n\n\n\n영향점\n\n\nsummary(influence.measures(model_reg))\n\nPotentially influential observations of\n     lm(formula = y ~ x, data = dt) :\n\n   dfb.1_ dfb.x   dffit   cov.r   cook.d hat    \n18  0.83  -1.11_* -1.16_*  2.96_*  0.68   0.65_*\n19  0.14   0.27    0.85    0.40_*  0.22   0.05  \n\n\n\n*였떤 점만 뽑아서 보여줘ㅡ\n\n\n## 18제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-18,]), col='red', lwd=2)\ntext(dt[18,], pos=2, \"18\")\nlegend('topright', legend=c(\"full\", \"del(18)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-19,]), col='red', lwd=2)\ntext(dt[19,], pos=2, \"19\")\nlegend('topright', legend=c(\"full\", \"del(19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# not leverage and high influence, outlier\n\n\n\n\n\n19번째는 살짝 애매하다.\n\n\n## 18, 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18,19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-c(18,19),]), col='red', lwd=2)\ntext(dt[c(18,19),], pos=2, c(\"18\",\"19\"))\nlegend('topright', legend=c(\"full\", \"del(18,19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)"
  },
  {
    "objectID": "posts/10. GLS실습.html",
    "href": "posts/10. GLS실습.html",
    "title": "10. GLS 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/10. GLS실습.html#변수변환",
    "href": "posts/10. GLS실습.html#변수변환",
    "title": "10. GLS 실습",
    "section": "변수변환",
    "text": "변수변환\n\nbar_e1 &lt;- mean(resid(m)[-1])\nbar_e2 &lt;- mean(resid(m)[-20])\n# hat_rho &lt;- sum((resid(m)[-1]-bar_e1)*(resid(m)[-20] - bar_e2))/sum((resid(m)[-1]-bar_e1)^2) #일밎게 구하는거\nhat_rho &lt;- sum(resid(m)[-1]*resid(m)[-20])/sum((resid(m)[-20])^2)  #대략적으로 구하는거 \n# hat_rho &lt;- cor(resid(m)[-1], resid(m)[-20])\n\n\nhat_rho\n\n0.891002040194419\n\n\n\n\\(Corr(\\epsilon_i, \\epsilon_j) \\neq 0, i \\neq j\\)\n\n- 1차 자기상관 회귀모형\n\n\\(y=\\beta_0 + \\beta_1 x + \\epsilon\\)\n\\(\\epsilon_i = \\rho \\epsilon_{i-1} + \\delta_i\\)\n\\(\\delta_i\\) ~ \\(N(0,\\sigma^2), iid\\)\n현시점의 오차가 다음 시점의 오차에 영향을 준다. -&gt;시계열 데이터와 관련된 모형\n\\(y_i=\\beta_0+\\beta_1x_i + \\epsilon_i\\)\n\\(\\rho y_{i-1} = \\beta_0 + \\beta_1 \\rho x_{i-1} + \\rho \\epsilon_{i-1}\\)\n\\(y_i - \\rho y_{i-1}=(1-\\rho)\\beta_0 + \\beta_1 (x_i - \\rho x_{i-1}) + \\epsilon_i - \\rho \\epsilon_{i-1}\\)\n위를\n\\(y' = \\beta_0' + \\beta_1'x_i' + \\delta_i\\)\n\\(\\hat \\rho = \\dfrac{\\sum_{i=2}^n e_i e_{i-1}}{\\sum_{i=2}^n e_i^2}=0.891002040194419\\)\n\n\ny1 &lt;- dt$y[-1]-hat_rho*dt$y[-20] \nx1 &lt;- dt$x[-1]-hat_rho*dt$x[-20]\nm1 &lt;- lm(y1~x1)\nsummary(m1)\n\n\nCall:\nlm(formula = y1 ~ x1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.9956  -1.2060   0.7668   3.7082   8.1241 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  40.2455    13.1639   3.057  0.00713 **\nx1            0.4862     0.6483   0.750  0.46354   \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 6.347 on 17 degrees of freedom\nMultiple R-squared:  0.03202,   Adjusted R-squared:  -0.02492 \nF-statistic: 0.5624 on 1 and 17 DF,  p-value: 0.4635\n\n\n\ndwtest(m1)\n\n\n    Durbin-Watson test\n\ndata:  m1\nDW = 1.3704, p-value = 0.04663\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n데이터가 많지 않아서 크게 dw가 차이나지 ㄴ않음..\n\n\npar(mfrow=c(1,1))\nplot(y1~x1,pch=16,  xlab=expression(x^\"'\"),  ylab=expression(y^\"'\"), \n     main=\"scatter plot after v ariable transformation\")\nabline(m1, col='darkorange', lwd=2)\n\n\n\n\n\nplot(fitted(m1), resid(m1), pch=16,xlab=expression(hat(y)), ylab='residual', \n     main=paste0(\"residual plot for OLS : DW statistics = \", round(dwtest(m1)$stat,4))) \nabline(h=0)"
  },
  {
    "objectID": "posts/10. GLS실습.html#gls",
    "href": "posts/10. GLS실습.html#gls",
    "title": "10. GLS 실습",
    "section": "GLS",
    "text": "GLS\n\nV&lt;-diag(20)\nV&lt;-hat_rho^abs(row(V)-col(V))\nX&lt;-model.matrix(m)\nV.inv&lt;-solve(V)\nbeta&lt;-solve(t(X)%*%V.inv%*%X)%*%t(X)%*%V.inv%*%dt$y \nbeta\n\n\nA matrix: 2 × 1 of type dbl\n\n\n(Intercept)\n131.745681\n\n\nx\n1.714326\n\n\n\n\n\n\nplot(y~x, dt, pch=16)\nabline(m, col='darkorange', lwd=2)\nabline(beta[1], beta[2], col='steelblue', lwd=2)\nlegend(\"topleft\", c(\"OLS\", \"GLS\"),col=c(\"darkorange\", 'steelblue'), lty=1, lwd=2)\n\n\n\n\n- 시간 변수를 더 추가해보자.\n\ndt$t &lt;- 1:nrow(dt)\n\nm2 &lt;- lm(y~t, dt) \nsummary(m2)\n\n\nCall:\nlm(formula = y ~ t, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.107  -6.594   0.372   8.359  16.907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 348.0605     5.4652   63.69  &lt; 2e-16 ***\nt             6.2023     0.4562   13.60  6.6e-11 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.77 on 18 degrees of freedom\nMultiple R-squared:  0.9113,    Adjusted R-squared:  0.9063 \nF-statistic: 184.8 on 1 and 18 DF,  p-value: 6.605e-11\n\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.959  -8.874   2.035   9.035  16.623 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   89.234     26.721   3.339  0.00365 ** \nx              2.024      0.166  12.196 3.89e-10 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 12.98 on 18 degrees of freedom\nMultiple R-squared:  0.892, Adjusted R-squared:  0.886 \nF-statistic: 148.7 on 1 and 18 DF,  p-value: 3.888e-10"
  },
  {
    "objectID": "posts/AS1.html",
    "href": "posts/AS1.html",
    "title": "AS HW1",
    "section": "",
    "text": "원점을 지나는 회귀모형은 다음과 같이 정의할 수 있다.\n\\[y_i = β_1x_i + ϵ_i,  ϵ_i ∼_{i.i.d.} N(0, σ^2), i = 1, \\dots , n\\]\n\n\n오차제곱합을 정의하고 \\(β_1\\)의 최소제곱추정량 \\((\\hatβ_1)\\)을 구하여라.\n\\(S=\\sum_{i=1}^n \\epsilon^2 = \\sum_{i=1}^n (y_i-\\beta_1 x_i)^2 = \\sum(y_i^2 + \\beta_1^2 x_i^2 -2 \\beta_1 x_i y_i)\\)\n\\(\\widehat \\beta_1 = argmin \\sum_{i=1}^n(y_i - \\beta_1 x_i)^2\\)\n\\(\\dfrac{\\partial S}{\\partial \\beta_1}= -2 \\sum_{i=1}^n x_i(y_i-\\beta_1 x_i)\\)\n= \\(\\sum x_iy_i - \\beta_1 \\sum x_i^2 = 0\\)\n\\(\\therefore \\widehat \\beta_1 = \\dfrac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\)\n\n\n\n\\(E(\\hatβ_1)\\)을 구하여라.\n\\(a_i = \\dfrac{x_i}{\\sum_{i=1}^n x_i^2}\\)라고 놓자.\n즉, \\(\\widehat \\beta_1 = \\sum_{i=1}^n a_i y_i\\)\n\\(E(\\widehat \\beta_1)= E(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i E(y_i)=\\sum_{i=1}^n a_i E(\\beta_1 x_i + \\epsilon_i)=\\sum_{i=1}^n a_i \\beta_1 x_i = \\dfrac{\\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n x_i^2} \\beta_1 = \\beta_1\\)\n\\(E(\\hatβ_1)=\\beta_1\\)이므로 불편추정량\n\n\n\n\\(Var(\\hatβ_1)\\)을 구하여라.\n\\(Var(\\widehat \\beta_1)= Var(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i^2 Var(y_i)=\\dfrac{\\sigma^2}{\\sum_{i=1}^n x_i^2}\\)\n\\(\\because Var(y_i) = \\sigma^2\\)\n\n\n\n제곱합에 대한 분산분석표를 작성하여라.\n\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n\\(SSR\\)\n1\n\\(MSR=\\dfrac{SSR}{1}\\)\n\\(\\dfrac{MSR}{MSE}\\)\n\\(P(F \\geq F_0)\\)\n\n\n잔차\n\\(SSE\\)\n\\(n-1\\)\n\\(MSE=\\dfrac{SSE}{n-1}\\)\n\n\n\n\n계\n\\(SST\\)\n\\(n\\)\n\n\n\n\n\n\n\\(SSR=\\sum_{i=1}^n (\\widehat y_i)^2 = \\sum (\\widehat \\beta_1 x_i)^2 = \\widehat \\beta_1^2 \\sum x_i^2\\)\n\\(SSE=\\sum_{i=1}^n(y_i - \\widehat y)^2 = \\sum(y_i - \\widehat \\beta_1 x_i)^2 = \\sum y_i^2 - SSR\\)\n\\(SST=SSE+SSR=\\sum y_i^2\\)\n절편이 없는 모형은 평균이 0인 느낌\n\\(R^2=\\dfrac{\\sum \\widehat y_i^2}{\\sum y_i^2}\\)\n\n\n\n회귀모형의 유의성 검정을 하기 위한 가설을 설정하고, 검정통계량을 제시하여라.\n\n가설 \\(H_0: \\beta_1 = 0 \\ vs \\ H_1:\\beta_1 \\neq 0\\)\n검정통계량 \\(F=\\dfrac{MSR}{MSE}=\\dfrac{SSR/1}{SSE/(n-1)} \\sim_{H_0} F(1,n-1)\\)\n\n\n\n\n위의 가설에 대해, 유의수준 \\(α\\)에서 검정하는 방법을 기술하여라.\n\\(F_0 &gt; F_\\alpha(1,n-1)\\)이면 귀무가설을 기각(유의함)하고 그 외는 귀무가설을 채택\n혹은,\n유의확률 = \\(P(F&gt;F_0) &lt; \\alpha \\to H_0\\)기각\n유의확률 = \\(P(F&gt;F_0) &gt; \\alpha \\to H_0\\)기각못함\n\n\n\n다음의 가설에 대한 검정통계량을 제시하고, 유의수준 \\(α\\)에서 가설 검정하는 방법을 기술하여라.\n\\[H_0 : β_1 = 0        \\  vs   \\      H_1 : β_1 &gt; 0\\]\n검정통계량 \\(T=\\dfrac{\\widehat \\beta_1 - 0}{\\widehat{s.e}(\\widehat \\beta_1)} \\sim_{H_0} t(n-1)\\)\n\\(s.e(\\widehat \\beta_1)=\\sqrt{Var(\\widehat \\beta_1)} = \\dfrac{\\sigma}{\\sqrt{\\sum x_i^2}}\\)\n\\(\\widehat{s.e}(\\widehat \\beta_1)= \\sqrt{\\dfrac{\\hat {\\sigma^2}}{\\sum x_i^2}}, {\\hat \\sigma^2}=MSE\\) $\n유의확률 = \\(P(T &gt; t_0) &lt; \\alpha \\to H_0\\)기각\n혹은\n\\(t_0&gt;t_{\\alpha}(n-1) \\to H_0\\) 기각\n\\(t_0&lt;t_{\\alpha}(n-1) \\to H_0\\) 기각 못함"
  },
  {
    "objectID": "posts/AS1.html#section",
    "href": "posts/AS1.html#section",
    "title": "AS HW1",
    "section": "",
    "text": "오차제곱합을 정의하고 \\(β_1\\)의 최소제곱추정량 \\((\\hatβ_1)\\)을 구하여라.\n\\(S=\\sum_{i=1}^n \\epsilon^2 = \\sum_{i=1}^n (y_i-\\beta_1 x_i)^2 = \\sum(y_i^2 + \\beta_1^2 x_i^2 -2 \\beta_1 x_i y_i)\\)\n\\(\\widehat \\beta_1 = argmin \\sum_{i=1}^n(y_i - \\beta_1 x_i)^2\\)\n\\(\\dfrac{\\partial S}{\\partial \\beta_1}= -2 \\sum_{i=1}^n x_i(y_i-\\beta_1 x_i)\\)\n= \\(\\sum x_iy_i - \\beta_1 \\sum x_i^2 = 0\\)\n\\(\\therefore \\widehat \\beta_1 = \\dfrac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\\)"
  },
  {
    "objectID": "posts/AS1.html#section-1",
    "href": "posts/AS1.html#section-1",
    "title": "AS HW1",
    "section": "",
    "text": "\\(E(\\hatβ_1)\\)을 구하여라.\n\\(a_i = \\dfrac{x_i}{\\sum_{i=1}^n x_i^2}\\)라고 놓자.\n즉, \\(\\widehat \\beta_1 = \\sum_{i=1}^n a_i y_i\\)\n\\(E(\\widehat \\beta_1)= E(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i E(y_i)=\\sum_{i=1}^n a_i E(\\beta_1 x_i + \\epsilon_i)=\\sum_{i=1}^n a_i \\beta_1 x_i = \\dfrac{\\sum_{i=1}^n x_i^2}{\\sum_{i=1}^n x_i^2} \\beta_1 = \\beta_1\\)\n\\(E(\\hatβ_1)=\\beta_1\\)이므로 불편추정량"
  },
  {
    "objectID": "posts/AS1.html#section-2",
    "href": "posts/AS1.html#section-2",
    "title": "AS HW1",
    "section": "",
    "text": "\\(Var(\\hatβ_1)\\)을 구하여라.\n\\(Var(\\widehat \\beta_1)= Var(\\sum_{i=1}^n a_i y_i) = \\sum_{i=1}^n a_i^2 Var(y_i)=\\dfrac{\\sigma^2}{\\sum_{i=1}^n x_i^2}\\)\n\\(\\because Var(y_i) = \\sigma^2\\)"
  },
  {
    "objectID": "posts/AS1.html#section-3",
    "href": "posts/AS1.html#section-3",
    "title": "AS HW1",
    "section": "",
    "text": "제곱합에 대한 분산분석표를 작성하여라.\n\n\n\n\n\n\n\n\n\n\n\n요인\n제곱합(SS)\n자유도(df)\n평균제곱(MS)\n\\(F_0\\)\n유의확률\n\n\n\n\n회귀\n\\(SSR\\)\n1\n\\(MSR=\\dfrac{SSR}{1}\\)\n\\(\\dfrac{MSR}{MSE}\\)\n\\(P(F \\geq F_0)\\)\n\n\n잔차\n\\(SSE\\)\n\\(n-1\\)\n\\(MSE=\\dfrac{SSE}{n-1}\\)\n\n\n\n\n계\n\\(SST\\)\n\\(n\\)\n\n\n\n\n\n\n\\(SSR=\\sum_{i=1}^n (\\widehat y_i)^2 = \\sum (\\widehat \\beta_1 x_i)^2 = \\widehat \\beta_1^2 \\sum x_i^2\\)\n\\(SSE=\\sum_{i=1}^n(y_i - \\widehat y)^2 = \\sum(y_i - \\widehat \\beta_1 x_i)^2 = \\sum y_i^2 - SSR\\)\n\\(SST=SSE+SSR=\\sum y_i^2\\)\n절편이 없는 모형은 평균이 0인 느낌\n\\(R^2=\\dfrac{\\sum \\widehat y_i^2}{\\sum y_i^2}\\)"
  },
  {
    "objectID": "posts/AS1.html#section-4",
    "href": "posts/AS1.html#section-4",
    "title": "AS HW1",
    "section": "",
    "text": "회귀모형의 유의성 검정을 하기 위한 가설을 설정하고, 검정통계량을 제시하여라.\n\n가설 \\(H_0: \\beta_1 = 0 \\ vs \\ H_1:\\beta_1 \\neq 0\\)\n검정통계량 \\(F=\\dfrac{MSR}{MSE}=\\dfrac{SSR/1}{SSE/(n-1)} \\sim_{H_0} F(1,n-1)\\)"
  },
  {
    "objectID": "posts/AS1.html#section-5",
    "href": "posts/AS1.html#section-5",
    "title": "AS HW1",
    "section": "",
    "text": "위의 가설에 대해, 유의수준 \\(α\\)에서 검정하는 방법을 기술하여라.\n\\(F_0 &gt; F_\\alpha(1,n-1)\\)이면 귀무가설을 기각(유의함)하고 그 외는 귀무가설을 채택\n혹은,\n유의확률 = \\(P(F&gt;F_0) &lt; \\alpha \\to H_0\\)기각\n유의확률 = \\(P(F&gt;F_0) &gt; \\alpha \\to H_0\\)기각못함"
  },
  {
    "objectID": "posts/AS1.html#section-6",
    "href": "posts/AS1.html#section-6",
    "title": "AS HW1",
    "section": "",
    "text": "다음의 가설에 대한 검정통계량을 제시하고, 유의수준 \\(α\\)에서 가설 검정하는 방법을 기술하여라.\n\\[H_0 : β_1 = 0        \\  vs   \\      H_1 : β_1 &gt; 0\\]\n검정통계량 \\(T=\\dfrac{\\widehat \\beta_1 - 0}{\\widehat{s.e}(\\widehat \\beta_1)} \\sim_{H_0} t(n-1)\\)\n\\(s.e(\\widehat \\beta_1)=\\sqrt{Var(\\widehat \\beta_1)} = \\dfrac{\\sigma}{\\sqrt{\\sum x_i^2}}\\)\n\\(\\widehat{s.e}(\\widehat \\beta_1)= \\sqrt{\\dfrac{\\hat {\\sigma^2}}{\\sum x_i^2}}, {\\hat \\sigma^2}=MSE\\) $\n유의확률 = \\(P(T &gt; t_0) &lt; \\alpha \\to H_0\\)기각\n혹은\n\\(t_0&gt;t_{\\alpha}(n-1) \\to H_0\\) 기각\n\\(t_0&lt;t_{\\alpha}(n-1) \\to H_0\\) 기각 못함"
  },
  {
    "objectID": "posts/AS1.html#산점도",
    "href": "posts/AS1.html#산점도",
    "title": "AS HW1",
    "section": "(1) 산점도",
    "text": "(1) 산점도\n이 데이터의 산점도를 그리고 두 변수 사이의 관계를 설명하시오.\n\nplot(dist~speed,\n     data=cars,\n     xlab=\"speed\",\n     ylab=\"dist\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\n\n\n\n두 변수 사이에 선형관계가 있어 보인다."
  },
  {
    "objectID": "posts/AS1.html#회귀직선",
    "href": "posts/AS1.html#회귀직선",
    "title": "AS HW1",
    "section": "(2) 회귀직선",
    "text": "(2) 회귀직선\n최소제곱법의 의한 회귀직선을 적합시키시키고, 모형 적합 결과를 설명하시오.\n\ndt &lt;- data.frame(\n  i = 1:nrow(cars),\n  x = cars$speed,\n  y = cars$dist,\n  x_barx = cars$speed - mean(cars$speed),\n  y_bary = cars$dist - mean(cars$dist)) \ndt\n\n\nA data.frame: 50 × 5\n\n\ni\nx\ny\nx_barx\ny_bary\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4\n2\n-11.4\n-40.98\n\n\n2\n4\n10\n-11.4\n-32.98\n\n\n3\n7\n4\n-8.4\n-38.98\n\n\n4\n7\n22\n-8.4\n-20.98\n\n\n5\n8\n16\n-7.4\n-26.98\n\n\n6\n9\n10\n-6.4\n-32.98\n\n\n7\n10\n18\n-5.4\n-24.98\n\n\n8\n10\n26\n-5.4\n-16.98\n\n\n9\n10\n34\n-5.4\n-8.98\n\n\n10\n11\n17\n-4.4\n-25.98\n\n\n11\n11\n28\n-4.4\n-14.98\n\n\n12\n12\n14\n-3.4\n-28.98\n\n\n13\n12\n20\n-3.4\n-22.98\n\n\n14\n12\n24\n-3.4\n-18.98\n\n\n15\n12\n28\n-3.4\n-14.98\n\n\n16\n13\n26\n-2.4\n-16.98\n\n\n17\n13\n34\n-2.4\n-8.98\n\n\n18\n13\n34\n-2.4\n-8.98\n\n\n19\n13\n46\n-2.4\n3.02\n\n\n20\n14\n26\n-1.4\n-16.98\n\n\n21\n14\n36\n-1.4\n-6.98\n\n\n22\n14\n60\n-1.4\n17.02\n\n\n23\n14\n80\n-1.4\n37.02\n\n\n24\n15\n20\n-0.4\n-22.98\n\n\n25\n15\n26\n-0.4\n-16.98\n\n\n26\n15\n54\n-0.4\n11.02\n\n\n27\n16\n32\n0.6\n-10.98\n\n\n28\n16\n40\n0.6\n-2.98\n\n\n29\n17\n32\n1.6\n-10.98\n\n\n30\n17\n40\n1.6\n-2.98\n\n\n31\n17\n50\n1.6\n7.02\n\n\n32\n18\n42\n2.6\n-0.98\n\n\n33\n18\n56\n2.6\n13.02\n\n\n34\n18\n76\n2.6\n33.02\n\n\n35\n18\n84\n2.6\n41.02\n\n\n36\n19\n36\n3.6\n-6.98\n\n\n37\n19\n46\n3.6\n3.02\n\n\n38\n19\n68\n3.6\n25.02\n\n\n39\n20\n32\n4.6\n-10.98\n\n\n40\n20\n48\n4.6\n5.02\n\n\n41\n20\n52\n4.6\n9.02\n\n\n42\n20\n56\n4.6\n13.02\n\n\n43\n20\n64\n4.6\n21.02\n\n\n44\n22\n66\n6.6\n23.02\n\n\n45\n23\n54\n7.6\n11.02\n\n\n46\n24\n70\n8.6\n27.02\n\n\n47\n24\n92\n8.6\n49.02\n\n\n48\n24\n93\n8.6\n50.02\n\n\n49\n24\n120\n8.6\n77.02\n\n\n50\n25\n85\n9.6\n42.02\n\n\n\n\n\n\ndt$x_barx2 &lt;- dt$x_barx^2\ndt$y_bary2 &lt;- dt$y_bary^2\ndt$x_barxy_bary &lt;-dt$x_barx * dt$y_bary\ndt\n\n\nA data.frame: 50 × 8\n\n\ni\nx\ny\nx_barx\ny_bary\nx_barx2\ny_bary2\nx_barxy_bary\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4\n2\n-11.4\n-40.98\n129.96\n1679.3604\n467.172\n\n\n2\n4\n10\n-11.4\n-32.98\n129.96\n1087.6804\n375.972\n\n\n3\n7\n4\n-8.4\n-38.98\n70.56\n1519.4404\n327.432\n\n\n4\n7\n22\n-8.4\n-20.98\n70.56\n440.1604\n176.232\n\n\n5\n8\n16\n-7.4\n-26.98\n54.76\n727.9204\n199.652\n\n\n6\n9\n10\n-6.4\n-32.98\n40.96\n1087.6804\n211.072\n\n\n7\n10\n18\n-5.4\n-24.98\n29.16\n624.0004\n134.892\n\n\n8\n10\n26\n-5.4\n-16.98\n29.16\n288.3204\n91.692\n\n\n9\n10\n34\n-5.4\n-8.98\n29.16\n80.6404\n48.492\n\n\n10\n11\n17\n-4.4\n-25.98\n19.36\n674.9604\n114.312\n\n\n11\n11\n28\n-4.4\n-14.98\n19.36\n224.4004\n65.912\n\n\n12\n12\n14\n-3.4\n-28.98\n11.56\n839.8404\n98.532\n\n\n13\n12\n20\n-3.4\n-22.98\n11.56\n528.0804\n78.132\n\n\n14\n12\n24\n-3.4\n-18.98\n11.56\n360.2404\n64.532\n\n\n15\n12\n28\n-3.4\n-14.98\n11.56\n224.4004\n50.932\n\n\n16\n13\n26\n-2.4\n-16.98\n5.76\n288.3204\n40.752\n\n\n17\n13\n34\n-2.4\n-8.98\n5.76\n80.6404\n21.552\n\n\n18\n13\n34\n-2.4\n-8.98\n5.76\n80.6404\n21.552\n\n\n19\n13\n46\n-2.4\n3.02\n5.76\n9.1204\n-7.248\n\n\n20\n14\n26\n-1.4\n-16.98\n1.96\n288.3204\n23.772\n\n\n21\n14\n36\n-1.4\n-6.98\n1.96\n48.7204\n9.772\n\n\n22\n14\n60\n-1.4\n17.02\n1.96\n289.6804\n-23.828\n\n\n23\n14\n80\n-1.4\n37.02\n1.96\n1370.4804\n-51.828\n\n\n24\n15\n20\n-0.4\n-22.98\n0.16\n528.0804\n9.192\n\n\n25\n15\n26\n-0.4\n-16.98\n0.16\n288.3204\n6.792\n\n\n26\n15\n54\n-0.4\n11.02\n0.16\n121.4404\n-4.408\n\n\n27\n16\n32\n0.6\n-10.98\n0.36\n120.5604\n-6.588\n\n\n28\n16\n40\n0.6\n-2.98\n0.36\n8.8804\n-1.788\n\n\n29\n17\n32\n1.6\n-10.98\n2.56\n120.5604\n-17.568\n\n\n30\n17\n40\n1.6\n-2.98\n2.56\n8.8804\n-4.768\n\n\n31\n17\n50\n1.6\n7.02\n2.56\n49.2804\n11.232\n\n\n32\n18\n42\n2.6\n-0.98\n6.76\n0.9604\n-2.548\n\n\n33\n18\n56\n2.6\n13.02\n6.76\n169.5204\n33.852\n\n\n34\n18\n76\n2.6\n33.02\n6.76\n1090.3204\n85.852\n\n\n35\n18\n84\n2.6\n41.02\n6.76\n1682.6404\n106.652\n\n\n36\n19\n36\n3.6\n-6.98\n12.96\n48.7204\n-25.128\n\n\n37\n19\n46\n3.6\n3.02\n12.96\n9.1204\n10.872\n\n\n38\n19\n68\n3.6\n25.02\n12.96\n626.0004\n90.072\n\n\n39\n20\n32\n4.6\n-10.98\n21.16\n120.5604\n-50.508\n\n\n40\n20\n48\n4.6\n5.02\n21.16\n25.2004\n23.092\n\n\n41\n20\n52\n4.6\n9.02\n21.16\n81.3604\n41.492\n\n\n42\n20\n56\n4.6\n13.02\n21.16\n169.5204\n59.892\n\n\n43\n20\n64\n4.6\n21.02\n21.16\n441.8404\n96.692\n\n\n44\n22\n66\n6.6\n23.02\n43.56\n529.9204\n151.932\n\n\n45\n23\n54\n7.6\n11.02\n57.76\n121.4404\n83.752\n\n\n46\n24\n70\n8.6\n27.02\n73.96\n730.0804\n232.372\n\n\n47\n24\n92\n8.6\n49.02\n73.96\n2402.9604\n421.572\n\n\n48\n24\n93\n8.6\n50.02\n73.96\n2502.0004\n430.172\n\n\n49\n24\n120\n8.6\n77.02\n73.96\n5932.0804\n662.372\n\n\n50\n25\n85\n9.6\n42.02\n92.16\n1765.6804\n403.392\n\n\n\n\n\n\ncolSums(dt)\n\ni1275x770y2149x_barx-1.77635683940025e-14y_bary1.63424829224823e-13x_barx21370y_bary232538.98x_barxy_bary5387.4\n\n\n\\(\\beta_1 = \\dfrac{S_{xy}}{S_{xx}}\\)\n\\(\\beta_0 = \\bar y - \\beta_1 \\bar x\\)\n\nbeta1 &lt;- as.numeric(colSums(dt)[8]/colSums(dt)[6])\nbeta0 &lt;- mean(cars$dist) - beta1 * mean(cars$speed)\nbeta1\nbeta0\n\n3.93240875912409\n\n\n-17.579094890511\n\n\n\nmodel &lt;- lm(dist~speed, cars)\nmodel\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\n\nsummary(model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\np-value의 값이 1.49\\(e^{-12}\\)로 0.05보다 작아 유의하다.\nF검정은 89.57\n\\(\\mathbb{R^2}\\)는 0.6511\n\\(\\beta_0, \\beta_1\\)의 p-value값은 유의하다."
  },
  {
    "objectID": "posts/AS1.html#산점도-위에-회귀직선",
    "href": "posts/AS1.html#산점도-위에-회귀직선",
    "title": "AS HW1",
    "section": "(3) 산점도 위에 회귀직선",
    "text": "(3) 산점도 위에 회귀직선\n데이터의 산점도를 그리고 추정한 회귀직선을 (1)에서 그린 산점도 위에 그리시오.\n\nplot(dist~speed,\n     data=cars,\n     xlab=\"speed\",\n     ylab=\"dist\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\nabline(model, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/AS1.html#분산분석",
    "href": "posts/AS1.html#분산분석",
    "title": "AS HW1",
    "section": "(4) 분산분석",
    "text": "(4) 분산분석\n분산분석표를 작성하고 회귀직선의 유의 여부를 검정하시오.\n\nanova(model)\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nspeed\n1\n21185.46\n21185.4589\n89.56711\n1.489836e-12\n\n\nResiduals\n48\n11353.52\n236.5317\nNA\nNA\n\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,48)\n\n4.04265212856665\n\n\n\n\\(F_0 &gt; F_{0.05}(0.95,1,48) = 4.04\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다."
  },
  {
    "objectID": "posts/AS1.html#결정계수-상관계수",
    "href": "posts/AS1.html#결정계수-상관계수",
    "title": "AS HW1",
    "section": "(5) 결정계수, 상관계수",
    "text": "(5) 결정계수, 상관계수\n결정계수와 상관계수를 구하고 이 둘의 관계를 설명하시오.\n\\(\\mathbb{R^2} = \\dfrac{SSR}{SST} = \\dfrac{21185.46}{32538.98} = 0.651079413060889\\)\n- 결정계수 직접계산\n\nSST = sum((dt$y- mean(dt$y))^2)\nSSR = sum(((-17.57909 + 3.932409*dt$x)-mean(dt$y))^2)\n\n\nSST\nSSR\n\n32538.98\n\n\n21185.4615442987\n\n\n\nR2=SSR/SST\nR2\n\n0.651079460520848\n\n\n- 결정계수 코드\n\nsummary(model)$r.squared\n\n0.651079380758251\n\n\n\\(r_{xy} = \\dfrac{S_{xy}}{\\sqrt{S{(xx)}S{(yy)}}}\\)\n\nSxy &lt;- sum((dt$x - mean(dt$x))*(dt$y - mean(dt$y)))\nSxx &lt;- sum((dt$x - mean(dt$x))^2)\nSyy &lt;- sum((dt$y - mean(dt$y))^2)\n\n\nrxy&lt;-Sxy/sqrt(Sxx*Syy)\n\n\nrxy**2\n\n0.651079380758251\n\n\n단순선형회귀모형에서는 표본상관계수와 결정계수가 같다.\n\\(\\mathbb{R^2} = r_{xy}^2\\)"
  },
  {
    "objectID": "posts/AS1.html#개별-회귀계수-유의성검정",
    "href": "posts/AS1.html#개별-회귀계수-유의성검정",
    "title": "AS HW1",
    "section": "(6) 개별 회귀계수 유의성검정",
    "text": "(6) 개별 회귀계수 유의성검정\n\\(β_0, β_1\\)에 대한 개별 회귀계수의 유의성검정을 수행하시오.\n가설 \\(H_0: \\beta_1 = 0\\) vs \\(H_1: not H_0\\)\n- 직접구현\n\nSSE=SST-SSR\nMSE=SSE/48\n\n\ntvalue1 = beta1/(sqrt((MSE/sum((dt$x-mean(dt$x))^2))))\ntvalue1\n\n9.46399107202371\n\n\n\ntvalue0 = beta0/(sqrt((MSE*((1/48)+((mean(dt$x))^2/sum((dt$x-mean(dt$x))^2))))))\ntvalue0\n\n-2.59546417223485\n\n\n- 코드구현\n\nsummary(model)$coef\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-17.579095\n6.7584402\n-2.601058\n1.231882e-02\n\n\nspeed\n3.932409\n0.4155128\n9.463990\n1.489836e-12\n\n\n\n\n\n- 결과\n\nqt(0.975,48)\n\n2.01063475762423\n\n\n\nqt(0.025,48)\n\n-2.01063475762423\n\n\n\n\\(\\beta_0\\)에 대한 \\(t-vlaue\\)값이 \\(-2.601058\\)\n\\(\\beta_1\\)에 대한 \\(t-vlaue\\)값이 \\(9.463990\\)\n\\(\\beta_0, \\beta_1\\)의 t-value는 유의수준 \\(\\alpha=0.05\\)에서의 \\(tvalue=-2.011\\)보다 크기 때문에 유의하다. 즉 귀무가설을 기각한다. \\(\\beta_0, \\beta_1\\)은 모두 0이 아니다."
  },
  {
    "objectID": "posts/AS1.html#개별-회귀계수-신뢰구간",
    "href": "posts/AS1.html#개별-회귀계수-신뢰구간",
    "title": "AS HW1",
    "section": "(7) 개별 회귀계수 신뢰구간",
    "text": "(7) 개별 회귀계수 신뢰구간\n\\(β_0, β_1\\)에 대한 90% 신뢰구간을 구하시오.\n\\[\\widehat \\beta_0 \\pm t_{\\alpha/2}(n-2) \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]\n\\[\\widehat \\beta_1 \\pm t_{\\alpha/2}(n-2) \\dfrac{\\widehat \\sigma}{\\sqrt{S_{xx}}}\\]\n\n함수사용\n\n\nconfint(model, level=0.9)\n\n\nA matrix: 2 × 2 of type dbl\n\n\n\n5 %\n95 %\n\n\n\n\n(Intercept)\n-28.914514\n-6.243676\n\n\nspeed\n3.235501\n4.629317\n\n\n\n\n\n\n직접계산\n\n\ncoef(model) + qt(0.95, 48) * summary(model)$coef[,2]\n\n(Intercept)-6.24367551036937speed4.62931684193222\n\n\n\ncoef(model) - qt(0.95, 48) * summary(model)$coef[,2]\n\n(Intercept)-28.9145142706524speed3.23550067631595"
  },
  {
    "objectID": "posts/AS1.html#평균반응-신뢰구간",
    "href": "posts/AS1.html#평균반응-신뢰구간",
    "title": "AS HW1",
    "section": "(8) 평균반응, 신뢰구간",
    "text": "(8) 평균반응, 신뢰구간\n속도가 18.5mph 인 차량의 평균 제동거리를 예측하고, 95% 신뢰구간을 구하시오.\n개별 speed = 18,5\n\nnew_speed &lt;- data.frame(speed=18.5)\n\n- 코드\n\nmodel$coefficients[1] + model$coefficients[2]*18.5\n\n(Intercept): 55.1704671532847\n\n\n\npredict(model, \n        newdata = new_speed,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n55.17047\n50.08797\n60.25296\n\n\n\n\n\n- 직접계산\n- 평균반응 추정량\n\\[\\widehat \\mu_0 = \\widehat \\beta_0 + \\widehat \\beta_1 x_0\\]\n\nmu0 = beta0+beta1*18.5\nmu0\n\n55.1704671532847\n\n\n\\[Var(\\widehat\\mu_0))=\\sigma^2(\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar x)^2}{S_{xx}})\\]\n\nvarmu=((MSE*((1/48)+((18.5-mean(dt$x))^2/sum((dt$x-mean(dt$x))^2)))))\nsemu=sqrt(varmu)\n\n\nsemu\n\n2.5664989466792\n\n\n\nmu0+qt(0.975,48)*semu    \n\n60.3307591408838\n\n\n\nmu0-qt(0.975,48)*semu          \n\n50.0101751656855"
  },
  {
    "objectID": "posts/AS1.html#개별반응-신뢰구간",
    "href": "posts/AS1.html#개별반응-신뢰구간",
    "title": "AS HW1",
    "section": "(9) 개별반응, 신뢰구간",
    "text": "(9) 개별반응, 신뢰구간\n속도가 18.5mph 인 차량의 개별 제동거리를 예측하고, 95% 신뢰구간을 구하시오.\n- 코드\n\npredict(model, newdata = new_speed, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n55.17047\n23.83284\n86.5081\n\n\n\n\n\n- 직접계산\n\nvary0=MSE*(1+(1/48)+(18.5-mean(dt$x))^2/sum((dt$x-mean(dt$x))^2))\nvary0\n\n243.118551337083\n\n\n\nsqrt(vary0)\n\n15.5922593403613\n\n\n\nmu0+qt(0.975,48)*sqrt(vary0)\n\n86.5208057329061\n\n\n\nmu0-qt(0.975,48)*sqrt(vary0)\n\n23.8201285736632\n\n\n\nqt(0.95,8)\n\n1.8595480375309"
  },
  {
    "objectID": "posts/AS1.html#원점-지나는-회귀직선",
    "href": "posts/AS1.html#원점-지나는-회귀직선",
    "title": "AS HW1",
    "section": "(10) 원점 지나는 회귀직선",
    "text": "(10) 원점 지나는 회귀직선\n원점을 지나는 회귀직선을 구하시오.\n- 코드\n\nmodel2 &lt;- lm(dist ~ 0 + speed, cars)\nsummary(model2)\n\n\nCall:\nlm(formula = dist ~ 0 + speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.183 -12.637  -5.455   4.590  50.181 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nspeed   2.9091     0.1414   20.58   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 16.26 on 49 degrees of freedom\nMultiple R-squared:  0.8963,    Adjusted R-squared:  0.8942 \nF-statistic: 423.5 on 1 and 49 DF,  p-value: &lt; 2.2e-16\n\n\n\\(\\widehat {dist} = 2.9091 \\widehat {speed}\\)\n- 직접계산\n\nbeta1_0 &lt;- sum(dt$x * dt$y)/sum((dt$x^2))\nbeta1_0 \n\n2.9091321439371"
  },
  {
    "objectID": "posts/AS1.html#원점-지나는-회귀계수-신뢰구간",
    "href": "posts/AS1.html#원점-지나는-회귀계수-신뢰구간",
    "title": "AS HW1",
    "section": "(11) 원점 지나는 회귀계수 신뢰구간",
    "text": "(11) 원점 지나는 회귀계수 신뢰구간\n위 회귀직선에서 회귀계수(기울기)의 90% 신뢰구간을 구하시오.\n-코드\n\nconfint(model2, level=0.9)\n\n\nA matrix: 1 × 2 of type dbl\n\n\n\n5 %\n95 %\n\n\n\n\nspeed\n2.67212\n3.146144\n\n\n\n\n\n- 직접계산\n\ncolSums(dt)[6] #Sxx\n\nx_barx2: 1370\n\n\n\nSSR_0 = sum(((2.909132*dt$x))^2 )\n\n\nSSE_0 = sum((dt$y - 2.909132*dt$x)^2)\n\n\nSST_0 = sum(dt$y^2)\n\n\nMSE_0 = SSE_0/49\n\n\nsigma_0 = sqrt(MSE_0)\n\n\nbeta1_0 + qt(0.95,49) * sigma_0/sqrt(1370)\n\n3.64560479353767\n\n\n\nbeta1_0 - qt(0.95,49) * sigma_0/sqrt(1370)\n\n2.17265949433653\n\n\n\n qt(0.95,49)\n\n1.67655089261685"
  },
  {
    "objectID": "posts/AS1.html#원점-지난-회귀직선-분산분석",
    "href": "posts/AS1.html#원점-지난-회귀직선-분산분석",
    "title": "AS HW1",
    "section": "(12) 원점 지난 회귀직선 분산분석",
    "text": "(12) 원점 지난 회귀직선 분산분석\n원점을 지나는 회귀직선에 대한 분산분석표를 작성하고, 회귀직선의 유의 여부를 검정하시오.\n\nanova(model2)\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nspeed\n1\n111949.22\n111949.2232\n423.4682\n9.227817e-26\n\n\nResiduals\n49\n12953.78\n264.3628\nNA\nNA\n\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,49)\n\n4.03839263368304\n\n\n\n\\(F_0 &gt; F_{0.05}(0.95,1,49) = 4.04\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다."
  },
  {
    "objectID": "posts/AS1.html#원점-지나는-회귀직선-결정계수",
    "href": "posts/AS1.html#원점-지나는-회귀직선-결정계수",
    "title": "AS HW1",
    "section": "(13) 원점 지나는 회귀직선 결정계수",
    "text": "(13) 원점 지나는 회귀직선 결정계수\n원점을 지나는 회귀직선의 결정계수를 구하시오.\n- 코드\n\nsummary(model2)$r.squared\n\n0.896289305805206\n\n\n- 직접계산\n\nSSR_0/SST_0\n\n0.896289217112581"
  },
  {
    "objectID": "posts/AS1.html#회귀직선-결과-비교",
    "href": "posts/AS1.html#회귀직선-결과-비교",
    "title": "AS HW1",
    "section": "(14) 회귀직선 결과 비교",
    "text": "(14) 회귀직선 결과 비교\n원점을 포함한 회귀직선과 포함하지 않은 회귀직선의 결과를 비교하여라.\n\nsummary(model)$r.squared\nsummary(model2)$r.squared\n\n0.651079380758251\n\n\n0.896289305805206\n\n\n원점을 포함한 회귀직선의 R스퀘어값이 더 크므로 model2가 더 좋은 것 같다."
  },
  {
    "objectID": "posts/AS1.html#잔차-산점도",
    "href": "posts/AS1.html#잔차-산점도",
    "title": "AS HW1",
    "section": "(15) 잔차 산점도",
    "text": "(15) 잔차 산점도\n잔차에 대한 산점도를 그리고, 결과를 설명하여라.\n\ncars$disthat &lt;- model$fitted\ncars$resid &lt;- model$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ speed, cars, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ disthat, cars, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')"
  },
  {
    "objectID": "posts/AS1.html#잔차-등분산성-검정",
    "href": "posts/AS1.html#잔차-등분산성-검정",
    "title": "AS HW1",
    "section": "(16) 잔차 등분산성 검정",
    "text": "(16) 잔차 등분산성 검정\n잔차에 대한 등분산성 검정을 수행하시오.\n가설:\n\nbptest(model)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model\nBP = 3.2149, df = 1, p-value = 0.07297"
  },
  {
    "objectID": "posts/AS1.html#잔차-정규성-검정",
    "href": "posts/AS1.html#잔차-정규성-검정",
    "title": "AS HW1",
    "section": "(17) 잔차 정규성 검정",
    "text": "(17) 잔차 정규성 검정\n잔차에 대한 히스토그램, QQ plot을 그리고, 정규성 검정을 수행하여라.\n가설:\n\nqqnorm(cars$resid, pch=16)\nqqline(cars$resid, col=2)\n\nhist(cars$resid)\n\n\n\n\n\n\n\n\nshapiro.test(resid(model))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model)\nW = 0.94509, p-value = 0.02152\n\n\n\n\\(H_0\\): 정규성 만족, \\(H_1\\): 정규성만족X\np-value의 값이 0.05 보다 작으므로 귀무가설을 기각"
  },
  {
    "objectID": "posts/AS1.html#잔차-독립성-검정",
    "href": "posts/AS1.html#잔차-독립성-검정",
    "title": "AS HW1",
    "section": "(18) 잔차 독립성 검정",
    "text": "(18) 잔차 독립성 검정\n잔차에 대한 독립성 검정을 수행하시오.\n\ndwtest(model, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.1904\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nDW TEST의 p-value=0.1904이므로 H0를 기각할 수 없다. 채택. 즉 서로 독립이다.\n\n\ndwtest(model, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho &gt; 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.09522\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\ndwtest(model, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho &lt; 0\n\n\n    Durbin-Watson test\n\ndata:  model\nDW = 1.6762, p-value = 0.9048\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/02. CH0304.html",
    "href": "posts/02. CH0304.html",
    "title": "02. SLR실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/02. CH0304.html#회귀모형의-유의성-검정-분산분석표anova",
    "href": "posts/02. CH0304.html#회귀모형의-유의성-검정-분산분석표anova",
    "title": "02. SLR실습",
    "section": "회귀모형의 유의성 검정, 분산분석표(ANOVA)",
    "text": "회귀모형의 유의성 검정, 분산분석표(ANOVA)\n\nanova(model1)\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx\n1\n313.04348\n313.043478\n45.24034\n0.0001486582\n\n\nResiduals\n8\n55.35652\n6.919565\nNA\nNA\n\n\n\n\n\n\na &lt;- summary(model1)\n\n\nls(a)\n\n\n'adj.r.squared''aliased''call''coefficients''cov.unscaled''df''fstatistic''r.squared''residuals''sigma''terms'\n\n\n\na$r.squared\n\n0.849737997450786\n\n\n\na$adj.r.squared\n\n0.830955247132134\n\n\n\na$fstatistic\n\nvalue45.2403393025447numdf1dendf8\n\n\n\na$coef ## 회귀계수의 유의성 검정\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-2.269565\n3.212348\n-0.7065129\n0.4999255886\n\n\nx\n2.608696\n0.387847\n6.7260939\n0.0001486582\n\n\n\n\n\n\na$coef[,2] # s.e\n\n(Intercept)3.21234769191659x0.387847045641519"
  },
  {
    "objectID": "posts/02. CH0304.html#회귀계수의-신뢰구간",
    "href": "posts/02. CH0304.html#회귀계수의-신뢰구간",
    "title": "02. SLR실습",
    "section": "회귀계수의 신뢰구간",
    "text": "회귀계수의 신뢰구간\n\nconfint(model1, level=0.95)\n\n\nA matrix: 2 × 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-9.677252\n5.138122\n\n\nx\n1.714319\n3.503073\n\n\n\n\n\n\\[\\widehat \\beta \\pm t_{\\alpha/2}(n-2) s.e(\\widehat \\beta)\\]\n\nqt(0.025,8)\n\n-2.30600413520417\n\n\n\nqt(0.975,8) #왼쪽에 있는거 t_alpha/2 (n-2)\n\n2.30600413520417\n\n\n\ncoef(model1) + qt(0.975, 8) * summary(model1)$coef[,2]\n\n(Intercept)5.1381218438819x3.50307254324997\n\n\n\ncoef(model1) - qt(0.975, 8) * summary(model1)$coef[,2]\n\n(Intercept)-9.6772522786645x1.71431876109785"
  },
  {
    "objectID": "posts/02. CH0304.html#절편이-없는-회귀모형",
    "href": "posts/02. CH0304.html#절편이-없는-회귀모형",
    "title": "02. SLR실습",
    "section": "절편이 없는 회귀모형",
    "text": "절편이 없는 회귀모형\n\\[y=\\beta_1 x + \\epsilon\\]\n\nmodel2 &lt;- lm(y ~ 0 + x, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ 0 + x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0641 -1.5882  0.2638  1.4818  3.9359 \n\nCoefficients:\n  Estimate Std. Error t value Pr(&gt;|t|)    \nx   2.3440     0.0976   24.02  1.8e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.556 on 9 degrees of freedom\nMultiple R-squared:  0.9846,    Adjusted R-squared:  0.9829 \nF-statistic: 576.8 on 1 and 9 DF,  p-value: 1.798e-09\n\n\n\nsummary(model1)$r.squared\nsummary(model2)$r.squared\n\n0.849737997450786\n\n\n0.984636756628312\n\n\n\n절편이 없는 회귀모형의 R스퀘어가 더 크므로 model2가 더 좋은 것 같다.\n\n\nanova(model2)\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx\n1\n3769.1895\n3769.1895\n576.8138\n1.79763e-09\n\n\nResiduals\n9\n58.8105\n6.5345\nNA\nNA"
  },
  {
    "objectID": "posts/02. CH0304.html#lse",
    "href": "posts/02. CH0304.html#lse",
    "title": "02. SLR실습",
    "section": "LSE",
    "text": "LSE\n\ndt1 &lt;- data.frame(\n  i = 1:nrow(dt),\n  x = dt$x,\n  y = dt$y,\n  x_barx = dt$x - mean(dt$x),\n  y_bary = dt$y - mean(dt$y))\n\n\ndt1$x_barx2 &lt;- dt1$x_barx^2\ndt1$y_bary2 &lt;- dt1$y_bary^2\ndt1$xy &lt;-dt1$x_barx * dt1$y_bary\n\n\ndt1\n\n\nA data.frame: 10 × 8\n\n\ni\nx\ny\nx_barx\ny_bary\nx_barx2\ny_bary2\nxy\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4\n9\n-4\n-9.6\n16\n92.16\n38.4\n\n\n2\n8\n20\n0\n1.4\n0\n1.96\n0.0\n\n\n3\n9\n22\n1\n3.4\n1\n11.56\n3.4\n\n\n4\n8\n15\n0\n-3.6\n0\n12.96\n0.0\n\n\n5\n8\n17\n0\n-1.6\n0\n2.56\n0.0\n\n\n6\n12\n30\n4\n11.4\n16\n129.96\n45.6\n\n\n7\n6\n18\n-2\n-0.6\n4\n0.36\n1.2\n\n\n8\n10\n25\n2\n6.4\n4\n40.96\n12.8\n\n\n9\n6\n10\n-2\n-8.6\n4\n73.96\n17.2\n\n\n10\n9\n20\n1\n1.4\n1\n1.96\n1.4\n\n\n\n\n\n\nround(colSums(dt1),3)\n\ni55x80y186x_barx0y_bary0x_barx246y_bary2368.4xy120\n\n\n- 직접계산\n\\[\\widehat{\\beta_0} = \\bar y - \\widehat{\\beta_1} + \\bar x \\]\n\\[\\widehat{\\beta_1} = \\dfrac{S_{xy}}{S_{xx}}\\]\n\nbeta1 &lt;- as.numeric(colSums(dt1)[8]/colSums(dt1)[6])\n\nbeta0 &lt;- mean(dt$y) - beta1 *  mean(dt$x)\n\n\ncat(\"hat beta0 = \", beta0)\n\nhat beta0 =  -2.269565\n\n\n\ncat(\"hat beta1 = \", beta1)\n\nhat beta1 =  2.608696"
  },
  {
    "objectID": "posts/02. CH0304.html#그림",
    "href": "posts/02. CH0304.html#그림",
    "title": "02. SLR실습",
    "section": "그림",
    "text": "그림\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     pch  = 20,\n     cex  = 2,\n     col  = \"darkorange\",\n     ylim = c(0,35),\n     xlim = c(0, 12))\nabline(model1, col='steelblue', lwd=2)\nabline(model2, col='violet', lwd=2)\n\n\n\n\n\nco &lt;- coef(model1)\nco_2 &lt;- coef(model2) \n\n\nggplot(dt, aes(x, y)) +\n   geom_point(col='steelblue', lwd=3) +\n   geom_abline(intercept = co[1], slope = co[2], col='darkorange', lwd=1) +\n   geom_abline(intercept = 0, slope = co_2, col='darkgreen', lwd=1) +\n   xlab(\"광고료\")+ylab(\"총판매액\")+\n   theme_bw()+\n   theme(axis.title = element_text(size = 16))\n\nWarning message in geom_point(col = \"steelblue\", lwd = 3):\n“Ignoring unknown parameters: `linewidth`”"
  },
  {
    "objectID": "posts/02. CH0304.html#신뢰대",
    "href": "posts/02. CH0304.html#신뢰대",
    "title": "02. SLR실습",
    "section": "신뢰대",
    "text": "신뢰대\n\nbb &lt;- summary(model1)$sigma *\n  sqrt( 1 + 1/10 + (dt$x - 8)^2/46) ## 개별 y에 대한 추정량의 표준오차\ndt$ma95y &lt;- model1$fitted + 2.306*bb\ndt$mi95y &lt;- model1$fitted - 2.306*bb\n\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\nggplot(dt, aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=lm,\n              color=\"red\", fill=\"#69b3a2\", se=TRUE)+\n  geom_line(aes(x,mi95y), col='darkgrey', lty=2)+\n  geom_line(aes(x,ma95y), col='darkgrey', lty=2) +\n  theme_bw() +\n  theme(axis.title = element_blank())\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/02. CH0304.html#평균반응-개별y-추정",
    "href": "posts/02. CH0304.html#평균반응-개별y-추정",
    "title": "02. SLR실습",
    "section": "평균반응, 개별y 추정",
    "text": "평균반응, 개별y 추정\n\\[E(Y|x_0)\\]\n\\[y=E(Y|x_0)+\\epsilon\\]\n\n\\(x_0\\)=4.5\n\n\nnew_dt &lt;- data.frame(x = 4.5)\n\n\\[\\widehat \\mu_0 = \\widehat y_0 = \\widehat \\beta_0 + \\widehat \\beta_1 4.5\\]\n\nmodel1$coefficients[1] + model1$coefficients[2]*4.5\n\n(Intercept): 9.46956521739131\n\n\n\npredict(model1, newdata = new_dt)\n\n1: 9.46956521739131\n\n\n\npredict(model1, \n        newdata = new_dt,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n9.469565\n5.79826\n13.14087\n\n\n\n\n\n\npredict(model1, newdata = new_dt, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n9.469565\n2.379125\n16.56001\n\n\n\n\n\n\ndt_pred &lt;- data.frame(\n  x = c(1:12, 20, 35, 50),\n  predict(model1, \n          newdata=data.frame(x=c(1:12, 20, 35, 50)), \n          interval=\"confidence\", level = 0.95),\n  predict(model1, \n          newdata=data.frame(x=c(1:12, 20, 35, 50)), \n          interval=\"prediction\", level = 0.95)[,-1])\ndt_pred\n\n\nA data.frame: 15 × 6\n\n\n\nx\nfit\nlwr\nupr\nlwr.1\nupr.1\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0.3391304\n-6.2087835\n6.887044\n-8.5867330\n9.264994\n\n\n2\n2\n2.9478261\n-2.7509762\n8.646628\n-5.3751666\n11.270819\n\n\n3\n3\n5.5565217\n0.6905854\n10.422458\n-2.2199297\n13.332973\n\n\n4\n4\n8.1652174\n4.1058891\n12.224546\n0.8663128\n15.464122\n\n\n5\n5\n10.7739130\n7.4756140\n14.072212\n3.8692308\n17.678595\n\n\n6\n6\n13.3826087\n10.7597808\n16.005437\n6.7738957\n19.991322\n\n\n7\n7\n15.9913043\n13.8748223\n18.107786\n9.5667143\n22.415894\n\n\n8\n8\n18.6000000\n16.6817753\n20.518225\n12.2379683\n24.962032\n\n\n9\n9\n21.2086957\n19.0922136\n23.325178\n14.7841056\n27.633286\n\n\n10\n10\n23.8173913\n21.1945634\n26.440219\n17.2086783\n30.426104\n\n\n11\n11\n26.4260870\n23.1277880\n29.724386\n19.5214047\n33.330769\n\n\n12\n12\n29.0347826\n24.9754543\n33.094111\n21.7358781\n36.333687\n\n\n13\n20\n49.9043478\n39.0017505\n60.806945\n37.4278703\n62.380825\n\n\n14\n35\n89.0347826\n64.8105387\n113.259027\n64.0626010\n114.006964\n\n\n15\n50\n128.1652174\n90.5524421\n165.777993\n90.0664414\n166.263993\n\n\n\n\n\n\nnames(dt_pred)[5:6] &lt;- c('plwr', 'pupr')\ndt_pred\n\n\nA data.frame: 15 × 6\n\n\n\nx\nfit\nlwr\nupr\nplwr\npupr\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n0.3391304\n-6.2087835\n6.887044\n-8.5867330\n9.264994\n\n\n2\n2\n2.9478261\n-2.7509762\n8.646628\n-5.3751666\n11.270819\n\n\n3\n3\n5.5565217\n0.6905854\n10.422458\n-2.2199297\n13.332973\n\n\n4\n4\n8.1652174\n4.1058891\n12.224546\n0.8663128\n15.464122\n\n\n5\n5\n10.7739130\n7.4756140\n14.072212\n3.8692308\n17.678595\n\n\n6\n6\n13.3826087\n10.7597808\n16.005437\n6.7738957\n19.991322\n\n\n7\n7\n15.9913043\n13.8748223\n18.107786\n9.5667143\n22.415894\n\n\n8\n8\n18.6000000\n16.6817753\n20.518225\n12.2379683\n24.962032\n\n\n9\n9\n21.2086957\n19.0922136\n23.325178\n14.7841056\n27.633286\n\n\n10\n10\n23.8173913\n21.1945634\n26.440219\n17.2086783\n30.426104\n\n\n11\n11\n26.4260870\n23.1277880\n29.724386\n19.5214047\n33.330769\n\n\n12\n12\n29.0347826\n24.9754543\n33.094111\n21.7358781\n36.333687\n\n\n13\n20\n49.9043478\n39.0017505\n60.806945\n37.4278703\n62.380825\n\n\n14\n35\n89.0347826\n64.8105387\n113.259027\n64.0626010\n114.006964\n\n\n15\n50\n128.1652174\n90.5524421\n165.777993\n90.0664414\n166.263993\n\n\n\n\n\n\nbarx &lt;- mean(dt$x)\nbary &lt;- mean(dt$y)"
  },
  {
    "objectID": "posts/02. CH0304.html#신뢰대2",
    "href": "posts/02. CH0304.html#신뢰대2",
    "title": "02. SLR실습",
    "section": "신뢰대2",
    "text": "신뢰대2\n\nplot(y~x, data = dt,\n     xlab = \"광고료\",\n     ylab = \"총판매액\",\n     ylim = c(min(dt_pred$plwr), max(dt_pred$pupr)),\n     xlim = c(1,50),\n     pch  = 20,\n     cex  = 2,\n     col  = \"grey\"\n     )\nabline(model1, lwd = 5, col = \"darkorange\")\nlines(dt_pred$x, dt_pred$lwr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred$x, dt_pred$upr, col = \"dodgerblue\", lwd = 3, lty = 2)\nlines(dt_pred$x, dt_pred$plwr, col = \"darkgreen\", lwd = 3, lty = 3)\nlines(dt_pred$x, dt_pred$pupr, col = \"darkgreen\", lwd = 3, lty = 3)\n\nabline(v=barx, lty=2, lwd=0.2, col='dark grey')"
  },
  {
    "objectID": "posts/02. CH0304.html#잔차분석",
    "href": "posts/02. CH0304.html#잔차분석",
    "title": "02. SLR실습",
    "section": "잔차분석",
    "text": "잔차분석\n- \\(\\epsilon\\) : 선형성, 등분산성, 정규성, 독립성\n\ndt$yhat &lt;- model1$fitted\ndt$resid &lt;- model1$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ x, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, dt, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\npar(mfrow=c(1,1))\n\n\n\n\n\n등분산성\n\n\\(H_0\\):등분산 vs \\(H_1\\):이분산 (Heteroscedesticity)\n\n\nbptest(model1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 1.6727, df = 1, p-value = 0.1959\n\n\n\n\n잔차의 QQ plot\n\nqqnorm(dt$resid, pch=16)\nqqline(dt$resid, col = 2)\n\nhist(dt$resid)\n\n\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n\\(H_0\\):normal distribution vs \\(H_1\\): not \\(H_0\\)\n\n\nshapiro.test(resid(model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model1)\nW = 0.92426, p-value = 0.3939\n\n\n\n\n독립성검정: DW test\n\ndwtest(model1, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.3916\nalternative hypothesis: true autocorrelation is not 0\n\n\n\ndwtest(model1, alternative = \"greater\")  #H0 : uncorrelated vs H1 : rho &gt; 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.1958\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\ndwtest(model1, alternative = \"less\")     #H0 : uncorrelated vs H1 : rho &lt; 0\n\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 1.4679, p-value = 0.8042\nalternative hypothesis: true autocorrelation is less than 0"
  },
  {
    "objectID": "posts/AS1_3.html",
    "href": "posts/AS1_3.html",
    "title": "AS HW1_",
    "section": "",
    "text": "library(dplyr)\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\n데이터셋\n\nexams &lt;- read.csv(\"~/Dropbox/coco/posts/Applied statistics/exams.csv\")\nhead(exams)\n\n\nA data.frame: 6 × 8\n\n\n\ngender\nrace.ethnicity\nparental.level.of.education\nlunch\ntest.preparation.course\nmath.score\nreading.score\nwriting.score\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\nfemale\ngroup D\nsome college\nstandard\ncompleted\n59\n70\n78\n\n\n2\nmale\ngroup D\nassociate's degree\nstandard\nnone\n96\n93\n87\n\n\n3\nfemale\ngroup D\nsome college\nfree/reduced\nnone\n57\n76\n77\n\n\n4\nmale\ngroup B\nsome college\nfree/reduced\nnone\n70\n70\n63\n\n\n5\nfemale\ngroup D\nassociate's degree\nstandard\nnone\n83\n85\n86\n\n\n6\nmale\ngroup C\nsome high school\nstandard\nnone\n68\n57\n54\n\n\n\n\n\n\nsummary(exams)\n\n    gender          race.ethnicity     parental.level.of.education\n Length:1000        Length:1000        Length:1000                \n Class :character   Class :character   Class :character           \n Mode  :character   Mode  :character   Mode  :character           \n                                                                  \n                                                                  \n                                                                  \n    lunch           test.preparation.course   math.score     reading.score   \n Length:1000        Length:1000             Min.   : 15.00   Min.   : 25.00  \n Class :character   Class :character        1st Qu.: 58.00   1st Qu.: 61.00  \n Mode  :character   Mode  :character        Median : 68.00   Median : 70.50  \n                                            Mean   : 67.81   Mean   : 70.38  \n                                            3rd Qu.: 79.25   3rd Qu.: 80.00  \n                                            Max.   :100.00   Max.   :100.00  \n writing.score   \n Min.   : 15.00  \n 1st Qu.: 59.00  \n Median : 70.00  \n Mean   : 69.14  \n 3rd Qu.: 80.00  \n Max.   :100.00  \n\n\n\ndata &lt;- filter(exams,race.ethnicity== 'group E')\nhead(data)\n\n\nA data.frame: 6 × 8\n\n\n\ngender\nrace.ethnicity\nparental.level.of.education\nlunch\ntest.preparation.course\nmath.score\nreading.score\nwriting.score\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\nfemale\ngroup E\nassociate's degree\nstandard\nnone\n82\n83\n80\n\n\n2\nmale\ngroup E\nmaster's degree\nfree/reduced\nnone\n56\n46\n43\n\n\n3\nfemale\ngroup E\nassociate's degree\nfree/reduced\nnone\n80\n82\n85\n\n\n4\nmale\ngroup E\nassociate's degree\nstandard\nnone\n89\n88\n86\n\n\n5\nfemale\ngroup E\nassociate's degree\nstandard\nnone\n80\n79\n71\n\n\n6\nfemale\ngroup E\nsome college\nfree/reduced\nnone\n69\n74\n75\n\n\n\n\n\n- 데이터: Kaggle의 exams 데이터\n\ngroup E의 아래 두 데이터 상관관계를 보고자 함\nReading score: The student’s score on a standardized reading test\nWriting score: The student’s score on a standardized writing test\n\n\nnrow(data)\nncol(data)\n\n143\n\n\n8\n\n\n\n\n산점도\n\ndt &lt;- data.frame(\n  i = 1:nrow(data),\n  x = data$reading.score,\n  y = data$writing.score,\n  x_barx = data$reading.score - mean(data$reading.score),\n  y_bary = data$writing.score - mean(data$writing.score)) \nhead(dt)\n\n\nA data.frame: 6 × 5\n\n\n\ni\nx\ny\nx_barx\ny_bary\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n83\n80\n6.384615\n4.96503497\n\n\n2\n2\n46\n43\n-30.615385\n-32.03496503\n\n\n3\n3\n82\n85\n5.384615\n9.96503497\n\n\n4\n4\n88\n86\n11.384615\n10.96503497\n\n\n5\n5\n79\n71\n2.384615\n-4.03496503\n\n\n6\n6\n74\n75\n-2.615385\n-0.03496503\n\n\n\n\n\n\ndt$x_barx2 &lt;- dt$x_barx^2\ndt$y_bary2 &lt;- dt$y_bary^2\ndt$x_barxy_bary &lt;-dt$x_barx * dt$y_bary\nhead(dt)\n\n\nA data.frame: 6 × 8\n\n\n\ni\nx\ny\nx_barx\ny_bary\nx_barx2\ny_bary2\nx_barxy_bary\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n83\n80\n6.384615\n4.96503497\n40.763314\n2.465157e+01\n31.69983862\n\n\n2\n2\n46\n43\n-30.615385\n-32.03496503\n937.301775\n1.026239e+03\n980.76277569\n\n\n3\n3\n82\n85\n5.384615\n9.96503497\n28.994083\n9.930192e+01\n53.65788058\n\n\n4\n4\n88\n86\n11.384615\n10.96503497\n129.609467\n1.202320e+02\n124.83270576\n\n\n5\n5\n79\n71\n2.384615\n-4.03496503\n5.686391\n1.628094e+01\n-9.62183970\n\n\n6\n6\n74\n75\n-2.615385\n-0.03496503\n6.840237\n1.222554e-03\n0.09144701\n\n\n\n\n\n\n\nplot(y~x,\n     data=dt,\n     xlab=\"reading.score\",\n     ylab=\"writing.score\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\n\n\n\n양의 상관관계가 있어 보인다.\n\n\n\n회귀직선\n\nmodel_ &lt;- lm(y~x,dt)\nmodel_\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nCoefficients:\n(Intercept)            x  \n     -2.506        1.012  \n\n\n\\(\\widehat y =-2.506 + 1.012 x\\)\n\nsummary(model_)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.5572  -3.4544   0.3703   3.3341  12.7208 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.5063     2.2880  -1.095    0.275    \nx             1.0121     0.0294  34.419   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.778 on 141 degrees of freedom\nMultiple R-squared:  0.8936,    Adjusted R-squared:  0.8929 \nF-statistic:  1185 on 1 and 141 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nplot(y~x,\n     data=dt,\n     xlab=\"reading.score\",\n     ylab=\"writing.score\",\n     pch=16,\n     cex=2,\n     col=\"darkorange\")\n\n\nabline(model_, col='steelblue', lwd=2)\n\n\n\n\n\n\n분산분석\n\nanova(model_)\n\n\nA anova: 2 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx\n1\n27045.856\n27045.85588\n1184.685\n1.731866e-70\n\n\nResiduals\n141\n3218.969\n22.82957\nNA\nNA\n\n\n\n\n\n\n가정: \\(H_0: \\beta_1 = 0\\) vs \\(H_1: \\beta_1 \\neq 0\\)\n\n\nqf(0.95,1,141)\n\n3.90825811075366\n\n\n\n\\(F_0 &gt; F_{0.05}(0.95,1,141) = 3.91\\) 이므로 귀무가설을 기각할 수 잇다. 즉 회귀직선이 유의하다.\n\n\n\n결정계수, 상관계수\n\nsummary(model_)$r.squared\n\n0.893639917777383\n\n\n\nSxy &lt;- sum((dt$x - mean(dt$x))*(dt$y - mean(dt$y)))\nSxx &lt;- sum((dt$x - mean(dt$x))^2)\nSyy &lt;- sum((dt$y - mean(dt$y))^2)\n\n\nrxy&lt;-Sxy/sqrt(Sxx*Syy)\n\n\nrxy**2\n\n0.893639917777383\n\n\n\n\n개별 회귀계수 유의성 검정\n\\(β_0, β_1\\)에 대한 개별 회귀계수의 유의성검정을 수행하시오.\n가설 \\(H_0: \\beta_1 = 0\\) vs \\(H_1: not H_0\\)\n\nsummary(model_)$coef\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n-2.506277\n2.2880027\n-1.09540\n2.752092e-01\n\n\nx\n1.012084\n0.0294046\n34.41926\n1.731866e-70\n\n\n\n\n\n\nqt(0.975,141)\n\n1.97693148863425\n\n\n\\(\\beta_0\\)의 t-value= -1.09540 &lt; 1.97693148863425 이므로 귀무가설을 기각할 수 없다.\n\n\n신뢰구간\n\nconfint(model_, level=0.9)\n\n\nA matrix: 2 × 2 of type dbl\n\n\n\n5 %\n95 %\n\n\n\n\n(Intercept)\n-6.2945971\n1.282043\n\n\nx\n0.9633983\n1.060771\n\n\n\n\n\n\n\n평균반응\nreading score가 61.2 인 학생의 평균 wiring score 예측하고, 95% 신뢰구간을 구하시오.\n\nnew_score &lt;- data.frame(x=61.2)\n\n- 코드\n\nmodel_$coefficients[1] + model_$coefficients[2]*61.2\n\n(Intercept): 59.4332934119049\n\n\n\npredict(model_, \n        newdata = new_score,\n        interval = c(\"confidence\"),  #구간추정\n        level = 0.95)  ##평균반응\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n59.43329\n58.23874\n60.62785\n\n\n\n\n\n\n\n개별 y\nreading score가 61.2 인 학생의 개별 wiring score 예측하고, 95% 신뢰구간을 구하시오.\n\npredict(model_, newdata = new_score, \n        interval = c(\"prediction\"),  \n        level = 0.95)  ## 개별 y\n\n\nA matrix: 1 × 3 of type dbl\n\n\n\nfit\nlwr\nupr\n\n\n\n\n1\n59.43329\n49.91222\n68.95437\n\n\n\n\n\n\n\n잔차\n\ndata$yhat &lt;- model_$fitted\ndata$resid &lt;- model_$residuals\n\n\npar(mfrow=c(1,2))\nplot(resid ~ reading.score, data, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\nplot(resid ~ yhat, data, pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n\n\n\n\n\nbptest(model_)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_\nBP = 0.057101, df = 1, p-value = 0.8111\n\n\n\nqqnorm(data$resid, pch=16)\nqqline(data$resid, col=2)\nhist(data$resid)\n\n\n\n\n\n\n\n\nshapiro.test(resid(model_))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_)\nW = 0.9925, p-value = 0.6551\n\n\n\n\\(H_0\\): 정규성 만족, \\(H_1\\): 정규성만족X\np-value의 값이 0.05 보다 크므로 귀무가설 채택. 즉 정규성 가정을 만족한다.\n\n\ndwtest(model_, alternative = \"two.sided\") \n\n\n    Durbin-Watson test\n\ndata:  model_\nDW = 2.1808, p-value = 0.275\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nH0 : uncorrelated vs H1 : rho != 0\n\np-value값이 0.275로 0.05보다 크므로 독립성을 먼족한다."
  },
  {
    "objectID": "posts/AS3_3.html",
    "href": "posts/AS3_3.html",
    "title": "AS HW3_3(야구 투수)",
    "section": "",
    "text": "python-data-analysis data\nData Source\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/AS3_3.html#leverage",
    "href": "posts/AS3_3.html#leverage",
    "title": "AS HW3_3(야구 투수)",
    "section": "leverage",
    "text": "leverage\n\nhead(hatvalues(model2))\n\n10.28854950392561620.23891595070461730.32145716944908740.14678848429772450.14665250591637160.213076764712581\n\n\n\n2*(26+1)/nrow(dt)\n\n0.355263157894737\n\n\n\nwhich(hatvalues(model2)&gt;2*(26+1)/nrow(dt))\n\n202083838989949497979898105105115115\n\n\n\n\\(\\bar h=2 \\dfrac{p+1}{n}=2\\dfrac{26+1}{152}=0.355263157894737\\)"
  },
  {
    "objectID": "posts/AS3_3.html#이상치",
    "href": "posts/AS3_3.html#이상치",
    "title": "AS HW3_3(야구 투수)",
    "section": "이상치",
    "text": "이상치\n\ns_residual &lt;- rstandard(model2)\nhead(s_residual)\nhist(s_residual)\n\n10.3021994651797920.6037634769336436.0155083493491940.63198884760614351.1238740408836361.47300340677256\n\n\n\n\n\n\ns_residual_i &lt;- rstudent(model2)\nhead(s_residual_i)\nhist(s_residual_i)\n\n10.30109825071033120.60222232786380737.1079324915355240.63046387594556351.125068212601161.48000065156462\n\n\n\n\n\n\nwhich.max(s_residual_i)\ns_residual_i[which.max(s_residual_i)]\n\n3: 3\n\n\n3: 7.10793249153552"
  },
  {
    "objectID": "posts/AS3_3.html#영향점",
    "href": "posts/AS3_3.html#영향점",
    "title": "AS HW3_3(야구 투수)",
    "section": "영향점",
    "text": "영향점\n\ninfluence(model2)\n\n\n    $hat\n        10.28854950392561620.23891595070461730.32145716944908740.14678848429772450.14665250591637160.21307676471258170.19525013624359380.19858640005228490.178333591390648100.166941732848571110.136178129436448120.238129074463412130.110911601252613140.181085111598219150.0980295773316395160.119360621751013170.182756829765843180.139133515052432190.170150052021869200.507420170752351210.272566034438367220.108949843433006230.313684025229151240.0956994082756664250.191999401404375260.203250075084746270.242275776168802280.136990770860488290.174471019582568300.191142951112772310.18435648446416320.169092471408825330.139972064857371340.0949325721692582350.0779019670016611360.143310446194389370.204034788565768380.213723357627437390.249370554307032400.316760977023647410.131722513024269420.158695928354715430.332843504149478440.117370877098042450.147523628309658460.244275396873614470.146029952897111480.240573711517038490.29928391787881500.11378550515296510.148603598423763520.170850826976274530.199192002046881540.127201705469007550.179669038883243560.150041910207444570.109910022755335580.154033436537081590.229069096468305600.14601010845374610.116682024514985620.194892799508189630.09415851902261640.204252494276957650.202592346442938660.0965392040697959670.0922253267127638680.127487695931689690.116039978062371700.106939064613047710.0963656821620291720.202653946171695730.107479600829368740.104207414554694750.148774134801202760.107827930338103770.103491290921513780.204673276991790.110327515369565800.126915268627387810.274077455975528820.116665824559715830.356999451193955840.139131641644354850.130927658571057860.0951664092838356870.188360783892982880.112153858861555890.53802263252448900.29442640864548910.214940410515145920.117632435006589930.0999607010516776940.373848340929269950.216982277091479960.272211931643945970.417031583892121980.657461277293355990.1193576287895831000.1279389822655831010.09647170562952891020.1479186482145141030.1097626286522691040.2851313456868241050.4772411085642391060.1937115501296581070.1335526253453941080.3383645354083391090.1062007242500761100.09138431210442111110.1167997969775161120.06847861205056451130.1791765232833691140.07635272751872971150.6247747042945991160.2231880667839971170.1279148214434021180.1838392486514821190.11370236443591200.09425274522586591210.1392934682099461220.08885835413961171230.1096065221195891240.2520837321957981250.1457558709322421260.1145686669404671270.09764033679775631280.1270294782885131290.09516267073376851300.1537513034085771310.1343707484516211320.1255752056352171330.1122480273748161340.2038112204196171350.1102364889145971360.2344593936757331370.2087107520619811380.1161074397024131390.1102568829380171400.1290804869130631410.09789230476441931420.1919293181117061430.1361421310370611440.1754772361116841450.08766504843419861460.1171789720656181470.1122187578069011480.1305032270750791490.1399484064244781500.1279298860736461510.1268124042730071520.182774662111392\n\n    $coefficients\n        \n\nA matrix: 152 × 27 of type dbl\n\n\n\n(Intercept)\n팀명KT\n팀명LG\n팀명NC\n팀명SK\n팀명두산\n팀명롯데\n팀명삼성\n팀명한화\n승\n⋯\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n\n\n\n\n1\n-862.646548\n135.8785435\n41.063626\n93.6776586\n459.380072\n-0.9827616\n64.157743\n108.481695\n174.558297\n39.125334\n⋯\n-24.754211\n-31.890077\n-241.51556\n-259.73463\n6.39014269\n13.8765104\n-1.856373e+02\n641.36091\n-475.64526\n411.452668\n\n\n2\n-263.986866\n-41.7296418\n698.308578\n160.4404418\n129.183434\n2.5924320\n204.358592\n77.995650\n136.667193\n-86.886663\n⋯\n-330.523389\n-197.286863\n-1562.32953\n-236.39812\n-5.13990757\n3.6962299\n3.927276e+01\n5577.95966\n-4548.10168\n521.375388\n\n\n3\n11053.862820\n-4356.1582706\n-6086.905945\n-6095.6125710\n-6640.087069\n-8851.9958983\n-8076.850734\n-4585.399626\n-4221.311860\n2576.100209\n⋯\n-1692.541755\n-94.442582\n-2264.38310\n-2177.64809\n54.24275554\n44.2362931\n-3.999554e+03\n16117.67511\n-14720.73161\n4202.042314\n\n\n4\n-502.598729\n84.3393929\n757.150048\n45.3257385\n78.547977\n-5.0782628\n14.101181\n120.313073\n147.512678\n-79.740142\n⋯\n-138.441740\n-121.122415\n-789.37195\n-828.81020\n1.18303752\n21.1839480\n2.475688e+01\n2746.63897\n-2216.07645\n188.995744\n\n\n5\n-450.052763\n218.8067859\n97.624169\n80.6811225\n58.582514\n21.9584059\n1657.466197\n56.702812\n218.013491\n-4.430005\n⋯\n357.490450\n242.652547\n1542.24046\n648.61247\n-1.65925569\n-19.9485043\n-1.140481e+02\n-6012.98806\n5043.86203\n206.778248\n\n\n6\n2023.426022\n1440.7972428\n-343.571378\n70.2054817\n-103.393939\n-83.5663557\n-96.968161\n-30.559193\n57.506760\n-313.112777\n⋯\n54.659079\n84.663948\n466.06473\n-907.44591\n-24.27187904\n-72.9391426\n1.315624e+03\n-855.09337\n691.79801\n-261.535759\n\n\n7\n9884.740560\n-2451.7818582\n1262.225017\n386.5264737\n333.852827\n-219.7228419\n-263.836098\n1161.858316\n845.042071\n78.337321\n⋯\n-1168.239877\n107.056048\n-1458.05953\n1681.23579\n-64.55837815\n-54.4773820\n1.766216e+03\n11942.69043\n-11203.07777\n-2984.069748\n\n\n8\n4479.948553\n178.0576271\n46.903429\n241.4342339\n-53.892206\n2204.2643161\n-37.341281\n130.443348\n249.111187\n78.822661\n⋯\n457.356882\n366.899000\n2508.52110\n903.16935\n-52.18178584\n-143.7498073\n1.269444e+03\n-8002.09225\n6504.26079\n-1034.000833\n\n\n9\n-3535.908173\n-205.4681617\n649.562151\n-651.0620271\n-178.665646\n-3834.3668918\n-27.537475\n-220.838893\n-494.428757\n226.002719\n⋯\n-54.034191\n-250.411549\n-820.85526\n724.18390\n51.47629936\n177.9665843\n-8.160430e+02\n491.19316\n-176.32223\n-1082.112488\n\n\n10\n7.324447\n68.4103043\n76.191081\n72.2852389\n73.313489\n68.2007401\n76.252214\n68.323721\n70.087726\n10.634392\n⋯\n-23.993778\n-11.058311\n-103.13852\n-33.01432\n-0.78647558\n-1.3410334\n-8.571630e+00\n403.21044\n-337.52075\n1.082713\n\n\n11\n921.101879\n68.2829912\n79.145714\n68.1853662\n-15.454381\n-27.5847210\n-63.502766\n683.569995\n82.996042\n33.259846\n⋯\n-14.498487\n62.085654\n473.87053\n-122.77489\n0.07233331\n19.8780874\n1.675206e+02\n-918.96711\n588.53300\n-264.202410\n\n\n12\n-3807.255774\n-183.8862407\n-171.740332\n106.4804768\n194.462859\n-1655.2084994\n77.241909\n-155.669810\n-84.711494\n70.233450\n⋯\n-537.821955\n-587.068181\n-4016.74977\n-1020.53050\n18.64566652\n57.2247007\n2.732147e+02\n12125.84525\n-9560.76996\n429.747296\n\n\n13\n-5847.203268\n3986.2261715\n4031.051987\n3504.5040961\n3811.288142\n3906.5676324\n3876.664810\n3478.673387\n3748.438968\n88.575853\n⋯\n-23.669933\n22.354713\n-1189.43357\n-1372.97441\n-7.31232175\n-20.6146604\n-8.099508e+02\n3476.16806\n-2657.09678\n92.530859\n\n\n14\n-2120.868860\n-594.0474697\n-681.318386\n-310.8045100\n-259.347870\n250.0486199\n-2948.865736\n-420.883058\n-495.258678\n-78.019071\n⋯\n310.093442\n-67.361577\n690.74436\n-2012.00768\n27.24018823\n57.7474189\n-1.069010e+03\n-3582.90083\n3247.34704\n1743.490117\n\n\n15\n-297.537398\n-581.4300742\n-225.900371\n-462.7785917\n-335.652813\n109.9041251\n157.256648\n-2945.054406\n-700.539747\n-205.981101\n⋯\n819.169278\n268.750942\n1996.32068\n-15.91131\n-21.53787361\n-94.0017508\n4.666101e+01\n-10143.01859\n8936.68002\n-455.735181\n\n\n16\n-651.083662\n-249.6795926\n-189.989499\n-229.2804281\n-119.660434\n-3.5392335\n-1095.112629\n-261.007346\n-247.641279\n-101.857997\n⋯\n174.584877\n54.248366\n541.42702\n269.13220\n4.87145112\n2.5677217\n-1.845716e+02\n-2601.69300\n2261.94645\n217.191270\n\n\n17\n3503.643511\n-234.4749308\n-1443.757003\n-63.1456680\n-148.701318\n-106.8215802\n6.878622\n-5.607388\n-80.004194\n-92.453541\n⋯\n-47.478256\n216.396657\n1400.55827\n1321.96526\n-13.35268978\n-44.1444104\n9.452789e+02\n-2367.52073\n1521.85544\n-440.532243\n\n\n18\n1749.686074\n-290.0387843\n-345.033788\n30.4077575\n-270.893493\n329.1955829\n-165.837106\n1525.207409\n-209.839068\n-5.976719\n⋯\n185.855468\n183.261154\n2069.18575\n-456.45364\n6.59500961\n13.1986682\n-7.523333e+02\n-5512.35112\n4244.41255\n704.195736\n\n\n19\n2820.721009\n137.5374607\n-486.668484\n-0.9645205\n25.893453\n2.6335605\n110.377040\n193.061951\n166.016314\n55.213840\n⋯\n-162.179603\n165.445107\n707.78021\n-111.35774\n-5.61046242\n-10.6957201\n-2.160731e+02\n-234.00664\n-245.72787\n57.935624\n\n\n20\n-2049.236571\n-174.6845560\n-163.341842\n-399.2096462\n-8.330715\n50.6150702\n957.496272\n21.659368\n-178.727392\n-162.637383\n⋯\n-88.703457\n-180.148828\n-798.08404\n989.36598\n12.25659383\n13.7077214\n-1.444085e+01\n2198.73528\n-1628.47863\n112.432311\n\n\n21\n-3245.870842\n402.8703963\n41.796792\n-520.4255622\n399.048550\n525.9084997\n-481.450396\n234.761022\n3247.081289\n442.745455\n⋯\n326.183755\n-416.682568\n-1986.59055\n-5860.79100\n-7.72651939\n-47.7120938\n-3.732516e+02\n3660.67193\n-2206.18937\n503.377038\n\n\n22\n1538.237534\n140.6463052\n59.694859\n54.0147900\n-1272.908227\n-48.7400034\n30.255231\n96.391143\n130.245558\n31.054207\n⋯\n75.384718\n145.033299\n738.46881\n607.17617\n-15.48097481\n-50.5627942\n5.913144e+02\n-1983.05925\n1566.57801\n-357.548510\n\n\n23\n-5.107253\n-0.7582332\n-3.814803\n24.2065190\n0.205665\n-2.0466246\n2.836022\n-1.124301\n-1.199196\n-2.115045\n⋯\n-5.599786\n-3.890191\n-30.59798\n25.85302\n-0.15275610\n-0.2283475\n9.407654e-02\n95.70152\n-77.32872\n3.058915\n\n\n24\n518.255637\n18.1856112\n-36.728495\n52.0285559\n-27.525170\n37.4841119\n-66.046229\n60.694434\n426.486665\n-2.851971\n⋯\n27.478836\n49.491842\n394.49443\n-228.74521\n-0.56684539\n6.4643210\n5.038581e+00\n-998.31581\n742.35834\n-10.012841\n\n\n25\n3329.424175\n-287.0946265\n-176.878052\n-48.2815370\n-1180.689581\n167.8105290\n230.368341\n-11.149033\n-85.733883\n-175.047859\n⋯\n-102.447293\n233.273979\n1407.69323\n406.86206\n-7.01776544\n-26.0814364\n-3.178479e+02\n-2008.13481\n1170.52139\n787.891492\n\n\n26\n664.077102\n-781.3556934\n191.661951\n53.1642805\n-59.976313\n130.4812021\n67.050801\n-26.676732\n-98.737071\n30.059132\n⋯\n68.255121\n-8.069716\n123.94861\n-530.98910\n-2.98709097\n-23.1657553\n-2.791274e+02\n-442.20310\n396.60028\n-236.230754\n\n\n27\n-585.837817\n-146.4556323\n-79.316418\n-1347.6261498\n-78.887653\n144.6772846\n-43.179185\n-94.733714\n-160.553885\n-194.771574\n⋯\n157.768514\n50.080383\n576.90657\n1042.51356\n0.19581193\n-26.9374327\n-3.267655e+02\n-2494.52320\n2190.80534\n338.174186\n\n\n28\n-653.152268\n-97.5267486\n31.977857\n-947.8587136\n22.100354\n25.0497334\n29.398697\n-29.861465\n-88.264167\n28.762554\n⋯\n-243.678912\n-282.236249\n-1689.20193\n120.67303\n-3.90401021\n-25.2537579\n1.443872e+02\n5480.58966\n-4321.30940\n-129.610861\n\n\n29\n55.788175\n-10.2435118\n-40.000226\n307.3457058\n-24.040558\n48.5029583\n-23.218082\n-15.128389\n-23.528533\n-14.400644\n⋯\n78.870453\n55.507651\n395.16955\n65.49953\n0.25486050\n-3.8342398\n-7.791548e+01\n-1350.35689\n1094.64179\n47.785784\n\n\n30\n1813.825937\n-181.2747876\n-180.715310\n562.2916846\n-159.828617\n-1740.4167157\n-47.966774\n56.252528\n-84.067276\n-105.811492\n⋯\n275.043947\n372.520519\n2444.54940\n-1854.07175\n17.55692188\n96.3270587\n2.138224e+02\n-7420.07437\n5750.84320\n324.320594\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋱\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n123\n800.058491\n61.3916545\n23.6207608\n41.00125296\n29.81712981\n65.3548761\n-47.0024461\n166.25834653\n-323.8854391\n15.60072914\n⋯\n-103.1250466\n65.368297\n383.71508\n-743.746946\n7.438724353\n47.3268382\n-82.28789237\n188.75554\n-458.109297\n61.918251\n\n\n124\n2947.647093\n-29.8890440\n-59.3373313\n-65.60593840\n-42.22429902\n43.2474126\n-54.6034442\n32.01591793\n-285.9959698\n-5.63343405\n⋯\n-217.5310081\n41.439347\n-11.45375\n-1194.923344\n-13.041513329\n-55.7575641\n35.39177230\n2240.14770\n-2196.816522\n-55.230807\n\n\n125\n650.691565\n-22.6399143\n18.6086714\n-23.76414748\n0.74260305\n454.6891141\n39.0025098\n11.85809179\n-1.4770582\n-3.74737156\n⋯\n-82.9502864\n-26.492330\n-157.17349\n1092.143893\n-10.065140516\n-59.1927361\n-58.00724089\n1191.89677\n-1017.393117\n32.758846\n\n\n126\n-2053.188940\n-26.9891918\n-251.4147789\n-407.74781956\n-271.71249785\n-58.0564594\n-298.0179629\n80.83542990\n1679.7733154\n-28.99190468\n⋯\n48.3212549\n-384.172009\n-811.12880\n-2368.017933\n12.154418239\n71.8345078\n47.52448646\n1091.74369\n-338.771742\n150.911590\n\n\n127\n28.398635\n344.8655039\n80.6970327\n23.90935652\n53.68405820\n-21.1954960\n61.0120712\n13.77804180\n48.7825846\n-11.89304352\n⋯\n-87.3634556\n-16.819867\n-392.06075\n207.986181\n-3.341088992\n-7.1847198\n-8.75487172\n1380.33275\n-1146.843728\n-30.903050\n\n\n128\n1652.394837\n-18.5437160\n13.5366665\n-0.46772022\n-28.56638697\n574.9569720\n63.7235976\n-38.26612942\n-1.1907369\n10.47812523\n⋯\n51.1943935\n40.807042\n274.27546\n-470.887171\n-18.493387296\n-46.6744071\n-20.57785068\n-773.06630\n648.263024\n6.891148\n\n\n129\n-1067.485413\n9.5874385\n-13.1565656\n-47.45169478\n10.81515486\n-19.8221702\n-7.7188675\n-55.12022715\n-323.9848925\n-21.76113054\n⋯\n90.6248354\n-44.227842\n-179.12264\n124.481459\n1.784397369\n-0.9224876\n-15.45758875\n-334.83520\n451.355644\n58.312300\n\n\n130\n-83.299810\n-600.9557245\n-602.0676408\n-494.28585877\n-572.60006215\n-495.9003197\n-498.7706469\n-625.26454890\n-629.4609064\n4.08961069\n⋯\n234.6494568\n104.859117\n870.76414\n287.396885\n5.610168739\n3.5299978\n11.42519358\n-3742.48893\n3180.671542\n-13.599112\n\n\n131\n-500.388376\n-100.7444319\n-41.0052268\n-123.51248352\n-38.70000863\n419.2369592\n-20.2592642\n-112.36047244\n-93.6399324\n22.85031468\n⋯\n71.9661143\n-44.915827\n16.31502\n250.435384\n0.146531687\n-23.1274124\n36.53224574\n-319.22156\n378.598779\n-87.869834\n\n\n132\n8.802451\n0.9259852\n0.1466575\n-35.54295465\n3.75696625\n-4.4532138\n-1.5148811\n2.49125333\n1.4369049\n0.07987611\n⋯\n-13.3112514\n-5.333722\n-55.20828\n-6.758667\n0.021471158\n0.9692705\n1.42370423\n206.52532\n-176.727042\n-3.393562\n\n\n133\n-574.585355\n-26.4385586\n30.9981925\n-0.48950398\n8.68690633\n-68.3899730\n21.3309851\n206.23368450\n-30.9631918\n-14.10714088\n⋯\n-48.1543055\n-35.412977\n-281.78832\n581.417489\n4.296896360\n14.9955201\n-17.84923782\n655.68872\n-512.217089\n-28.054961\n\n\n134\n454.500049\n1012.2388149\n934.6414852\n1197.85750224\n1175.42657852\n1265.7067485\n1329.2094889\n1091.38205044\n1182.6563588\n-64.40873807\n⋯\n7.8453021\n111.833517\n891.25368\n873.364218\n-3.178043642\n-19.8790108\n-222.49138346\n-1710.01624\n1186.529919\n642.223465\n\n\n135\n-150.903595\n5.3216677\n0.1825169\n-0.07368799\n4.84874635\n-14.7152423\n0.9646185\n-42.18243278\n1.1755574\n-0.81296372\n⋯\n0.7278599\n1.842843\n-17.41462\n56.818181\n1.251827572\n2.3360857\n-3.94616485\n-5.36159\n9.265935\n4.466258\n\n\n136\n-1021.314405\n-98.7238942\n-51.7666548\n-56.12785266\n86.26860717\n-69.7897065\n12.3052320\n-99.70307559\n-398.7149637\n-12.68762511\n⋯\n-112.3714441\n-170.273003\n-1025.43441\n859.036920\n0.671894044\n-11.2785510\n166.10599705\n2907.17181\n-2241.137696\n-30.219180\n\n\n137\n-92.295301\n-14.4984765\n-24.8568219\n-278.60030620\n6.25216339\n-20.4906498\n-27.2341800\n-56.33746948\n-29.0934096\n-2.40359503\n⋯\n55.9666164\n-18.999577\n-124.04694\n-330.979265\n-2.702248413\n8.6784197\n4.80610349\n-230.31114\n253.179079\n-53.842602\n\n\n138\n29.112456\n6.2840519\n-5.7148507\n3.75601069\n0.04182485\n4.2364066\n-65.8123590\n0.07014357\n3.3992650\n-4.64612042\n⋯\n15.1197676\n14.670682\n76.52154\n-53.301021\n-0.003368986\n1.0360532\n0.07268611\n-260.94394\n207.224587\n2.462129\n\n\n139\n-190.786378\n-9.3587049\n66.6020636\n-25.18837097\n-6.26675334\n2.4747330\n-8.4662958\n-14.49620417\n-12.9474269\n-2.39101898\n⋯\n5.1460866\n-25.033125\n-101.37708\n-144.120610\n0.299818551\n0.7977554\n-6.17979398\n203.10201\n-126.714954\n-5.314948\n\n\n140\n330.859345\n-129.9659431\n5.5210239\n38.42625308\n16.24328158\n6.4256437\n8.1633331\n40.90311030\n14.3955041\n-14.14857137\n⋯\n-99.0579197\n-16.321465\n-210.89079\n46.916450\n-0.096039739\n5.2979184\n11.32560912\n1198.31423\n-1080.417830\n9.515088\n\n\n141\n253.134566\n880.4882114\n-163.4244807\n-234.81429533\n-100.04673237\n-150.4922696\n-113.0972300\n-127.79823095\n-64.0017849\n-91.56752375\n⋯\n78.9704299\n137.112838\n876.59884\n-1425.273784\n8.486629115\n48.6511587\n206.46151822\n-2561.23913\n1998.559776\n-43.022044\n\n\n142\n-1745.948637\n-41.2439140\n-83.2341799\n37.23970487\n-432.66316954\n-160.0491324\n-125.5077136\n-8.75004942\n-80.0299850\n-3.83491779\n⋯\n50.5009435\n27.191511\n183.05972\n453.550421\n26.675238798\n86.9148870\n-31.31653430\n-1106.89153\n813.977068\n-72.669730\n\n\n143\n398.622344\n-8.1922737\n-251.4832083\n18.17004758\n6.92811163\n3.0755669\n39.7296215\n-49.12701050\n-0.4386896\n-12.41087106\n⋯\n82.8326631\n90.150448\n463.56521\n215.575350\n-2.729174419\n-11.6307711\n8.21053100\n-1612.08273\n1294.803945\n43.402582\n\n\n144\n495.109581\n76.7131861\n155.8523185\n1616.63051301\n-10.08254805\n36.0587687\n-91.7614033\n78.99475089\n14.5362941\n85.33661159\n⋯\n59.8647807\n99.175192\n569.12249\n-73.580625\n-0.754967183\n51.2928725\n-349.38696081\n-2112.03120\n1610.780190\n-224.678098\n\n\n145\n-112.772205\n1.4492315\n3.5464765\n-1.57302163\n5.05021181\n-0.8185177\n-8.1049185\n3.97037910\n-61.0929670\n2.21886797\n⋯\n-11.7382865\n-27.246351\n-143.15998\n34.480805\n-0.144896684\n-0.5665661\n-3.77722152\n391.58037\n-301.389067\n-1.949472\n\n\n146\n-88.283534\n10.7048817\n13.6928018\n-18.11258396\n8.55892699\n-9.0865772\n-7.5236115\n101.69692060\n14.7320202\n5.25198506\n⋯\n-38.5700253\n-16.820777\n-123.25312\n-70.166772\n1.358316332\n0.2309205\n6.74864314\n590.38499\n-498.090707\n-7.773714\n\n\n147\n-403.212718\n642.6271261\n77.4380182\n46.53256248\n-17.00959592\n-0.2314690\n-93.2181807\n87.27347550\n2.3559446\n55.24649827\n⋯\n-103.5916356\n-72.518469\n-510.22809\n-189.573987\n1.980178299\n10.1587969\n-50.23659840\n1758.70884\n-1436.854184\n-185.393703\n\n\n148\n-594.893674\n73.1212522\n-61.2223178\n-13.27564523\n19.76349388\n-15.9823549\n-51.5921980\n-16.02328613\n-319.4843593\n3.24013286\n⋯\n123.4359240\n-6.382326\n77.13897\n-165.830729\n0.151546512\n-8.2257977\n129.99264287\n-986.94052\n954.761274\n-21.998743\n\n\n149\n1023.282469\n-665.0285166\n-730.9607809\n-749.49772609\n-736.59755244\n-722.8070736\n-807.9038628\n-758.16970626\n-728.9877714\n0.02174726\n⋯\n148.3244343\n64.500200\n454.34350\n-1376.032904\n-1.702162127\n12.2428383\n45.45495230\n-2115.20826\n1793.213430\n-199.236016\n\n\n150\n1558.029347\n-154.1881035\n120.0579567\n16.24256690\n11.15542693\n130.1763384\n1270.3614170\n-34.86162805\n-40.3051543\n5.84729410\n⋯\n-186.0803631\n-85.084341\n-508.60537\n133.482101\n-16.058363346\n-17.4083172\n-1.28801067\n2502.81290\n-2107.631361\n-39.682898\n\n\n151\n-5753.348399\n-283.1742933\n-81.9610343\n-433.54419097\n46.25744706\n-222.4814789\n58.8772124\n622.60953892\n-315.4128362\n-95.29155972\n⋯\n-64.4745392\n-250.327195\n-1958.69186\n2183.369837\n20.492713471\n-19.3700573\n51.38129846\n4373.76074\n-3069.737778\n-182.814538\n\n\n152\n-1051.184047\n1111.5842587\n236.2877293\n-152.92290331\n27.40101114\n15.2258545\n-72.6077141\n-71.66937291\n-1.3939597\n105.33035663\n⋯\n61.6526277\n39.901618\n-122.85567\n-939.469987\n4.589945993\n-6.3958662\n-372.67379036\n-20.64059\n88.881614\n-478.220047\n\n\n\n\n\n    $sigma\n        117906.6540203489217887.0601846758315099.3420379582417884.5571388776517822.4649458087617757.0504364998717089.6514954561817736.0113562899917448.80305531431017913.02630703711117891.02511680661217834.50625022971317347.81904781891417595.48862917271517569.97777274931617863.18444404971717821.17354533171817743.50825304221917886.348458632017883.4525311892117341.47254373892217809.03532692317913.1808794432417899.71759359822517848.03483903822617872.65171244822717864.47065185022817880.28042213792917910.56588151213017789.40666809593117679.85830935863217888.20058159483317853.48505826873417846.28942827913517714.95915542083617832.84267081563717891.62385050733817910.27360863413917743.68177345054017737.54173610244117819.60510088034217901.13556806324317888.53385079984417871.99733154584517571.30571795994617871.99396634964717898.56504433214817895.27069438314917760.36529289315017902.85514492125117912.44588143635217443.40714686595317911.51216334115417909.45357117285517898.93078232865617895.16831352315717911.13126501755817908.919110975917902.75366752336017897.45867395786117896.11971021246217901.79769510576317912.40046863886417883.8429382916517911.63559024146617911.77567077186717912.55659070256817889.3136420616917887.52629372377017912.78284515867117886.82165821737217909.58073995017317908.73677971837417889.09962729727517889.29965965587617903.18678593287717910.89660663727817905.20080418897917888.70437552838017893.57640101798117886.49381914048217909.24942558188317898.50380336128417910.62801530228517909.48951452498617901.26965560388717905.59359840228817850.63766221568917898.91173038769017909.78830654589117861.35391230829217896.20282214819317899.21463122329417912.84973810869517902.98878228849617906.94487502519717904.77758202779817802.9677727789917906.778823471710017906.76713799910117850.036253553410217912.307253135810317912.099050060310417906.135812459310517894.699975266810617893.481730111110717865.422641828410817912.709356651910917881.882768945711017901.586226358211117857.932733569111217908.547032434311317912.319115393611417911.825941086811517846.168227582611617905.336833679211717912.729081904611817909.112779127211917912.945119365412017911.108483196812117909.932880216812217639.119959336212317898.21471259612417907.071138964812517907.678381733312617651.309316030912717907.00424588912817903.707210687512917905.456096342813017903.611980326913117906.558143356413217913.149873329813317909.382235824513417861.838002592913517913.079603641513617904.304602484213717911.095644954313817913.047167200113917912.929478050814017912.148625818914117839.162792587514217901.811101855514317910.555038660314417839.634191517814517912.900495229914617912.565749659414717889.616774606314817903.511790432114917893.596369185515017870.04546453115117808.670461434515217858.7205376198\n\n    $wt.res\n        14547.7336192947829397.4956124771388407.6711652987410415.1793361701518522.9256009572623313.04055873457-53633.6431981562825054.21002751699-40904.508671742510-799.146684744017119221.6727443881212-16302.065586654513-46881.434349454714-33846.451607187915-36906.701047684916-13978.345400837417-18255.05972337931825414.307345272219-9945.46036856484208064.67995245582142639.299604665822-20276.318417367623234.224150505587247357.8791112106725-15280.125127358826-11973.131522577527-12798.607799673928-11228.9775756711293107.329699947530-21054.422548754131-28982.697490065932-9602.677719341833-15091.898227806334-16386.668598064935-28417.862618948136-17468.1236414896378731.8100073088538-3196.4279782625239-23719.311086832940-23033.73425479114118975.67076694242-6713.49904733997438547.0743045200844-12703.06154486044535811.01197791144611754.895452714647-7449.4463985873948-7775.25461826454921765.2935705962506380.54360101737511687.597144717445241325.2172142929532449.55133163459543810.5710378226855-7209.47096256423568249.0978996137157-2859.24208038813584010.263202047359-5980.19457857839607725.8889953656661-8184.6609405034262-6384.7664256213631792.441953445746410182.865580843165-2353.21654871309662389.9660740833671609.38702108936689618.738028714536910037.089977063270-1284.729169485437110286.397561769372-3579.77860759976734205.9466588359749789.7534115263475-9503.459282578476-6298.517490206773027.9834049079278-5315.3183965467379-9835.880541394580-8721.6469436552981-9276.67543600023823936.5806561698836477.6103804949184-3135.48129870082853784.1408383193866923.5754635633687-5236.0679176655188-15694.747070759789-5413.8826692118990-3269.460929202299113437.0067415927928160.3380910971393-7476.1470422149294985.47731265708295-5958.6944357198396-4496.3057438234974669.53046496029812931.537714453995011.208499559291004991.26848768361101-15908.5349393203102-1837.119401855691032085.54721345792104-4735.645543850521056552.75454524543106-8401.5629125054510713551.8842289014108-1199.5059075753109-11146.1885015905110-6845.3827575268311114714.1888609504112-4387.256626926481131791.073701743211142373.45855718614115-10560.5979475899116-5208.231784646241171349.09136928341118-3848.8382870097119-999.5550606763211202900.126126289481213533.6844949789412233179.9379422187123-7697.12899838064124-4511.819428578781254576.7670950966812631978.32999834041274982.78653733111286066.32680879122129-5578.264425802931306002.639146003851315052.96184403839132-436.2842019671671333879.49802713941134-13468.6820619341135-686.614938444772136-5499.17802088991137-2719.00790416754138-771.8131685648551391031.90031335901140-2015.7691796237714117207.3547671985142-6392.74556117009143-3185.1848931851714416398.4227199604145-1099.691635912661461575.756253820021479640.81254734795148-6116.235196595391498651.9020457045815012922.125445959415120107.074421493615214052.8622818209\n\n\n\n\n\ninfluence.measures(model2)\n\nInfluence measures of\n     lm(formula = 연봉.2018. ~ ., data = dt) :\n\n       dfb.1_ dfb.팀명KT dfb.팀명LG dfb.팀명NC dfb.팀명SK dfb.팀명두산\n1   -0.022926   0.020606   6.16e-03   1.28e-02   7.03e-02    -1.43e-04\n2   -0.007024  -0.006335   1.05e-01   2.19e-02   1.98e-02     3.77e-04\n3    0.348392  -0.783439  -1.08e+00  -9.87e-01  -1.20e+00    -1.52e+00\n4   -0.013374   0.012806   1.14e-01   6.20e-03   1.20e-02    -7.38e-04\n5   -0.012017   0.033339   1.47e-02   1.11e-02   9.00e-03     3.20e-03\n6    0.054229   0.220339  -5.19e-02   9.67e-03  -1.59e-02    -1.22e-02\n7    0.275261  -0.389590   1.98e-01   5.53e-02   5.35e-02    -3.34e-02\n8    0.120207   0.027262   7.10e-03   3.33e-02  -8.32e-03     3.23e-01\n9   -0.096438  -0.031977   9.99e-02  -9.13e-02  -2.80e-02    -5.71e-01\n10   0.000195   0.010371   1.14e-02   9.87e-03   1.12e-02     9.90e-03\n11   0.024501   0.010364   1.19e-02   9.32e-03  -2.37e-03    -4.01e-03\n12  -0.101593  -0.027999  -2.59e-02   1.46e-02   2.99e-02    -2.41e-01\n13  -0.160404   0.623989   6.24e-01   4.94e-01   6.02e-01     5.86e-01\n14  -0.057362  -0.091681  -1.04e-01  -4.32e-02  -4.04e-02     3.70e-02\n15  -0.008059  -0.089864  -3.45e-02  -6.44e-02  -5.23e-02     1.63e-02\n16  -0.017346  -0.037956  -2.86e-02  -3.14e-02  -1.83e-02    -5.15e-04\n17   0.093561  -0.035729  -2.17e-01  -8.67e-03  -2.29e-02    -1.56e-02\n18   0.046928  -0.044389  -5.22e-02   4.19e-03  -4.18e-02     4.82e-02\n19   0.075050   0.020881  -7.30e-02  -1.32e-04   3.96e-03     3.83e-04\n20  -0.054532  -0.026525  -2.45e-02  -5.46e-02  -1.28e-03     7.36e-03\n21  -0.089075   0.063087   6.47e-03  -7.34e-02   6.30e-02     7.89e-02\n22   0.041105   0.021446   9.00e-03   7.42e-03  -1.96e-01    -7.12e-03\n23  -0.000136  -0.000115  -5.72e-04   3.31e-03   3.14e-05    -2.97e-04\n24   0.013779   0.002759  -5.51e-03   7.11e-03  -4.21e-03     5.44e-03\n25   0.088775  -0.043681  -2.66e-02  -6.62e-03  -1.81e-01     2.44e-02\n26   0.017682  -0.118719   2.88e-02   7.28e-03  -9.19e-03     1.90e-02\n27  -0.015606  -0.022263  -1.19e-02  -1.85e-01  -1.21e-02     2.11e-02\n28  -0.017384  -0.014812   4.80e-03  -1.30e-01   3.38e-03     3.64e-03\n29   0.001482  -0.001553  -6.00e-03   4.20e-02  -3.68e-03     7.04e-03\n30   0.048523  -0.027672  -2.73e-02   7.73e-02  -2.46e-02    -2.54e-01\n31  -0.320376  -0.301160   2.30e-02  -9.09e-02   1.80e-02    -6.99e-02\n32   0.025715  -0.001859   7.76e-03  -2.21e-02  -8.62e-03    -2.40e-03\n33  -0.005311  -0.029511  -1.31e-02  -2.06e-02  -7.24e-03    -1.66e-02\n34   0.017863  -0.015759  -2.46e-02  -1.30e-03  -2.08e-02     1.08e-02\n35  -0.000764   0.036363   2.76e-02  -1.49e-02   2.89e-02    -5.32e-03\n36   0.054772  -0.003522  -1.59e-01   2.99e-02   4.77e-03    -2.78e-02\n37  -0.032835  -0.110457  -1.18e-01  -9.56e-02  -1.15e-01    -1.11e-01\n38  -0.010694  -0.001456   1.14e-02   1.21e-02   7.48e-03    -1.55e-03\n39  -0.087145   0.017071  -2.66e-01  -4.67e-02   9.14e-04    -9.59e-03\n40   0.009530  -0.037652  -5.66e-02  -3.74e-01  -2.15e-02    -1.85e-02\n41   0.031349  -0.051650  -4.86e-02   1.75e-02  -3.12e-02     5.04e-03\n42  -0.048213   0.076360   9.98e-02   7.49e-02   9.05e-02     7.37e-02\n43  -0.042520   0.041313   1.01e-01   9.71e-03   9.81e-03    -1.35e-02\n44   0.041997  -0.004640  -6.71e-03   7.16e-03  -1.01e-02     9.07e-03\n45   0.406510  -0.075242   3.12e-01   2.88e-02  -8.42e-02     6.44e-02\n46  -0.053910  -0.045951  -6.84e-03  -1.38e-02   1.30e-01    -1.91e-02\n47   0.035042   0.006549   1.01e-02   8.44e-04   9.81e-03    -9.24e-02\n48   0.067646   0.004315  -1.41e-02  -1.83e-02  -1.03e-01     2.79e-03\n49   0.063957   0.036333  -3.73e-02   9.19e-02  -8.64e-02     3.40e-01\n50  -0.024285  -0.004843   3.15e-03   9.25e-03   4.00e-03    -3.20e-03\n51   0.000504  -0.000238   1.38e-03   2.19e-02   3.43e-03    -1.35e-03\n52   0.091024  -0.045568  -9.54e-02  -2.23e-02  -6.31e-02     7.70e-02\n53  -0.008857  -0.003444  -3.76e-03  -6.47e-03  -8.38e-04     2.64e-02\n54  -0.006932  -0.009428  -5.65e-03  -8.12e-03   4.02e-02    -3.03e-04\n55   0.009093   0.116284   8.59e-02   1.03e-01   8.79e-02     9.68e-02\n56   0.011220   0.009699  -5.75e-03  -1.53e-02   1.20e-02     4.68e-03\n57  -0.007994   0.003786  -2.92e-02   6.01e-05   7.71e-04     6.09e-04\n58   0.000285   0.042380  -1.74e-03   6.31e-03  -2.56e-03    -2.79e-03\n59   0.008408  -0.008265   3.70e-03   2.47e-03   3.37e-03     8.11e-03\n60  -0.012635  -0.024707  -1.04e-02  -2.21e-02  -7.99e-03    -4.50e-03\n61  -0.049517   0.099350   9.94e-02   8.68e-02   1.00e-01     8.18e-02\n62   0.062103  -0.015566   1.13e-03   3.37e-03  -1.77e-02     4.44e-03\n63   0.007787   0.017072   7.49e-04   2.05e-03  -5.22e-04     3.13e-03\n64  -0.033691   0.057774  -1.84e-02  -1.92e-02  -1.16e-02     4.75e-03\n65   0.016013   0.010197   9.60e-04   8.60e-03  -3.94e-05     2.84e-03\n66  -0.014620  -0.003171   2.41e-02  -3.09e-03   3.91e-03    -3.71e-03\n67  -0.005305   0.010897  -1.58e-03  -4.76e-03  -1.22e-03     7.78e-04\n68   0.006706  -0.003586   1.23e-02   2.13e-03   9.50e-02     7.16e-03\n69  -0.031523  -0.007001  -4.26e-03  -9.53e-03   1.07e-01    -1.76e-02\n70  -0.006162  -0.002657  -1.08e-03  -4.23e-03  -2.00e-03    -1.10e-03\n71  -0.017653  -0.013326   9.48e-02  -1.63e-02   8.81e-03    -5.82e-03\n72   0.002912  -0.005460  -6.89e-04  -1.08e-02   2.76e-03    -3.23e-03\n73   0.005191   0.038692   1.94e-03   9.56e-03   3.51e-03    -8.05e-04\n74  -0.034814  -0.006259  -1.47e-03  -5.88e-03   1.04e-01    -4.74e-03\n75  -0.031325   0.140722   1.27e-01   1.20e-01   1.34e-01     9.45e-02\n76  -0.027124   0.073127   6.93e-02   6.38e-02   6.64e-02     6.92e-02\n77   0.015650  -0.000885   2.27e-02  -1.35e-03  -4.93e-03     7.22e-04\n78  -0.032079  -0.022932  -5.90e-03  -2.78e-03  -5.16e-02     3.05e-03\n79  -0.060957  -0.013845  -1.18e-02  -1.84e-02  -3.66e-03    -2.93e-02\n80   0.014737  -0.021016  -1.36e-02  -1.97e-02  -8.66e-02     8.34e-04\n81  -0.270983  -0.009102   2.70e-02  -3.43e-02  -6.60e-02    -2.80e-02\n82   0.008466  -0.002720  -7.95e-04  -6.50e-04   9.73e-04    -4.53e-04\n83  -0.057206  -0.010555  -1.49e-02  -9.26e-03   1.42e-02     1.13e-03\n84  -0.000782   0.007458  -3.08e-02   4.02e-03   2.01e-05     1.06e-04\n85   0.004650  -0.044358  -4.65e-02  -3.69e-02  -4.16e-02    -5.17e-02\n86  -0.027375  -0.003043   5.31e-02  -1.31e-02  -3.75e-03    -7.43e-03\n87  -0.017976  -0.011762  -1.80e-02  -2.61e-02  -2.31e-02    -6.09e-03\n88  -0.064968   0.023977   1.19e-02   4.65e-02   2.23e-02     6.05e-03\n89  -0.226454  -0.035068  -5.30e-02  -3.14e-02   5.01e-02    -1.18e-03\n90   0.014097   0.018150   1.29e-02   1.49e-02   4.15e-03     1.86e-02\n91   0.097817  -0.002872  -5.46e-02  -2.45e-02   1.06e-01    -1.69e-02\n92  -0.015414  -0.011112  -9.45e-03   1.01e-01  -3.89e-03    -1.68e-02\n93  -0.036223   0.003334   4.62e-04  -7.78e-03   5.42e-03     4.03e-03\n94   0.001385  -0.000112  -3.03e-03  -5.23e-03   2.65e-04     1.19e-02\n95  -0.096377   0.071904   5.91e-02   4.50e-02   6.43e-02     6.23e-02\n96   0.018624  -0.041578   8.20e-03   1.51e-03   6.93e-03     3.37e-02\n97   0.017663   0.069924   2.19e-02   1.24e-02   1.59e-02     5.61e-02\n98  -0.185955   0.027450   1.93e-01   2.65e-01   1.72e-01     2.21e-01\n99  -0.001512  -0.012467  -8.47e-03  -1.33e-02  -7.15e-03     6.45e-02\n100 -0.003515  -0.008462  -6.01e-03   5.86e-02  -5.24e-03     1.45e-03\n101 -0.094192   0.002876   7.96e-04   1.17e-02   2.99e-03     1.15e-02\n102 -0.010149   0.001962  -4.41e-03  -4.61e-03  -2.12e-02     1.04e-05\n103  0.003916  -0.000934   2.79e-03   2.45e-02   2.10e-03     1.64e-03\n104 -0.004561  -0.004325   3.05e-03   7.18e-03  -1.05e-03    -3.73e-02\n105 -0.092550   0.017766   2.63e-02   4.18e-02   2.93e-02     9.59e-02\n106  0.000833   0.011672   8.92e-03  -2.68e-03   2.01e-02     8.67e-03\n107 -0.039583  -0.008677  -1.19e-02   1.47e-01   1.38e-04    -1.41e-02\n108  0.020107  -0.004696  -9.37e-04  -1.29e-04  -1.13e-03     3.56e-03\n109  0.060305   0.024186   2.36e-02   3.52e-02   1.60e-02     1.38e-02\n110 -0.015632  -0.015663  -1.37e-02  -1.65e-02  -1.03e-02    -5.27e-04\n111 -0.030442  -0.017239  -2.97e-02  -1.64e-02   1.23e-01     2.61e-03\n112  0.000623  -0.002578  -4.92e-03  -1.68e-03  -2.36e-03     5.25e-04\n113  0.022930   0.004442  -1.76e-03   2.04e-03  -1.95e-03     4.60e-03\n114 -0.004953   0.000527   1.76e-03   2.74e-04   2.34e-02     1.20e-03\n115  0.221130   0.175119   1.19e-01   2.31e-01   1.76e-01     1.50e-01\n116  0.066855   0.017948   1.14e-02   2.77e-02  -4.25e-02     2.20e-03\n117  0.003916  -0.000784  -8.51e-04  -1.71e-03  -1.16e-03     2.25e-03\n118  0.013705  -0.012557  -6.20e-02  -1.26e-02  -1.66e-02     6.75e-04\n119  0.000476  -0.008820   4.81e-05   1.69e-03   5.50e-04     9.28e-04\n120 -0.001290   0.026339  -9.73e-05  -5.32e-03  -1.33e-03    -2.74e-03\n121  0.010176   0.044036   1.65e-02   1.67e-02   1.34e-02    -1.07e-03\n122  0.213848  -0.001271   4.01e-03   6.29e-02  -2.17e-02     5.31e-03\n123  0.021273   0.009314   3.54e-03   5.60e-03   4.56e-03     9.49e-03\n124  0.078336  -0.004533  -8.90e-03  -8.96e-03  -6.46e-03     6.28e-03\n125  0.017292  -0.003433   2.79e-03  -3.25e-03   1.14e-04     6.60e-02\n126 -0.055356  -0.004152  -3.82e-02  -5.65e-02  -4.22e-02    -8.55e-03\n127  0.000755   0.052298   1.21e-02   3.27e-03   8.21e-03    -3.08e-03\n128  0.043922  -0.002813   2.03e-03  -6.39e-05  -4.37e-03     8.35e-02\n129 -0.028372   0.001454  -1.97e-03  -6.48e-03   1.65e-03    -2.88e-03\n130 -0.002214  -0.091151  -9.03e-02  -6.75e-02  -8.76e-02    -7.20e-02\n131 -0.013299  -0.015278  -6.15e-03  -1.69e-02  -5.92e-03     6.09e-02\n132  0.000234   0.000140   2.20e-05  -4.85e-03   5.74e-04    -6.46e-04\n133 -0.015268  -0.004009   4.65e-03  -6.69e-05   1.33e-03    -9.93e-03\n134  0.012109   0.153892   1.40e-01   1.64e-01   1.80e-01     1.84e-01\n135 -0.004009   0.000807   2.74e-05  -1.01e-05   7.41e-04    -2.14e-03\n136 -0.027147  -0.014974  -7.76e-03  -7.67e-03   1.32e-02    -1.01e-02\n137 -0.002452  -0.002198  -3.73e-03  -3.80e-02   9.56e-04    -2.97e-03\n138  0.000773   0.000953  -8.57e-04   5.13e-04   6.39e-06     6.15e-04\n139 -0.005069  -0.001419   9.98e-03  -3.44e-03  -9.58e-04     3.59e-04\n140  0.008790  -0.019703   8.28e-04   5.25e-03   2.48e-03     9.33e-04\n141  0.006753   0.134032  -2.46e-02  -3.22e-02  -1.54e-02    -2.19e-02\n142 -0.046414  -0.006256  -1.25e-02   5.09e-03  -6.62e-02    -2.32e-02\n143  0.010592  -0.001242  -3.77e-02   2.48e-03   1.06e-03     4.46e-04\n144  0.013208   0.011677   2.35e-02   2.22e-01  -1.55e-03     5.26e-03\n145 -0.002996   0.000220   5.32e-04  -2.15e-04   7.72e-04    -1.19e-04\n146 -0.002345   0.001623   2.05e-03  -2.47e-03   1.31e-03    -1.32e-03\n147 -0.010726   0.097548   1.16e-02   6.36e-03  -2.60e-03    -3.36e-05\n148 -0.015813   0.011091  -9.18e-03  -1.81e-03   3.02e-03    -2.32e-03\n149  0.027215  -0.100926  -1.10e-01  -1.02e-01  -1.13e-01    -1.05e-01\n150  0.041492  -0.023431   1.80e-02   2.22e-03   1.71e-03     1.89e-02\n151 -0.153745  -0.043180  -1.24e-02  -5.95e-02   7.11e-03    -3.25e-02\n152 -0.028012   0.169025   3.55e-02  -2.09e-02   4.20e-03     2.22e-03\n    dfb.팀명롯데 dfb.팀명삼성 dfb.팀명한화    dfb.승    dfb.패    dfb.세\n1       0.009233     1.62e-02     2.67e-02  3.74e-02 -3.40e-02 -6.95e-03\n2       0.029441     1.16e-02     2.10e-02 -8.32e-02  9.49e-02 -6.82e-02\n3      -1.378418    -8.11e-01    -7.67e-01  2.92e+00 -1.39e+00  6.34e-01\n4       0.002032     1.80e-02     2.26e-02 -7.64e-02 -5.17e-02 -2.77e-02\n5       0.239648     8.49e-03     3.36e-02 -4.26e-03 -8.09e-02  1.06e-02\n6      -0.014072    -4.59e-03     8.88e-03 -3.02e-01  2.56e-01 -2.51e-01\n7      -0.039783     1.81e-01     1.36e-01  7.85e-02 -6.48e-01  1.49e-03\n8      -0.005425     1.96e-02     3.85e-02  7.61e-02  2.02e-01 -1.12e-01\n9      -0.004067    -3.38e-02    -7.77e-02  2.22e-01 -1.89e-01  2.99e-01\n10      0.010969     1.02e-02     1.07e-02  1.02e-02  3.84e-03  2.12e-03\n11     -0.009146     1.02e-01     1.27e-02  3.18e-02  9.35e-03  5.30e-03\n12      0.011161    -2.33e-02    -1.30e-02  6.75e-02  2.41e-01 -1.74e-01\n13      0.575851     5.35e-01     5.93e-01  8.75e-02 -1.95e-01  1.92e-01\n14     -0.431867    -6.38e-02    -7.72e-02 -7.60e-02  1.05e-01 -2.12e-02\n15      0.023064    -4.47e-01    -1.09e-01 -2.01e-01  1.83e-01  2.90e-02\n16     -0.157978    -3.90e-02    -3.80e-02 -9.77e-02  4.50e-02  3.30e-02\n17      0.000995    -8.40e-04    -1.23e-02 -8.89e-02  2.34e-01 -1.67e-01\n18     -0.024085     2.29e-01    -3.24e-02 -5.77e-03  1.65e-02  9.22e-02\n19      0.015902     2.88e-02     2.55e-02  5.29e-02 -8.23e-02  5.67e-02\n20      0.137969     3.23e-03    -2.74e-02 -1.56e-01 -3.07e-02  4.02e-01\n21     -0.071542     3.61e-02     5.14e-01  4.37e-01  6.33e-03  8.90e-01\n22      0.004378     1.44e-02     2.01e-02  2.99e-02  5.81e-02 -9.62e-02\n23      0.000408    -1.68e-04    -1.84e-04 -2.02e-03  1.75e-03  1.38e-03\n24     -0.009508     9.05e-03     6.54e-02 -2.73e-03 -1.85e-02  7.45e-03\n25      0.033261    -1.67e-03    -1.32e-02 -1.68e-01  4.92e-02 -5.50e-02\n26      0.009667    -3.98e-03    -1.52e-02  2.88e-02 -2.32e-02  9.69e-02\n27     -0.006228    -1.42e-02    -2.47e-02 -1.87e-01 -1.78e-01  8.53e-02\n28      0.004237    -4.46e-03    -1.35e-02  2.76e-02  1.04e-01  4.26e-03\n29     -0.003341    -2.25e-03    -3.60e-03 -1.38e-02 -1.68e-02  6.51e-03\n30     -0.006948     8.44e-03    -1.30e-02 -1.02e-01  8.92e-02 -2.77e-01\n31     -0.021949    -7.73e-02    -4.32e-02 -1.13e-01 -8.46e-02 -2.02e-01\n32     -0.119558     4.32e-03     1.82e-03 -2.32e-02 -5.36e-02  1.82e-02\n33     -0.194309    -1.17e-02    -1.14e-02  1.12e-01  8.08e-02 -2.21e-02\n34      0.002183    -1.57e-02    -1.57e-01 -1.02e-01 -1.80e-02 -2.88e-02\n35      0.016789     1.27e-02    -2.05e-01 -2.91e-02 -1.50e-01  8.72e-02\n36     -0.006467     2.71e-02     1.90e-02 -3.77e-04  2.39e-02 -2.04e-01\n37     -0.115662    -1.18e-01    -1.26e-01  1.25e-01  1.04e-01 -1.47e-02\n38      0.004817    -2.38e-02     2.46e-03  6.15e-03 -2.18e-02 -2.58e-02\n39     -0.048013     2.80e-03    -1.06e-02 -4.35e-01 -3.90e-01  9.55e-02\n40      0.048268    -1.07e-01    -2.33e-02 -5.59e-03  1.30e-01 -5.41e-01\n41     -0.008150    -3.36e-03     1.32e-01 -2.31e-01  7.66e-02 -9.49e-02\n42      0.085163     6.92e-02     7.81e-02  3.60e-02  5.21e-02 -2.12e-02\n43     -0.025771     5.69e-02     2.87e-02 -2.64e-02 -1.05e-01  2.80e-02\n44      0.000546    -9.34e-03    -1.22e-01  2.09e-02  6.34e-02  7.47e-02\n45      0.022151     6.01e-03    -3.41e-02 -2.57e-03  3.70e-01 -2.15e-02\n46     -0.014733    -3.75e-02    -4.87e-02 -1.11e-01  1.70e-01 -1.38e-02\n47      0.013744     2.36e-03     1.22e-02  2.71e-02  4.17e-02  5.03e-02\n48      0.000401    -3.77e-03     5.14e-03  5.99e-02 -1.51e-02  1.20e-01\n49     -0.087051     1.26e-01     1.07e-02 -2.98e-01 -2.14e-01 -3.41e-01\n50     -0.003382     6.93e-02    -9.47e-04  2.92e-03  1.38e-02 -3.90e-02\n51      0.003364    -2.96e-03     1.21e-03 -2.93e-03  3.25e-03 -1.10e-02\n52      0.579514    -1.06e-01    -8.18e-02 -2.45e-01  1.34e-01  9.64e-02\n53     -0.001511    -3.04e-03    -2.62e-03  2.34e-02  1.86e-02 -2.81e-03\n54      0.005150    -1.34e-02    -5.87e-03  1.58e-03  3.02e-02  2.58e-03\n55      0.078032     1.10e-01     1.12e-01  5.31e-02  6.71e-03 -4.08e-03\n56      0.000498    -9.47e-03     8.79e-02  1.67e-02 -5.05e-03 -1.57e-02\n57      0.000483     1.14e-03     2.59e-03 -6.30e-03 -1.51e-02 -1.51e-03\n58     -0.004085     5.52e-03     2.21e-03 -4.20e-02 -1.44e-02 -4.82e-02\n59     -0.073295    -3.17e-03    -8.87e-03 -7.08e-02  2.14e-02  6.31e-02\n60     -0.013516     5.89e-02    -1.31e-02 -7.04e-02  1.75e-02 -8.33e-02\n61      0.087411     1.01e-01     9.90e-02  4.65e-02  3.28e-02  2.08e-02\n62     -0.003964     1.77e-02    -5.97e-02 -2.56e-02  1.30e-02 -1.91e-02\n63     -0.000713     3.94e-03     1.82e-03  1.17e-02 -3.94e-03  1.66e-03\n64      0.004667    -4.21e-02    -4.56e-02 -2.09e-02  1.23e-01  5.95e-02\n65     -0.004333    -1.00e-02     9.33e-03 -2.29e-04 -3.14e-02 -7.66e-03\n66      0.000359    -3.52e-03    -2.41e-03 -6.35e-03 -2.20e-03 -6.59e-03\n67     -0.000325    -3.82e-03    -3.16e-03 -4.63e-03 -8.07e-04  2.86e-03\n68      0.006226    -3.01e-04    -7.57e-04  1.06e-01  4.16e-02 -2.54e-02\n69     -0.012955    -6.58e-03    -5.23e-03 -7.31e-02 -4.64e-03 -4.94e-02\n70     -0.001750    -1.34e-02    -2.88e-03  2.67e-03  3.42e-03 -3.81e-05\n71     -0.004961    -1.98e-02    -1.19e-02 -4.21e-03 -4.47e-03 -1.11e-02\n72     -0.047260    -2.41e-03    -7.89e-04  1.25e-02 -1.19e-02  5.41e-02\n73      0.004467    -1.65e-03     5.46e-04  5.44e-03  4.34e-03  2.24e-03\n74      0.013173    -2.86e-02    -1.08e-02 -1.77e-02 -1.11e-02  8.58e-03\n75      0.114156     1.09e-01     1.32e-01  2.73e-02 -5.05e-03 -1.69e-02\n76      0.060837     7.64e-02     7.30e-02 -9.19e-03 -1.32e-02 -1.31e-02\n77     -0.004286     1.49e-03    -6.49e-05 -7.28e-03 -1.37e-02 -5.02e-03\n78      0.008765    -1.91e-02    -1.20e-02 -3.63e-04  2.83e-02 -3.71e-02\n79     -0.008887    -1.10e-01    -1.84e-02 -1.32e-02  5.80e-02 -4.22e-02\n80      0.004413    -2.87e-02    -1.15e-02 -4.73e-02  2.72e-02 -3.14e-03\n81     -0.002156    -4.51e-02    -3.12e-02  1.13e-02  4.85e-02  2.28e-03\n82      0.047693     1.02e-03     9.42e-04  6.25e-03 -5.54e-03 -5.61e-03\n83     -0.013003     4.82e-02    -2.34e-02  3.33e-02  1.32e-02  6.53e-03\n84      0.001901     7.53e-03     6.90e-03 -7.71e-03 -4.46e-05 -4.82e-03\n85     -0.043143    -4.23e-02    -4.63e-02 -1.63e-02  2.79e-03 -2.04e-03\n86     -0.007360    -1.39e-02    -7.40e-03 -1.84e-02 -2.72e-02 -8.95e-03\n87     -0.018623    -8.55e-03    -6.41e-02 -1.77e-02  2.19e-05 -1.25e-03\n88      0.021526     8.60e-03    -1.23e-01  7.54e-02  7.76e-02 -2.51e-02\n89     -0.004000    -1.04e-02    -1.78e-02 -1.45e-02  5.29e-02 -3.14e-02\n90     -0.031702     1.15e-02     1.53e-02 -8.39e-03 -1.21e-02  1.97e-03\n91     -0.008281    -3.32e-02    -6.09e-03 -1.42e-01 -8.98e-02 -4.16e-02\n92     -0.005100    -1.22e-02    -1.38e-02 -7.01e-03  3.96e-02 -3.73e-02\n93      0.001431    -6.91e-02     3.15e-04 -1.32e-03  2.13e-02 -1.15e-03\n94     -0.002998    -6.58e-03    -1.05e-03  1.61e-03  2.31e-03  2.45e-03\n95      0.049060     6.94e-02     6.55e-02 -2.90e-03 -9.48e-03 -1.37e-02\n96      0.018122    -1.37e-02     8.04e-03 -2.34e-02  1.38e-02 -3.14e-03\n97      0.015846     1.19e-02     1.51e-02  1.63e-02 -1.67e-02  1.27e-02\n98      0.080352     2.07e-01     1.22e-01 -1.34e-03 -2.32e-02 -7.77e-02\n99     -0.002660    -1.87e-02    -1.30e-02 -1.12e-02 -5.21e-03  6.02e-03\n100    -0.002320    -1.13e-02    -1.01e-02  4.49e-04  4.94e-03 -8.42e-03\n101     0.011926    -2.15e-02    -1.52e-01  2.65e-02  5.47e-02  1.86e-02\n102    -0.003266    -5.93e-03    -2.01e-03 -1.74e-03  4.52e-04 -7.53e-04\n103     0.002428    -1.04e-03     5.27e-04  1.09e-03  6.25e-03 -3.36e-03\n104     0.004321     4.83e-03    -2.82e-03 -7.34e-03 -7.88e-03 -9.84e-03\n105     0.053140     1.64e-02    -1.17e-03  1.87e-02  5.65e-02  2.03e-02\n106     0.010466    -6.98e-02     1.53e-02 -1.12e-02  2.83e-02 -1.49e-02\n107    -0.025461    -4.29e-03    -3.65e-03 -3.79e-02 -3.90e-02 -8.79e-02\n108    -0.003759    -5.15e-03     9.97e-04 -1.76e-03 -2.78e-04 -2.98e-03\n109     0.024666     2.67e-02    -6.90e-02  7.56e-03  1.40e-02  2.43e-02\n110    -0.007036    -7.47e-02    -1.70e-02 -2.68e-02  8.18e-03 -6.17e-03\n111    -0.009083    -1.44e-02    -2.53e-02 -3.17e-03  1.29e-01  5.68e-02\n112    -0.003488    -4.01e-02    -3.84e-03  8.64e-03  1.18e-02  3.94e-03\n113     0.000678     1.66e-02     5.27e-03 -4.67e-03 -1.22e-02 -3.44e-03\n114    -0.000362     8.16e-04    -4.67e-04  1.60e-03 -7.00e-03  3.50e-03\n115     0.149409     2.00e-01     1.32e-01  6.41e-02 -3.00e-02  5.12e-02\n116     0.009193     2.25e-02     2.24e-02 -2.10e-03 -8.00e-03 -6.13e-03\n117     0.016857    -1.70e-03    -2.93e-04 -6.44e-03 -1.28e-03 -4.69e-03\n118    -0.018649    -1.25e-02    -1.41e-02 -1.25e-02 -4.29e-03 -3.71e-03\n119     0.000970    -8.59e-04     3.48e-04 -1.02e-04  3.10e-03 -1.43e-03\n120    -0.000153    -6.37e-03    -1.51e-03 -7.30e-03 -7.40e-03 -8.15e-04\n121     0.014385     1.31e-02     1.46e-02  1.02e-02  3.30e-03  2.13e-04\n122     0.036620     2.59e-02     2.96e-01 -1.52e-01  9.86e-02 -5.19e-02\n123    -0.006767     2.48e-02    -4.96e-02  1.49e-02 -3.55e-02  1.88e-02\n124    -0.007858     4.77e-03    -4.38e-02 -5.39e-03  2.28e-02 -4.61e-03\n125     0.005612     1.77e-03    -2.26e-04 -3.58e-03 -2.01e-02  1.10e-02\n126    -0.043507     1.22e-02     2.61e-01 -2.81e-02 -3.04e-02  6.39e-02\n127     0.008780     2.05e-03     7.47e-03 -1.14e-02 -2.84e-02  5.52e-03\n128     0.009172    -5.70e-03    -1.82e-04  1.00e-02 -2.51e-04  2.25e-02\n129    -0.001111    -8.22e-03    -4.96e-02 -2.08e-02 -2.19e-02 -9.12e-03\n130    -0.071789    -9.32e-02    -9.64e-02  3.91e-03  7.10e-02 -1.98e-02\n131    -0.002915    -1.67e-02    -1.43e-02  2.19e-02  4.77e-02 -4.06e-03\n132    -0.000218     3.71e-04     2.20e-04  7.64e-05  2.45e-04 -6.54e-06\n133     0.003069     3.07e-02    -4.74e-03 -1.35e-02 -5.46e-03  2.15e-03\n134     0.191763     1.63e-01     1.82e-01 -6.18e-02 -1.03e-02 -1.21e-02\n135     0.000139    -6.29e-03     1.80e-04 -7.77e-04 -1.51e-03 -1.60e-03\n136     0.001771    -1.49e-02    -6.11e-02 -1.21e-02  2.45e-02  1.48e-02\n137    -0.003918    -8.39e-03    -4.46e-03 -2.30e-03 -8.90e-03  3.65e-03\n138    -0.009467     1.05e-05     5.21e-04 -4.44e-03 -2.71e-03  1.37e-03\n139    -0.001218    -2.16e-03    -1.98e-03 -2.29e-03 -1.35e-03 -7.88e-04\n140     0.001174     6.09e-03     2.20e-03 -1.35e-02  7.11e-03 -5.12e-03\n141    -0.016337    -1.91e-02    -9.84e-03 -8.79e-02  4.34e-03 -8.88e-02\n142    -0.018066    -1.30e-03    -1.23e-02 -3.67e-03  9.59e-03 -2.00e-02\n143     0.005716    -7.32e-03    -6.72e-05 -1.19e-02  9.13e-03  7.56e-03\n144    -0.013255     1.18e-02     2.24e-03  8.19e-02 -1.68e-01  5.64e-02\n145    -0.001166     5.92e-04    -9.36e-03  2.12e-03 -4.94e-03  9.72e-04\n146    -0.001082     1.52e-02     2.26e-03  5.02e-03 -3.92e-03 -4.12e-03\n147    -0.013428     1.30e-02     3.61e-04  5.29e-02 -2.60e-02  1.17e-02\n148    -0.007426    -2.39e-03    -4.89e-02  3.10e-03 -1.16e-02  1.30e-02\n149    -0.116348    -1.13e-01    -1.12e-01  2.08e-05 -5.14e-02  1.30e-02\n150     0.183189    -5.21e-03    -6.19e-03  5.61e-03  6.98e-02  2.15e-02\n151     0.008519     9.33e-02    -4.86e-02 -9.17e-02  1.21e-01 -8.30e-02\n152    -0.010477    -1.07e-02    -2.14e-04  1.01e-01 -1.35e-01  1.56e-01\n     dfb.홀드  dfb.블론  dfb.경기  dfb.선발  dfb.이닝 dfb.삼진.9 dfb.볼넷.9\n1   -9.66e-03  1.79e-03  0.009598 -1.37e-04 -7.32e-03  -0.005081  -0.006808\n2   -5.77e-02  2.93e-02 -0.010553 -7.22e-02  2.09e-02  -0.067910  -0.042165\n3    3.77e-01 -3.23e-01 -0.297602  7.16e-02 -1.62e-01  -0.411959  -0.023911\n4    8.58e-04  4.74e-02 -0.035435 -3.20e-02  6.12e-02  -0.028449  -0.025890\n5    2.01e-02 -3.71e-03 -0.107680 -9.99e-02  1.41e-01   0.073717   0.052048\n6   -9.86e-02  1.04e-01  0.007422 -5.48e-02 -4.68e-02   0.011313   0.018227\n7    3.69e-02  1.38e-01  0.120403  2.38e-01 -7.10e-02  -0.251230   0.023948\n8   -7.32e-02  7.74e-03 -0.107786 -1.14e-01 -5.99e-03   0.094770   0.079082\n9    2.19e-01 -4.35e-02 -0.543359 -5.21e-01  6.09e-01  -0.011381  -0.054863\n10   7.14e-05 -2.59e-03  0.000688  8.79e-06 -4.52e-03  -0.004923  -0.002360\n11   2.12e-02  1.01e-02 -0.065370 -4.38e-02  4.70e-02  -0.002978   0.013266\n12  -1.85e-01 -9.35e-03  0.287263  1.58e-01 -3.51e-01  -0.110828  -0.125839\n13   1.40e-01 -1.46e-02 -0.030526  2.26e-02  9.47e-02  -0.005014   0.004926\n14  -8.81e-02  3.27e-02  0.253789  1.68e-01 -2.41e-01   0.064769  -0.014635\n15  -4.16e-02  1.95e-02 -0.202360 -2.40e-01  2.37e-01   0.171346   0.058475\n16   6.31e-03 -3.09e-02 -0.021347 -6.37e-02  6.21e-02   0.035919   0.011610\n17  -1.00e-01  1.11e-02  0.022768 -1.02e-01 -6.80e-02  -0.009791   0.046420\n18   4.12e-02  1.86e-03  0.041607  1.44e-01 -5.73e-02   0.038495   0.039484\n19   6.23e-02 -1.17e-02 -0.112841 -1.15e-01  1.20e-01  -0.033323   0.035361\n20   3.08e-02 -9.96e-02 -0.002912  1.67e-02  3.27e-02  -0.018229  -0.038510\n21  -1.89e-01 -1.48e-01  0.032255  2.71e-02 -1.67e-01   0.069127  -0.091856\n22  -4.17e-02  4.79e-02  0.015577 -4.16e-02 -5.54e-02   0.015557   0.031133\n23   5.77e-03 -3.22e-03 -0.001842 -1.78e-03  1.57e-03  -0.001149  -0.000830\n24   9.63e-03  8.12e-04  0.030584  6.37e-02 -3.81e-02   0.005642   0.010570\n25  -4.37e-02  6.26e-02 -0.020133 -9.96e-02  6.13e-02  -0.021095   0.049965\n26   1.73e-01  7.24e-02 -0.260953 -9.95e-02  1.65e-01   0.014035  -0.001726\n27  -4.26e-02  6.34e-02  0.068478  1.45e-01 -5.32e-03   0.032457   0.010717\n28   1.35e-02  1.39e-02 -0.018712  2.08e-02 -6.13e-02  -0.050086  -0.060343\n29   3.31e-03  7.41e-03  0.013753  3.41e-02 -9.60e-03   0.016184   0.011848\n30  -2.11e-01  2.97e-01  0.141736  2.10e-01 -2.29e-01   0.056822   0.080053\n31   1.37e-01 -1.15e-01 -0.068435 -9.08e-02  1.82e-01  -0.039865  -0.200992\n32   3.15e-02  2.45e-02 -0.109416 -1.33e-01  1.35e-01   0.008524   0.009079\n33  -1.47e-01 -8.81e-02  0.089221  1.22e-02 -9.82e-02   0.015740  -0.004726\n34  -5.48e-03  9.06e-02 -0.003453  7.46e-03 -1.14e-02   0.073861   0.032797\n35   6.90e-02 -8.37e-02 -0.005870 -3.04e-02  8.51e-02  -0.042296   0.010335\n36  -1.56e-01  7.06e-02  0.052950  2.66e-02 -6.53e-02  -0.053227   0.027242\n37  -1.16e-02  4.51e-02 -0.011026 -1.79e-02 -5.12e-02   0.026641  -0.023300\n38  -4.42e-02  2.94e-02  0.027316  2.45e-02 -2.30e-02  -0.029630  -0.028461\n39   4.13e-01  1.14e-01 -0.142143  1.63e-01  1.25e-01  -0.102267  -0.190471\n40   1.54e-02 -1.53e-01  0.060626 -4.53e-02 -4.26e-02   0.123916   0.057948\n41  -7.73e-02  9.11e-02  0.103787  9.91e-02 -4.76e-02  -0.004998   0.057323\n42  -5.26e-02  2.98e-02 -0.045823 -6.37e-02  3.06e-02  -0.013569  -0.042520\n43   2.31e-01 -2.47e-02  0.117610  1.99e-01 -1.47e-01  -0.093306  -0.066656\n44   1.07e-01 -3.46e-02 -0.068669  2.46e-02 -2.74e-02   0.022081   0.007298\n45  -1.28e-01  6.99e-02 -0.196641 -2.82e-01  9.05e-02   0.137899   0.269054\n46  -2.76e-02 -4.57e-02 -0.010158 -1.66e-02  3.33e-02  -0.066277  -0.071676\n47   2.01e-02 -8.44e-02 -0.036235 -4.27e-02  2.72e-02  -0.030520   0.020730\n48   9.19e-02 -1.82e-01 -0.021247  8.59e-03 -2.15e-03   0.075907   0.088361\n49  -5.90e-02  7.27e-01  0.197581  3.17e-01 -2.08e-01   0.141735   0.140256\n50  -1.84e-02  2.95e-02  0.047437  2.83e-02 -3.78e-02  -0.052406  -0.041920\n51  -1.64e-02 -2.98e-03  0.008583 -4.79e-03  2.74e-04  -0.006300  -0.004259\n52   4.97e-01 -4.43e-02 -0.353728 -2.03e-01  2.51e-01   0.256381   0.148256\n53  -5.74e-03 -3.68e-02  0.021868 -2.30e-03 -1.36e-02   0.001350  -0.003489\n54  -1.95e-02 -4.51e-02  0.017613 -1.39e-02 -2.38e-03  -0.002413  -0.007962\n55   3.25e-02  3.19e-02  0.044948  1.01e-01 -1.15e-01   0.032136   0.035848\n56  -2.29e-02 -7.85e-02  0.083882  3.88e-02 -6.15e-02  -0.019065  -0.020740\n57   5.73e-03  7.49e-03  0.005525  5.10e-03 -3.56e-03   0.008450   0.003133\n58  -6.80e-03  6.13e-02  0.020902  1.61e-02 -8.65e-03  -0.007686  -0.004613\n59   8.41e-02  1.97e-02 -0.103351 -3.40e-02  5.97e-02  -0.035385  -0.031510\n60  -8.99e-02  2.52e-02  0.041265 -3.12e-02  1.39e-02  -0.008965  -0.021885\n61   2.50e-02 -2.92e-02  0.003804  1.63e-02 -3.88e-02  -0.019082  -0.029108\n62  -4.58e-02  3.48e-02  0.002519 -1.89e-02  1.90e-02   0.000959   0.036888\n63   5.47e-03  3.71e-03 -0.000116  8.42e-03 -9.13e-03  -0.002480  -0.000124\n64   6.16e-03 -6.79e-02 -0.035000 -3.89e-02  6.33e-02  -0.009154  -0.009819\n65   8.10e-03  2.46e-02  0.019323  4.34e-02 -3.82e-02   0.002930   0.005469\n66  -1.03e-02 -3.20e-03  0.004149 -5.50e-03  4.31e-03  -0.013093  -0.018374\n67   3.16e-03 -5.88e-03 -0.007017 -1.00e-02  1.11e-02   0.005513  -0.001993\n68  -2.48e-02  1.71e-02 -0.042042 -6.91e-02  2.68e-02  -0.007141  -0.016303\n69  -2.30e-02  3.91e-02  0.031698  1.91e-02 -1.62e-02  -0.059816  -0.062974\n70  -1.60e-03 -3.14e-03 -0.005661 -1.10e-02  8.63e-03   0.006587  -0.001315\n71  -1.69e-02 -1.34e-02 -0.017849 -2.01e-02  1.09e-02  -0.028835  -0.056742\n72   1.22e-02 -7.10e-02 -0.001757 -3.03e-04  5.50e-03  -0.008665  -0.007570\n73   8.36e-03  3.43e-03 -0.003108  1.00e-02 -1.47e-02   0.002142  -0.005476\n74  -1.79e-02 -3.85e-02 -0.013535 -3.54e-02  4.07e-02   0.019205  -0.007330\n75  -4.48e-02  1.86e-02  0.015450 -4.22e-03  1.23e-02  -0.014073   0.009666\n76  -2.16e-02  2.55e-02  0.007829  9.38e-04  1.16e-02   0.027056   0.025013\n77  -6.37e-03 -5.68e-03  0.022142  2.27e-02 -1.93e-02   0.012019   0.015502\n78  -9.30e-02 -1.05e-02  0.049238  8.62e-03 -2.56e-02  -0.003977  -0.025520\n79  -4.65e-02 -2.07e-02 -0.002103 -6.88e-02  3.95e-02   0.038175  -0.002860\n80  -3.18e-02 -6.43e-02 -0.025146 -4.23e-02  2.70e-02   0.030566   0.009400\n81  -7.19e-03  5.23e-03  0.022995 -3.35e-02  2.62e-02  -0.139240  -0.276841\n82  -6.22e-03 -1.96e-02  0.023647  1.38e-02 -2.21e-02  -0.010817  -0.008264\n83   5.55e-03 -1.80e-02 -0.044180 -6.98e-03  7.65e-03   0.010362  -0.026853\n84  -3.22e-03  3.06e-05  0.023076  1.39e-02 -8.96e-03  -0.000153   0.019998\n85   5.41e-03 -5.05e-03  0.018981  2.49e-02 -2.35e-02  -0.026428  -0.011396\n86  -1.90e-02 -1.79e-02  0.046838  3.17e-02 -2.47e-02   0.030123   0.002058\n87  -4.50e-03  7.65e-03  0.019541  5.59e-03  3.76e-04   0.032452   0.006681\n88  -3.56e-02  3.66e-02  0.088734  9.21e-02 -1.15e-01  -0.013929  -0.003900\n89  -4.19e-02  2.04e-02  0.097695  2.38e-02 -6.67e-02  -0.246648  -0.396926\n90   4.58e-04  1.41e-02  0.014924  8.89e-03 -1.94e-03   0.005247   0.018853\n91  -1.05e-01 -8.38e-02  0.091995 -2.99e-03  5.71e-02   0.205877   0.259949\n92  -2.62e-02  1.96e-02  0.017156  1.86e-02 -3.97e-02   0.000911  -0.016548\n93  -9.26e-03 -1.99e-02  0.008673 -9.19e-03  1.41e-02   0.006770   0.004751\n94   1.40e-03 -4.23e-03 -0.006160 -1.86e-03  1.74e-03   0.003098   0.000106\n95  -1.59e-02  3.65e-03  0.043861  2.20e-02 -8.80e-03   0.052130   0.008500\n96  -2.24e-02 -5.23e-03  0.003364 -1.35e-02  1.43e-02   0.029937   0.010266\n97   2.20e-02 -1.91e-03 -0.048991 -7.93e-03  9.24e-03   0.036665   0.008060\n98  -5.49e-02  3.65e-02  0.150140  8.94e-02 -3.98e-02  -0.303162  -0.066898\n99  -1.36e-02 -9.13e-03  0.000675 -1.25e-03  5.41e-03   0.033600  -0.002790\n100 -2.49e-03  1.41e-02 -0.015332 -1.81e-03 -5.62e-03   0.029543  -0.003519\n101 -3.53e-03  1.50e-02  0.007638  3.10e-03  7.29e-05   0.062496  -0.001671\n102 -6.70e-04  9.37e-04  0.001909  4.22e-04  3.84e-03   0.022137   0.011416\n103 -3.20e-03  8.15e-04 -0.010480 -1.05e-02  6.34e-03  -0.007811  -0.005311\n104 -1.39e-02  4.28e-03  0.012048  2.12e-03  2.75e-03   0.031307   0.003941\n105  1.83e-02 -1.75e-03 -0.015624 -2.99e-02  4.63e-03  -0.134318  -0.039130\n106 -2.86e-02 -3.25e-02  0.035304  2.07e-04  1.09e-03  -0.091686  -0.017097\n107 -1.09e-01  6.79e-03  0.132423  8.80e-02 -7.17e-02  -0.011204  -0.011244\n108 -2.36e-03 -2.53e-03  0.004659  5.49e-03 -4.77e-03  -0.020711  -0.002773\n109  2.76e-02  8.17e-03 -0.014388 -1.00e-03  1.44e-02  -0.059640   0.023848\n110 -1.78e-02 -1.34e-02 -0.008230 -3.51e-02  4.15e-02   0.040312   0.011591\n111  3.89e-02 -4.00e-02  0.028576  2.28e-02 -6.27e-02   0.035775   0.018661\n112  4.15e-03 -5.26e-03 -0.000459  4.26e-03 -5.77e-03   0.010383   0.000509\n113 -4.07e-03  1.93e-03  0.011001  1.40e-02 -1.16e-02   0.016756   0.019715\n114  6.81e-03  1.50e-03 -0.000366  7.51e-03 -2.63e-03  -0.006827  -0.008167\n115  4.22e-02  6.16e-03 -0.088110 -3.75e-03  2.74e-02  -0.019274  -0.015402\n116 -2.19e-03  3.12e-03  0.013341  1.49e-02 -7.57e-05  -0.056387   0.025337\n117 -4.31e-03  1.45e-03  0.002666 -1.58e-03  3.83e-03   0.002087   0.004565\n118 -4.80e-03 -2.45e-03  0.008580  1.09e-02 -1.75e-03   0.056141   0.026774\n119 -4.01e-03  5.15e-04  0.004448  2.48e-03 -2.95e-03   0.002893   0.001260\n120 -4.67e-06 -1.52e-03 -0.012184 -1.55e-02  1.66e-02   0.010990   0.002676\n121  5.24e-03  1.81e-04 -0.000803 -4.13e-04 -7.11e-03  -0.040595  -0.009851\n122 -1.21e-02  1.21e-01 -0.056951 -6.66e-03 -3.17e-04  -0.036671   0.126021\n123  1.08e-02 -1.86e-02  0.043437  5.83e-02 -3.71e-02  -0.021175   0.013962\n124 -1.89e-03 -6.01e-03 -0.017510 -1.50e-02  6.71e-03  -0.044645   0.008847\n125 -7.27e-03 -8.63e-03  0.006376  6.43e-03  1.28e-03  -0.017024  -0.005655\n126  3.35e-01 -6.55e-02 -0.007428  9.21e-02 -8.29e-02   0.010061  -0.083203\n127  2.08e-03 -3.96e-03 -0.010007 -1.73e-02  2.99e-02  -0.017930  -0.003591\n128  7.22e-03 -7.37e-03 -0.020041 -4.14e-03  5.94e-04   0.010509   0.008713\n129 -3.71e-03  1.50e-02 -0.005512 -1.96e-02  2.33e-02   0.018601  -0.009443\n130 -3.18e-02  4.07e-03  0.017494  1.21e-02 -3.22e-02   0.048167   0.022390\n131 -2.52e-02 -2.49e-02 -0.008474 -2.54e-02 -1.83e-04   0.014770  -0.009589\n132 -4.45e-04 -1.15e-03  0.001884  8.75e-04 -6.83e-04  -0.002731  -0.001138\n133 -3.19e-03  4.25e-04 -0.015513 -2.85e-02  3.42e-02  -0.009882  -0.007559\n134  4.42e-02 -1.28e-01  0.067397  9.58e-02 -9.50e-02   0.001614   0.023935\n135 -1.85e-03 -6.89e-04  0.001370 -2.16e-03  2.59e-03   0.000149   0.000393\n136 -6.21e-02 -8.59e-02  0.072079  4.53e-02 -6.94e-02  -0.023066  -0.036356\n137 -5.02e-03 -9.65e-03  0.004825 -2.43e-03  9.37e-03   0.011484  -0.004055\n138  1.70e-03  8.70e-04  0.001552  3.65e-03 -3.72e-04   0.003102   0.003131\n139  8.95e-04  3.81e-03 -0.004829 -5.28e-03  6.32e-03   0.001056  -0.005342\n140 -9.86e-04  4.95e-03  0.005978  6.38e-03 -5.01e-03  -0.020324  -0.003483\n141 -1.12e-01 -4.30e-02  0.130677  2.77e-02 -3.88e-02   0.016269   0.029383\n142 -1.06e-02  6.80e-03  0.019666  1.09e-02  3.44e-03   0.010368   0.005807\n143 -3.33e-03 -2.69e-02  0.005123  3.66e-03 -5.96e-03   0.016997   0.019242\n144 -2.55e-02 -2.39e-02  0.076604  1.13e-01  1.09e-03   0.012333   0.021252\n145  2.19e-03  2.34e-03 -0.001565 -1.85e-04  1.37e-03  -0.002408  -0.005815\n146 -7.71e-03 -2.82e-03  0.007663  1.54e-03 -3.47e-03  -0.007913  -0.003590\n147  1.65e-02  2.85e-02  0.024618  5.62e-02 -2.57e-02  -0.021281  -0.015497\n148  3.23e-02 -2.18e-02  0.024349  4.91e-02 -5.35e-02   0.025338  -0.001363\n149 -7.48e-03 -1.78e-02  0.051852  7.65e-02 -3.81e-02   0.030464   0.013780\n150 -8.48e-04 -4.06e-02 -0.050651 -5.50e-02  2.17e-02  -0.038269  -0.018202\n151 -1.85e-01 -9.76e-02  0.093755 -1.11e-01  7.82e-02  -0.013305  -0.053736\n152  1.05e-01 -6.95e-02 -0.083537  3.85e-03  1.18e-01   0.012687   0.008541\n    dfb.홈런.9  dfb.BABI  dfb.LOB.   dfb.ERA  dfb.RA9.   dfb.FIP  dfb.kFIP\n1    -0.008195 -0.008859  2.45e-02  1.22e-02 -6.41e-02  6.91e-03 -0.006395\n2    -0.053074 -0.008072 -1.98e-02  3.25e-03  1.36e-02  6.02e-02 -0.061214\n3    -0.091125 -0.088087  2.47e-01  4.60e-02 -1.64e+00  2.06e-01 -0.234710\n4    -0.026819 -0.028305  4.55e-03  1.86e-02  8.56e-03  2.96e-02 -0.029831\n5     0.052581  0.022228 -6.40e-03 -1.76e-02 -3.96e-02 -6.51e-02  0.068133\n6     0.015949 -0.031213 -9.40e-02 -6.45e-02  4.58e-01 -9.30e-03  0.009379\n7    -0.051843  0.060087 -2.60e-01 -5.01e-02  6.39e-01  1.35e-01 -0.157821\n8     0.085942  0.031103 -2.02e-01 -1.27e-01  4.43e-01 -8.71e-02  0.088288\n9    -0.028585  0.025349  2.03e-01  1.60e-01 -2.89e-01  5.43e-03 -0.002433\n10   -0.003499 -0.001126 -3.02e-03 -1.18e-03 -2.96e-03  4.35e-03 -0.004536\n11    0.016094 -0.004191  2.78e-04  1.75e-02  5.79e-02 -9.92e-03  0.007919\n12   -0.136854 -0.034950  7.19e-02  5.04e-02  9.47e-02  1.31e-01 -0.129060\n13   -0.041662 -0.048339 -2.90e-02 -1.87e-02 -2.89e-01  3.87e-02 -0.036874\n14    0.023854 -0.069841  1.06e-01  5.15e-02 -3.76e-01 -3.93e-02  0.044431\n15    0.069040 -0.000553 -8.43e-02 -8.40e-02  1.64e-02 -1.11e-01  0.122452\n16    0.018417  0.009202  1.88e-02  2.26e-03 -6.39e-02 -2.81e-02  0.030485\n17    0.047754  0.045307 -5.15e-02 -3.89e-02  3.28e-01 -2.56e-02  0.020559\n18    0.070861 -0.015712  2.56e-02  1.17e-02 -2.62e-01 -6.00e-02  0.057589\n19    0.024045 -0.003803 -2.16e-02 -9.39e-03 -7.47e-02 -2.53e-03 -0.003307\n20   -0.027117  0.033790  4.71e-02  1.20e-02 -4.99e-03  2.37e-02 -0.021923\n21   -0.069609 -0.206421 -3.06e-02 -4.32e-02 -1.33e-01  4.07e-02 -0.030628\n22    0.025196  0.020824 -5.98e-02 -4.46e-02  2.05e-01 -2.15e-02  0.021177\n23   -0.001038  0.000881 -5.86e-04 -2.00e-04  3.25e-05  1.03e-03 -0.001039\n24    0.013392 -0.007805 -2.18e-03  5.67e-03  1.74e-03 -1.08e-02  0.009985\n25    0.047925  0.013923 -2.70e-02 -2.30e-02 -1.10e-01 -2.17e-02  0.015789\n26    0.004214 -0.018146 -1.15e-02 -2.04e-02 -9.66e-02 -4.78e-03  0.005342\n27    0.019623  0.035643  7.54e-04 -2.37e-02 -1.13e-01 -2.70e-02  0.029524\n28   -0.057405  0.004122 -1.50e-02 -2.22e-02  4.99e-02  5.92e-02 -0.058184\n29    0.013407  0.002234  9.79e-04 -3.36e-03 -2.69e-02 -1.46e-02  0.014714\n30    0.083499 -0.063657  6.79e-02  8.50e-02  7.43e-02 -8.05e-02  0.077827\n31   -0.169198  0.036573  2.29e-01  7.97e-02  6.13e-02  1.28e-01 -0.114583\n32    0.017413 -0.030119  9.42e-05  1.37e-02 -3.48e-02 -1.41e-02  0.013355\n33    0.017307 -0.033766  5.37e-02  5.46e-02  3.69e-02 -1.94e-02  0.019991\n34    0.046604  0.002701 -3.28e-02 -2.38e-02  2.58e-03 -6.08e-02  0.064541\n35   -0.016571 -0.037163  2.62e-02  3.84e-02 -8.40e-02  2.88e-02 -0.032760\n36    0.020120 -0.004970  5.42e-02  4.45e-02  1.46e-01  8.12e-03 -0.016185\n37   -0.014818  0.014042  2.90e-02 -1.23e-02 -3.25e-03 -5.78e-04  0.004536\n38   -0.028895  0.025919 -9.43e-03 -7.50e-03  1.05e-03  2.90e-02 -0.028735\n39   -0.168083 -0.109439 -5.10e-03 -9.09e-03  1.62e-02  1.57e-01 -0.150961\n40    0.074138 -0.022424 -1.96e-02 -4.43e-02 -4.61e-03 -9.30e-02  0.098283\n41    0.054075  0.035258  5.62e-02  3.85e-02 -2.32e-02 -3.73e-02  0.031923\n42   -0.041341 -0.003756 -1.14e-02 -5.36e-03  3.82e-02  3.24e-02 -0.029540\n43   -0.067545 -0.007169  5.92e-02  6.29e-02  1.29e-02  7.82e-02 -0.080563\n44    0.010332 -0.026086 -6.03e-02 -2.31e-02  5.37e-02 -1.51e-02  0.016216\n45    0.272687  0.026733 -3.02e-01 -1.24e-01  2.48e-01 -2.30e-01  0.216591\n46   -0.079967  0.031350  8.77e-03  7.03e-03  1.62e-01  7.77e-02 -0.077365\n47    0.003007 -0.024733 -2.51e-03  1.09e-02  1.43e-02  9.77e-03 -0.013523\n48    0.096850 -0.008649 -1.76e-02  4.52e-03  4.31e-03 -9.53e-02  0.094308\n49    0.181536  0.096778  1.00e-02  2.16e-02  2.02e-01 -1.81e-01  0.179748\n50   -0.040328  0.039396  1.63e-02  8.03e-03  4.46e-03  4.54e-02 -0.046396\n51   -0.007793 -0.003103 -8.50e-03 -2.54e-03  3.01e-03  7.50e-03 -0.007523\n52    0.162265  0.072317 -1.85e-01 -1.73e-01 -1.97e-01 -2.01e-01  0.209312\n53   -0.000513  0.011437  6.61e-03  3.55e-03  6.87e-03 -8.76e-04  0.001247\n54   -0.009254 -0.000266 -4.68e-03 -1.97e-03 -8.51e-03  7.29e-03 -0.006837\n55    0.035948 -0.028661 -1.22e-02 -2.44e-05  4.97e-02 -3.45e-02  0.034300\n56   -0.028240 -0.037786 -5.50e-02 -3.63e-02  3.30e-02  3.09e-02 -0.030670\n57    0.004116  0.004527  7.59e-03 -1.20e-03 -1.34e-02 -6.01e-03  0.006553\n58   -0.008036 -0.008476 -8.57e-03 -2.12e-04  2.12e-02  7.64e-03 -0.007577\n59   -0.035442 -0.006612 -2.59e-02 -2.16e-02  2.83e-03  3.96e-02 -0.040216\n60   -0.009787 -0.008575  1.70e-02  1.94e-02  5.41e-02  1.18e-02 -0.011818\n61   -0.024436  0.016022  2.33e-02  1.13e-02  3.87e-02  2.18e-02 -0.020437\n62    0.043312 -0.058454  2.20e-03  6.69e-02 -1.66e-03 -3.16e-02  0.027871\n63    0.001182 -0.000863 -5.81e-03 -4.19e-03 -3.86e-03  7.97e-04 -0.001176\n64   -0.013067 -0.008362  4.96e-02  2.21e-02 -1.31e-01  1.20e-02 -0.012046\n65    0.006593 -0.002841 -1.69e-02 -8.61e-03  2.89e-02 -5.01e-03  0.004748\n66   -0.018363  0.003528  5.99e-03  2.49e-03  1.72e-03  1.71e-02 -0.016741\n67    0.000824  0.000871  3.49e-03  9.60e-04 -7.09e-03 -2.43e-03  0.002913\n68   -0.011639 -0.013898 -1.35e-02 -1.19e-03 -5.01e-02  1.21e-02 -0.012106\n69   -0.068406 -0.057166  3.08e-02  3.15e-02  3.53e-02  7.05e-02 -0.070999\n70    0.001922 -0.000791  6.22e-03  4.55e-03 -8.89e-04 -4.02e-03  0.004581\n71   -0.052635 -0.022812 -8.84e-03 -1.44e-02  1.32e-02  4.98e-02 -0.048653\n72   -0.004239 -0.008131  7.32e-03  1.37e-02 -1.25e-02  5.71e-03 -0.006150\n73   -0.007207 -0.005987 -1.89e-02  3.23e-03  1.35e-02  2.53e-03 -0.001835\n74   -0.012863  0.037634 -2.10e-05 -2.22e-02 -2.98e-02  4.59e-04  0.002070\n75   -0.020085 -0.029503 -1.88e-03  1.56e-03  4.32e-03  1.79e-02 -0.018615\n76    0.030267  0.011537  4.10e-02  2.35e-02 -7.45e-03 -3.09e-02  0.030881\n77    0.017850  0.001338 -6.11e-03 -2.08e-03  1.49e-02 -1.65e-02  0.016075\n78   -0.019590  0.006957  1.95e-02  6.44e-03 -5.67e-02  1.45e-02 -0.012871\n79    0.001784 -0.015045  6.98e-02  5.90e-02  4.15e-02 -2.05e-02  0.023911\n80    0.016374 -0.012485 -1.91e-02 -1.77e-02  4.06e-02 -1.97e-02  0.020788\n81   -0.274927  0.038145  4.28e-02 -2.97e-02 -3.34e-02  2.34e-01 -0.220290\n82   -0.006930  0.005191 -1.48e-02 -5.20e-03  6.12e-03  8.25e-03 -0.008450\n83   -0.022155  0.085924  5.52e-02 -5.82e-02 -1.87e-02  1.95e-02 -0.019198\n84    0.014162  0.012817  1.46e-02  1.53e-03 -5.76e-03 -9.84e-03  0.008758\n85   -0.017608  0.012676  6.61e-03 -6.66e-03  1.73e-02  2.09e-02 -0.021750\n86    0.007955 -0.002338  2.06e-02  8.14e-03  3.57e-03 -1.82e-02  0.020771\n87    0.022839 -0.036877  3.62e-02  2.06e-02 -2.57e-02 -2.92e-02  0.031734\n88   -0.026690  0.073737  2.18e-02 -1.79e-02 -2.51e-02  2.01e-02 -0.018676\n89   -0.368460 -0.060182 -4.87e-02  3.47e-02  3.70e-02  3.40e-01 -0.327442\n90    0.009178  0.062995 -7.19e-02 -6.68e-02  3.03e-02 -7.92e-03  0.008524\n91    0.247478  0.008360  5.24e-02  3.12e-02  3.68e-03 -2.49e-01  0.244706\n92   -0.014570  0.000972  9.12e-03  3.81e-03  4.83e-02  6.17e-03 -0.004387\n93    0.000811  0.009006  3.74e-02  3.57e-03 -2.05e-02 -4.62e-04  0.000716\n94   -0.001371 -0.037818  9.64e-03  1.21e-02 -9.64e-03  8.00e-04 -0.000822\n95    0.028477 -0.006173  1.25e-01  6.63e-02 -4.32e-02 -4.06e-02  0.043878\n96    0.003349  0.009490 -1.08e-01 -7.44e-02  3.30e-02 -8.21e-03  0.010555\n97    0.022992  0.065041 -6.46e-02 -4.43e-02  4.36e-02 -1.81e-02  0.017803\n98   -0.135687 -0.288883  7.39e-01  1.31e+00 -2.65e-02  1.45e-01 -0.173537\n99    0.007514 -0.010105 -2.57e-02 -1.69e-02 -1.05e-02 -1.65e-02  0.019122\n100   0.006311 -0.001418 -1.12e-02 -5.78e-03  1.26e-02 -1.58e-02  0.018170\n101  -0.000427  0.104513  7.45e-03 -3.74e-02 -2.54e-02 -2.75e-02  0.034639\n102   0.012366 -0.006844  1.08e-02  1.25e-03 -5.39e-03 -1.63e-02  0.017341\n103  -0.006466 -0.002854 -7.37e-03 -6.38e-03  3.69e-03  8.36e-03 -0.008668\n104   0.016484  0.108019 -5.59e-02 -5.54e-02  4.15e-02 -2.14e-02  0.023191\n105  -0.082489  0.368910  1.71e-02 -2.05e-01 -2.45e-02  9.78e-02 -0.099325\n106  -0.045067  0.041848 -6.55e-03 -7.24e-02 -1.60e-02  7.35e-02 -0.079041\n107  -0.005848 -0.033056  8.59e-02  5.24e-02  1.71e-02  1.08e-02 -0.011923\n108  -0.001331 -0.001247 -1.95e-03  2.67e-03  2.71e-04  1.04e-02 -0.012708\n109  -0.009339  0.017893 -6.65e-02 -3.23e-02 -6.17e-03  2.96e-02 -0.035398\n110   0.021120  0.008499  1.44e-02  6.61e-03 -1.24e-02 -2.99e-02  0.031776\n111   0.026187  0.013704  4.32e-02  9.88e-03  2.02e-02 -3.20e-02  0.033948\n112   0.002521 -0.015709 -1.14e-03  5.33e-03 -3.54e-03 -5.47e-03  0.006001\n113   0.018116 -0.022591 -2.05e-02 -1.19e-03  7.72e-03 -1.78e-02  0.017598\n114  -0.007839  0.006822 -1.31e-04 -5.35e-03 -3.85e-03  8.30e-03 -0.008242\n115  -0.004630  0.125871 -2.59e-01 -3.10e-01 -4.02e-03 -1.19e-02  0.009023\n116  -0.004452 -0.004518 -4.13e-02 -1.01e-02  1.00e-02  2.61e-02 -0.033037\n117   0.003784 -0.004204 -3.03e-03 -2.79e-03 -7.84e-03 -2.76e-03  0.002583\n118   0.040355 -0.035537  9.06e-03  2.31e-02 -3.52e-03 -4.81e-02  0.049212\n119   0.000530 -0.003497 -3.81e-03  2.50e-03  7.46e-04 -1.68e-03  0.001848\n120   0.002735 -0.005598 -6.55e-03 -7.01e-03  9.61e-03 -6.09e-03  0.007013\n121  -0.023122  0.007595 -1.70e-02 -7.57e-04  1.26e-02  3.00e-02 -0.031903\n122   0.079074 -0.010742 -1.57e-01 -6.47e-02  1.47e-01 -4.32e-02  0.032740\n123   0.013027 -0.025380  2.86e-02  4.15e-02 -2.84e-02  2.04e-03 -0.006162\n124  -0.000389 -0.040757 -5.01e-02 -4.89e-02  1.22e-02  2.41e-02 -0.029534\n125  -0.005333  0.037250 -3.87e-02 -5.19e-02 -2.00e-02  1.28e-02 -0.013678\n126  -0.027923 -0.081939  4.74e-02  6.39e-02  1.66e-02  1.19e-02 -0.004621\n127  -0.013304  0.007094 -1.28e-02 -6.30e-03 -3.02e-03  1.49e-02 -0.015418\n128   0.009309 -0.016064 -7.10e-02 -4.09e-02 -7.11e-03 -8.34e-03  0.008717\n129  -0.006079  0.004246  6.85e-03 -8.09e-04 -5.34e-03 -3.61e-03  0.006069\n130   0.029553  0.009804  2.15e-02  3.10e-03  3.95e-03 -4.04e-02  0.042770\n131   0.000554  0.008542  5.63e-04 -2.03e-02  1.26e-02 -3.44e-03  0.005090\n132  -0.001873 -0.000230  8.24e-05  8.50e-04  4.91e-04  2.23e-03 -0.002375\n133  -0.009561  0.019829  1.65e-02  1.32e-02 -6.16e-03  7.07e-03 -0.006885\n134   0.030319  0.029864 -1.22e-02 -1.75e-02 -7.70e-02 -1.85e-02  0.015992\n135  -0.000591  0.001937  4.81e-03  2.05e-03 -1.36e-03 -5.78e-05  0.000125\n136  -0.034801  0.029305  2.58e-03 -9.89e-03  5.74e-02  3.13e-02 -0.030135\n137  -0.004208 -0.011287 -1.04e-02  7.61e-03  1.66e-03 -2.48e-03  0.003403\n138   0.002596 -0.001817 -1.29e-05  9.08e-04  2.51e-05 -2.81e-03  0.002785\n139  -0.003439 -0.004914  1.15e-03  6.99e-04 -2.13e-03  2.19e-03 -0.001703\n140  -0.007154  0.001600 -3.69e-04  4.65e-03  3.91e-03  1.29e-02 -0.014521\n141   0.029859 -0.048799  3.27e-02  4.28e-02  7.16e-02 -2.77e-02  0.026971\n142   0.006214  0.015474  1.02e-01  7.63e-02 -1.08e-02 -1.19e-02  0.010947\n143   0.015727  0.007351 -1.05e-02 -1.02e-02  2.83e-03 -1.74e-02  0.017404\n144   0.019385 -0.002519 -2.91e-03  4.52e-02 -1.21e-01 -2.29e-02  0.021738\n145  -0.004856  0.001176 -5.56e-04 -4.97e-04 -1.30e-03  4.22e-03 -0.004051\n146  -0.004181 -0.002393  5.21e-03  2.02e-04  2.33e-03  6.36e-03 -0.006694\n147  -0.017330 -0.006472  7.61e-03  8.92e-03 -1.74e-02  1.90e-02 -0.019336\n148   0.002618 -0.005657  5.82e-04 -7.22e-03  4.49e-02 -1.06e-02  0.012839\n149   0.015429 -0.046969 -6.54e-03  1.07e-02  1.57e-02 -2.28e-02  0.024127\n150  -0.017294  0.004562 -6.18e-02 -1.53e-02 -4.46e-04  2.70e-02 -0.028394\n151  -0.066831  0.074882  7.91e-02 -1.71e-02  1.78e-02  4.74e-02 -0.041498\n152  -0.004180 -0.032130  1.77e-02 -5.62e-03 -1.29e-01 -2.23e-04  0.001198\n      dfb.WAR    dffit   cov.r   cook.d    hat inf\n1    1.21e-01  0.19175 1.71187 1.37e-03 0.2885   *\n2    1.54e-01  0.33741 1.50837 4.24e-03 0.2389    \n3    1.47e+00  4.89234 0.00018 6.35e-01 0.3215   *\n4    5.57e-02  0.26150 1.33537 2.55e-03 0.1468    \n5    6.11e-02  0.46640 1.10654 8.04e-03 0.1467    \n6   -7.76e-02  0.77013 0.98385 2.18e-02 0.2131    \n7   -9.20e-01 -1.72321 0.12155 1.01e-01 0.1953   *\n8   -3.07e-01  0.78549 0.90615 2.26e-02 0.1986    \n9   -3.27e-01 -1.20483 0.36600 5.14e-02 0.1783    \n10   3.18e-04 -0.02188 1.49033 1.79e-05 0.1669    \n11  -7.78e-02  0.22019 1.34497 1.81e-03 0.1362    \n12   1.27e-01 -0.58547 1.28544 1.27e-02 0.2381    \n13   2.81e-02 -1.01228 0.24724 3.59e-02 0.1109   *\n14   5.22e-01 -0.99957 0.57712 3.60e-02 0.1811    \n15  -1.37e-01 -0.72916 0.48449 1.91e-02 0.0980    \n16   6.41e-02 -0.30699 1.21288 3.50e-03 0.1194    \n17  -1.30e-01 -0.53584 1.15092 1.06e-02 0.1828    \n18   2.09e-01  0.62061 0.86304 1.41e-02 0.1391    \n19   1.71e-02 -0.27639 1.38040 2.84e-03 0.1702    \n20   3.31e-02  0.65214 2.30532 1.58e-02 0.5074   *\n21   1.53e-01  1.76468 0.29627 1.09e-01 0.2726   *\n22  -1.06e-01 -0.42175 1.01745 6.56e-03 0.1089    \n23   9.00e-04  0.01067 1.80983 4.25e-06 0.3137   *\n24  -2.95e-03  0.14062 1.31891 7.37e-04 0.0957    \n25   2.33e-01 -0.46427 1.26271 7.99e-03 0.1920    \n26  -6.96e-02 -0.37906 1.37949 5.34e-03 0.2033    \n27   9.97e-02 -0.46539 1.41512 8.04e-03 0.2423    \n28  -3.82e-02 -0.26934 1.30327 2.70e-03 0.1370    \n29   1.41e-02  0.08778 1.49281 2.88e-04 0.1745    \n30   9.61e-02 -0.63972 1.05604 1.51e-02 0.1911    \n31  -2.04e-01 -0.86296 0.75022 2.71e-02 0.1844    \n32   1.60e-02 -0.26567 1.38638 2.63e-03 0.1691    \n33  -5.51e-03 -0.36773 1.20605 5.02e-03 0.1400    \n34   8.58e-02 -0.31259 1.12135 3.62e-03 0.0949    \n35   3.21e-02 -0.48557 0.73863 8.61e-03 0.0779    \n36  -8.03e-02 -0.43285 1.13742 6.93e-03 0.1433    \n37  -6.22e-03  0.27696 1.46227 2.86e-03 0.2040    \n38   8.24e-03 -0.10493 1.56596 4.11e-04 0.2137    \n39   5.81e-03 -0.88931 0.99031 2.90e-02 0.2494    \n40   9.20e-02 -1.06971 1.06784 4.19e-02 0.3168    \n41   1.01e-01  0.44511 1.07813 7.32e-03 0.1317    \n42  -4.68e-02 -0.17758 1.42375 1.18e-03 0.1587    \n43   4.36e-02  0.41318 1.72840 6.36e-03 0.3328   *\n44  -7.84e-02 -0.27589 1.24281 2.83e-03 0.1174    \n45  -1.92e-01  0.91825 0.51472 3.03e-02 0.1475    \n46  -2.33e-01  0.43015 1.45149 6.88e-03 0.2443    \n47  -3.48e-02 -0.18624 1.39180 1.29e-03 0.1460    \n48  -4.45e-02 -0.28062 1.54959 2.93e-03 0.2406    \n49   5.67e-02  0.95678 1.11608 3.36e-02 0.2993    \n50   1.22e-02  0.13566 1.35863 6.86e-04 0.1138    \n51   2.60e-03  0.04266 1.45568 6.79e-05 0.1486    \n52   1.50e-01  1.18103 0.35669 4.94e-02 0.1709    \n53  -9.43e-03  0.07622 1.54329 2.17e-04 0.1992    \n54   1.08e-02  0.08694 1.40724 2.82e-04 0.1272    \n55  -1.18e-04 -0.20812 1.45047 1.61e-03 0.1797    \n56   4.58e-03  0.21008 1.38411 1.64e-03 0.1500    \n57   2.90e-02 -0.05946 1.38690 1.32e-04 0.1099    \n58   1.02e-02  0.10389 1.44953 4.03e-04 0.1540    \n59  -1.32e-02 -0.20738 1.56132 1.60e-03 0.2291    \n60  -1.39e-02  0.19315 1.38713 1.39e-03 0.1460    \n61  -2.49e-02 -0.17686 1.33566 1.17e-03 0.1167    \n62   1.00e-02 -0.19557 1.49073 1.43e-03 0.1949    \n63   2.30e-03  0.03390 1.36800 4.29e-05 0.0942    \n64   1.46e-02  0.32338 1.42871 3.89e-03 0.2043    \n65  -2.57e-03 -0.07416 1.55045 2.05e-04 0.2026    \n66  -2.45e-03  0.04589 1.36903 7.86e-05 0.0965    \n67   2.67e-03  0.03006 1.36573 3.37e-05 0.0922    \n68   1.96e-03  0.22003 1.32471 1.80e-03 0.1275    \n69   3.68e-03  0.21624 1.30052 1.74e-03 0.1160    \n70  -3.34e-03 -0.02626 1.38918 2.57e-05 0.1069    \n71  -2.63e-02  0.19756 1.26950 1.45e-03 0.0964    \n72  -4.14e-03 -0.11285 1.54099 4.75e-04 0.2027    \n73  -5.45e-03  0.08627 1.37317 2.78e-04 0.1075    \n74   7.73e-03  0.19721 1.28945 1.45e-03 0.1042    \n75  -2.67e-02 -0.24072 1.35778 2.16e-03 0.1488    \n76   3.21e-03 -0.12949 1.35090 6.25e-04 0.1078    \n77  -6.66e-03  0.06066 1.37599 1.37e-04 0.1035    \n78   7.64e-02 -0.16886 1.52464 1.06e-03 0.2047    \n79  -3.77e-02 -0.20528 1.29678 1.57e-03 0.1103    \n80  -1.33e-02 -0.19888 1.34099 1.47e-03 0.1269    \n81  -3.10e-03 -0.37404 1.57872 5.21e-03 0.2741    \n82   6.32e-03  0.08499 1.38960 2.70e-04 0.1167    \n83  -5.20e-02  0.33630 1.84811 4.22e-03 0.3570   *\n84   1.61e-02 -0.07585 1.43180 2.15e-04 0.1391    \n85  -5.46e-03  0.08797 1.41342 2.89e-04 0.1309    \n86   8.92e-03  0.13186 1.32432 6.48e-04 0.0952    \n87   3.50e-02 -0.15637 1.49576 9.12e-04 0.1884    \n88   4.05e-02 -0.33164 1.15824 4.08e-03 0.1122    \n89   2.86e-02 -0.48024 2.57544 8.60e-03 0.5380   *\n90  -1.43e-02 -0.14039 1.74252 7.36e-04 0.2944   *\n91   9.17e-03  0.44427 1.35303 7.33e-03 0.2149    \n92  -1.92e-02  0.17724 1.33744 1.17e-03 0.1176    \n93  -5.36e-05 -0.14672 1.32315 8.02e-04 0.1000    \n94   5.52e-03  0.05372 1.98175 1.08e-04 0.3738   *\n95   4.01e-02 -0.19800 1.53830 1.46e-03 0.2170    \n96  -2.68e-02 -0.18000 1.67491 1.21e-03 0.2722   *\n97  -6.83e-02  0.28890 2.07736 3.11e-03 0.4170   *\n98  -3.41e-02  1.71942 2.59845 1.09e-01 0.6575   *\n99   9.36e-04  0.10979 1.38350 4.50e-04 0.1194    \n100 -5.86e-03  0.11433 1.39706 4.88e-04 0.1279    \n101 -1.04e-02 -0.30637 1.13607 3.48e-03 0.0965    \n102  9.43e-04 -0.04629 1.45391 8.00e-05 0.1479    \n103 -5.46e-03  0.04333 1.39072 7.01e-05 0.1098    \n104 -4.81e-02 -0.19755 1.70102 1.46e-03 0.2851   *\n105  3.15e-02  0.48391 2.24725 8.72e-03 0.4772   *\n106  3.87e-03 -0.25630 1.45167 2.45e-03 0.1937    \n107  1.98e-03  0.31994 1.24111 3.80e-03 0.1336    \n108 -3.35e-03 -0.05887 1.87467 1.29e-04 0.3384   *\n109 -1.48e-02 -0.22727 1.26447 1.92e-03 0.1062    \n110 -6.19e-03 -0.12722 1.32007 6.04e-04 0.0914    \n111 -2.37e-05  0.31884 1.19031 3.77e-03 0.1168    \n112 -2.74e-03 -0.06882 1.31493 1.77e-04 0.0685    \n113  3.79e-03  0.05156 1.50933 9.93e-05 0.1792    \n114 -3.13e-03  0.03964 1.33931 5.87e-05 0.0764    \n115 -1.56e-01 -1.24656 2.70378 5.76e-02 0.6248   *\n116 -2.63e-02 -0.17690 1.56161 1.17e-03 0.2232    \n117  6.55e-03  0.03089 1.42236 3.56e-05 0.1279    \n118 -3.57e-03 -0.11290 1.50335 4.76e-04 0.1838    \n119  7.01e-04 -0.02123 1.40047 1.68e-05 0.1137    \n120 -1.12e-02  0.05488 1.36283 1.12e-04 0.0943    \n121 -2.60e-03  0.08555 1.42907 2.73e-04 0.1393    \n122 -5.18e-02  0.61541 0.59293 1.37e-02 0.0889    \n123  1.82e-02 -0.15990 1.33345 9.53e-04 0.1096    \n124 -1.63e-02 -0.16914 1.63045 1.07e-03 0.2521    \n125  9.64e-03  0.11422 1.43013 4.87e-04 0.1458    \n126  4.50e-02  0.69256 0.63334 1.74e-02 0.1146    \n127 -9.09e-03  0.09636 1.35112 3.46e-04 0.0976    \n128  2.03e-03  0.13834 1.38279 7.14e-04 0.1270    \n129  1.72e-02 -0.10621 1.34114 4.21e-04 0.0952    \n130 -4.00e-03  0.15535 1.42604 9.00e-04 0.1538    \n131 -2.59e-02  0.11950 1.40656 5.33e-04 0.1344    \n132 -9.98e-04 -0.00987 1.42036 3.64e-06 0.1256    \n133 -8.25e-03  0.08175 1.38323 2.49e-04 0.1122    \n134  1.89e-01 -0.42756 1.33607 6.79e-03 0.2038    \n135  1.31e-03 -0.01430 1.39558 7.64e-06 0.1102    \n136 -8.89e-03 -0.19427 1.57968 1.41e-03 0.2345    \n137 -1.58e-02 -0.08764 1.55990 2.87e-04 0.2087    \n138  7.24e-04 -0.01661 1.40471 1.03e-05 0.1161    \n139 -1.56e-03  0.02150 1.39498 1.73e-05 0.1103    \n140  2.80e-03 -0.04642 1.42178 8.05e-05 0.1291    \n141 -1.27e-02  0.33455 1.10103 4.14e-03 0.0979    \n142 -2.14e-02 -0.19360 1.48533 1.40e-03 0.1919    \n143  1.28e-02 -0.07596 1.42653 2.15e-04 0.1361    \n144 -6.64e-02  0.46701 1.20635 8.08e-03 0.1755    \n145 -5.73e-04 -0.01992 1.36032 1.48e-05 0.0877    \n146 -2.29e-03  0.03411 1.40438 4.34e-05 0.1172    \n147 -5.46e-02  0.20335 1.30312 1.54e-03 0.1122    \n148 -6.47e-03 -0.14193 1.38749 7.51e-04 0.1305    \n149 -5.87e-02  0.21032 1.36139 1.65e-03 0.1399    \n150 -1.17e-02  0.29658 1.25046 3.27e-03 0.1279    \n151 -5.41e-02  0.46046 1.03712 7.82e-03 0.1268    \n152 -1.41e-01  0.41165 1.28947 6.29e-03 0.1828    \n\n\n- DIFFITS\n\ndffits(model2) \n\n10.19175471661083720.33741401362333834.8923353604171240.26150370503788650.46640225320488660.7701295172213567-1.7232125666810780.7854938678926929-1.204832253279710-0.0218808716371401110.22019323263774312-0.58547230656765113-1.0122774243338714-0.99957277457233415-0.72915571406497316-0.30699359879719917-0.535836713837508180.62061013434326819-0.276388313448388200.652143260513409211.7646817004662422-0.421754632686441230.0106704087127871240.14062038423335925-0.46427442465221526-0.37906360367464827-0.46538931076388128-0.269337052725852290.08778238053987730-0.63971898623320231-0.86295638176144432-0.26566548769917233-0.36772993928044334-0.31258593100128535-0.48556611409773936-0.432853697818921370.27695712526838138-0.10493329799032339-0.88931438960466540-1.06970517189083410.44511325202316442-0.177581717266609430.41317597086902744-0.275890809281245450.918250135494413460.43015211669117847-0.18624478444757948-0.28061753490423490.95678033471325500.135656311662547510.0426576319981019521.18102665082177530.0762187529529201540.086944137782729855-0.20812479717099560.21007745659337657-0.059458321956522580.10388618707298659-0.207378283361126600.19315049583804361-0.17685882130659962-0.195565640798869630.0338975972452186640.32338328036509565-0.0741579586370653660.0458876312546664670.0300572994815203680.220032581657125690.21623636255826170-0.0262624731607537710.19755981583951672-0.112849965978098730.0862667980373561740.19720826717288575-0.2407172894843576-0.129486653975584770.060664350620111678-0.16886322729250179-0.20527975381311380-0.19888493771628981-0.374036098808293820.084994057677105830.33629508844024184-0.0758524495008991850.0879718766613751860.13186201938057887-0.15636831989821788-0.33164235129064589-0.48024382118178890-0.140388573992965910.444267530051117920.17723980655992693-0.146722606234352940.053721748690641795-0.19800042154566196-0.180004343938124970.288897464411681981.71942477718016990.1097868986940591000.11432687119659101-0.3063721645149102-0.04629309930387091030.0433306268503036104-0.1975484115176121050.483912648221853106-0.2563023270476241070.319941379375536108-0.0588728085833438109-0.227267262569933110-0.1272218815964381110.318835761417345112-0.06882042188699771130.051564691370911140.0396413027451092115-1.246559855429116-0.1768998257054351170.0308873845642086118-0.112901316211509119-0.02122975020734141200.05488256484986841210.08555474275049241220.615406232920332123-0.159902509654916124-0.1691400994399591250.1142222082609431260.6925591921777061270.09635722999180761280.138336095805935129-0.1062127151445951300.1553506123541781310.119496269069625132-0.009870224098439571330.0817509188215599134-0.427559880304876135-0.0143031300279799136-0.194270075150927137-0.0876446606855829138-0.01661013875844951390.0214985587661076140-0.04642430309217271410.334545762535563142-0.193602855785598143-0.07595905991789121440.467007521252357145-0.01992342200434361460.03411013315551661470.203347747094969148-0.1419347058458871490.2103165413146761500.2965807086616761510.4604587162890681520.411652370285228\n\n\n\n2*sqrt(26+1/(152-26-1))\n\n10.1996078355984\n\n\n\nnrow(dt)\n\n152\n\n\n\nwhich(abs(dffits(model2)) &gt; 2*sqrt(26+1/(152-26-1)))\n\n\n\n\n\n오잉 없다.\n\n- Cook’s Distance\n\ncooks.distance(model2)\n\n10.0013718267404663420.004238209660997930.63493132476950740.0025450149781177650.0080396119454578660.021759427443934870.10090725395744180.022582695730329190.0514236288297546101.78749745812785e-05110.00180574596978199120.0126856628278567130.0358812148187878140.0359923140550369150.0190968290280876160.00349908645979213170.0106100101941948180.0141089593042995190.00284355082392002200.0158258491672502210.108963985128979220.00656415432623994234.25094807954496e-06240.000737169210218834250.00798929761548201260.00534048165220335270.00804250576322852280.00269847952419076290.000287615039740159300.01506883494593310.0270840444965382320.00262773677779308330.00501513072230009340.00362087045501922350.00860905182039586360.00693268853088638370.00285695152970965380.000410969275736363390.0289718606500854400.0418883564545206410.00732007394863249420.00117580671117435430.00635620470583292440.00282877833266007450.0302906324253581460.00687651533244473470.00129295359130817480.00293416426490539490.0335974671252982500.0006862824200229516.79331163710391e-05520.0493810477220956530.00021685350748066540.000282113292521071550.00161465633583651560.00164440483596627570.000131962215648585580.000402747294678546590.0016037788841031600.00139044130309376610.0011655998252313620.00142612196633079634.2896679866974e-05640.00389166176260077650.000205288344036934667.86043953144936e-05673.37282133759689e-05680.00180276706052307690.00174074955767207702.57499051672226e-05710.00145292048740253720.000475282662343774730.000277712566869316740.00144812305657954750.00215764263699819760.00062530078124179770.000137366245605136780.00106366980529164790.00156901927663369800.00147358896278508810.00520781729623918820.000269593964500882830.00421553572845086840.000214752939473668850.000288823413310092860.000648313974114493870.0009121226803552880.00407779800555876890.00859716165908423900.000735567782565387910.00732649347431043920.00117063792459619930.000802491213029919940.000107747677123021950.00146204782006016960.00120889124868621970.00311317539940665980.109025840013283990.0004496910476771811000.0004876511775497441010.003479806407605411028.00043947901423e-051037.00908257908494e-051040.00145589180278661050.008724912065858241060.002447220351944751070.003801418500330251080.0001293988262031321090.001921668703855571100.0006035106342588931110.00377202609254451120.0001767395047233921139.92628521414667e-051145.86615894584444e-051150.05758302914514271160.001167341887274081173.56175509678829e-051180.0004756904377684541191.68268191794619e-051200.0001124325332077631210.0002731834370800471220.01371058158309531230.000953033696214591240.001067383659034451250.0004868082670600911260.01738785122135251270.0003464118623379671280.0007137321039782861290.0004208255180810611300.0009000891396251011310.0005327349566430641323.63727564156477e-061330.0002494162613109551340.006786165622999031357.63802286263089e-061360.001407683437135881370.000286730243009131381.03006286026223e-051391.72556051195854e-051408.04571089191523e-051410.00414417677787121420.001397641306414931430.0002153552781416911440.008076029716348811451.48196505687254e-051464.34370870624436e-051470.001539781416332431480.0007513320687411691490.001647860350452621500.003268250834309861510.007823888647485081520.00628840047856568\n\n\n\nqf(0.5,26,152-19-1)\n\n0.979429146533322\n\n\n\nwhich(cooks.distance(model2) &gt;qf(0.5,26,132))\n\n\n\n\n없네? 왜 없을까낭..\n- Covratio\n\ncovratio(model2)\n\n11.7118673145098821.5083653737662830.0001798833092546241.3353664723888951.106543539285960.98384627248638170.12155340242128580.9061496389498190.366002421023644101.49033379423982111.34497082749312121.28544164892354130.247237105633754140.577122539365025150.484493599392309161.21287611006314171.15091634614759180.863041543876647191.38040471341328202.30532437945197210.296267378670927221.01745237237097231.80982751749533241.3189117725255251.26271423389352261.37948962802568271.41511561961242281.30326590932949291.49281218311284301.05604428171686310.750219983551635321.38637787312595331.20604793216974341.12135282591332350.738628703527732361.1374246382552371.4622707465942381.56595550601158390.990310192336547401.06784314502333411.0781332023755421.42374857095495431.72839704053936441.24280658263068450.514719268336933461.45148924084037471.3917967653191481.54958697897291491.11607937218169501.35862660955829511.455684329483520.356692006124781531.54329202235797541.40723657211145551.45046938778717561.38410775492674571.38689594799922581.44953242242669591.56131516904464601.38712642526316611.33566372939892621.49073347513489631.36800396431952641.42871414746568651.55044983161622661.36902763339381671.36573330756073681.32471348866426691.30052187797791701.38918181839399711.2695029715472721.54099303662624731.37316971058412741.28945292562349751.35778303115989761.35090488577226771.37599237010286781.52463502627899791.29677500502217801.34098741188176811.5787243631209821.38959628652113831.84810599223318841.43179917036847851.41342295544522861.32432092669281871.49576344083971881.15824076678625892.57544338948355901.74251693229226911.3530335356513921.33743775551805931.32314702654193941.98174707382672951.53830492796462961.67490730654056972.07736163327809982.59845452314754991.383498396374041001.397063221917081011.136068886443091021.453906427181251031.390718082526151041.701021637519461052.247254261541981061.451665570565231071.241110970508381081.874671573480631091.264474373519061101.320068450363621111.190308668308341121.314925221369811131.509326782367631141.33931025113821152.70378302934581161.56161419334961171.422364450644461181.503346306307281191.400467534379691201.362827643854121211.429070111355311220.5929285110935881231.33345221814191241.630452272168341251.430126078205951260.6333357100772721271.351119587574621281.382787758625221291.341143759803631301.42604211131951311.406556864223531321.420359361774011331.383234920952861341.336074672511581351.395577994570071361.579680940373751371.559896869090281381.404710277454391391.39497852295221401.421778067104451411.101026597405621421.485326494163611431.426530309384721441.206349794293041451.360316295190951461.404375632002311471.303121590990161481.387494115513961491.361390640025861501.250457644033981511.037118073125531521.28947074287796\n\n\n\nwhich(abs(covratio(model2)-1) &gt; 3*(26+1)/152)\n\n11337799131320202121232338384343484852525353595965657272818183838989909094949595969697979898104104105105108108115115116116124124136136137137\n\n\n\nsummary(influence.measures(model2))\n\nPotentially influential observations of\n     lm(formula = 연봉.2018. ~ ., data = dt) :\n\n    dfb.1_ dfb.팀명KT dfb.팀명LG dfb.팀명NC dfb.팀명SK dfb.팀명두산\n1   -0.02   0.02       0.01       0.01       0.07       0.00       \n3    0.35  -0.78      -1.08_*    -0.99      -1.20_*    -1.52_*     \n7    0.28  -0.39       0.20       0.06       0.05      -0.03       \n13  -0.16   0.62       0.62       0.49       0.60       0.59       \n20  -0.05  -0.03      -0.02      -0.05       0.00       0.01       \n21  -0.09   0.06       0.01      -0.07       0.06       0.08       \n23   0.00   0.00       0.00       0.00       0.00       0.00       \n43  -0.04   0.04       0.10       0.01       0.01      -0.01       \n83  -0.06  -0.01      -0.01      -0.01       0.01       0.00       \n89  -0.23  -0.04      -0.05      -0.03       0.05       0.00       \n90   0.01   0.02       0.01       0.01       0.00       0.02       \n94   0.00   0.00       0.00      -0.01       0.00       0.01       \n96   0.02  -0.04       0.01       0.00       0.01       0.03       \n97   0.02   0.07       0.02       0.01       0.02       0.06       \n98  -0.19   0.03       0.19       0.27       0.17       0.22       \n104  0.00   0.00       0.00       0.01       0.00      -0.04       \n105 -0.09   0.02       0.03       0.04       0.03       0.10       \n108  0.02   0.00       0.00       0.00       0.00       0.00       \n115  0.22   0.18       0.12       0.23       0.18       0.15       \n    dfb.팀명롯데 dfb.팀명삼성 dfb.팀명한화 dfb.승  dfb.패  dfb.세 dfb.홀드\n1    0.01         0.02         0.03         0.04   -0.03   -0.01  -0.01   \n3   -1.38_*      -0.81        -0.77         2.92_* -1.39_*  0.63   0.38   \n7   -0.04         0.18         0.14         0.08   -0.65    0.00   0.04   \n13   0.58         0.54         0.59         0.09   -0.20    0.19   0.14   \n20   0.14         0.00        -0.03        -0.16   -0.03    0.40   0.03   \n21  -0.07         0.04         0.51         0.44    0.01    0.89  -0.19   \n23   0.00         0.00         0.00         0.00    0.00    0.00   0.01   \n43  -0.03         0.06         0.03        -0.03   -0.11    0.03   0.23   \n83  -0.01         0.05        -0.02         0.03    0.01    0.01   0.01   \n89   0.00        -0.01        -0.02        -0.01    0.05   -0.03  -0.04   \n90  -0.03         0.01         0.02        -0.01   -0.01    0.00   0.00   \n94   0.00        -0.01         0.00         0.00    0.00    0.00   0.00   \n96   0.02        -0.01         0.01        -0.02    0.01    0.00  -0.02   \n97   0.02         0.01         0.02         0.02   -0.02    0.01   0.02   \n98   0.08         0.21         0.12         0.00   -0.02   -0.08  -0.05   \n104  0.00         0.00         0.00        -0.01   -0.01   -0.01  -0.01   \n105  0.05         0.02         0.00         0.02    0.06    0.02   0.02   \n108  0.00        -0.01         0.00         0.00    0.00    0.00   0.00   \n115  0.15         0.20         0.13         0.06   -0.03    0.05   0.04   \n    dfb.블론 dfb.경기 dfb.선발 dfb.이닝 dfb.삼진.9 dfb.볼넷.9 dfb.홈런.9\n1    0.00     0.01     0.00    -0.01    -0.01      -0.01      -0.01     \n3   -0.32    -0.30     0.07    -0.16    -0.41      -0.02      -0.09     \n7    0.14     0.12     0.24    -0.07    -0.25       0.02      -0.05     \n13  -0.01    -0.03     0.02     0.09    -0.01       0.00      -0.04     \n20  -0.10     0.00     0.02     0.03    -0.02      -0.04      -0.03     \n21  -0.15     0.03     0.03    -0.17     0.07      -0.09      -0.07     \n23   0.00     0.00     0.00     0.00     0.00       0.00       0.00     \n43  -0.02     0.12     0.20    -0.15    -0.09      -0.07      -0.07     \n83  -0.02    -0.04    -0.01     0.01     0.01      -0.03      -0.02     \n89   0.02     0.10     0.02    -0.07    -0.25      -0.40      -0.37     \n90   0.01     0.01     0.01     0.00     0.01       0.02       0.01     \n94   0.00    -0.01     0.00     0.00     0.00       0.00       0.00     \n96  -0.01     0.00    -0.01     0.01     0.03       0.01       0.00     \n97   0.00    -0.05    -0.01     0.01     0.04       0.01       0.02     \n98   0.04     0.15     0.09    -0.04    -0.30      -0.07      -0.14     \n104  0.00     0.01     0.00     0.00     0.03       0.00       0.02     \n105  0.00    -0.02    -0.03     0.00    -0.13      -0.04      -0.08     \n108  0.00     0.00     0.01     0.00    -0.02       0.00       0.00     \n115  0.01    -0.09     0.00     0.03    -0.02      -0.02       0.00     \n    dfb.BABI dfb.LOB. dfb.ERA dfb.RA9. dfb.FIP dfb.kFIP dfb.WAR dffit   cov.r  \n1   -0.01     0.02     0.01   -0.06     0.01   -0.01     0.12    0.19    1.71_*\n3   -0.09     0.25     0.05   -1.64_*   0.21   -0.23     1.47_*  4.89_*  0.00_*\n7    0.06    -0.26    -0.05    0.64     0.13   -0.16    -0.92   -1.72_*  0.12_*\n13  -0.05    -0.03    -0.02   -0.29     0.04   -0.04     0.03   -1.01    0.25_*\n20   0.03     0.05     0.01    0.00     0.02   -0.02     0.03    0.65    2.31_*\n21  -0.21    -0.03    -0.04   -0.13     0.04   -0.03     0.15    1.76_*  0.30_*\n23   0.00     0.00     0.00    0.00     0.00    0.00     0.00    0.01    1.81_*\n43  -0.01     0.06     0.06    0.01     0.08   -0.08     0.04    0.41    1.73_*\n83   0.09     0.06    -0.06   -0.02     0.02   -0.02    -0.05    0.34    1.85_*\n89  -0.06    -0.05     0.03    0.04     0.34   -0.33     0.03   -0.48    2.58_*\n90   0.06    -0.07    -0.07    0.03    -0.01    0.01    -0.01   -0.14    1.74_*\n94  -0.04     0.01     0.01   -0.01     0.00    0.00     0.01    0.05    1.98_*\n96   0.01    -0.11    -0.07    0.03    -0.01    0.01    -0.03   -0.18    1.67_*\n97   0.07    -0.06    -0.04    0.04    -0.02    0.02    -0.07    0.29    2.08_*\n98  -0.29     0.74     1.31_* -0.03     0.15   -0.17    -0.03    1.72_*  2.60_*\n104  0.11    -0.06    -0.06    0.04    -0.02    0.02    -0.05   -0.20    1.70_*\n105  0.37     0.02    -0.21   -0.02     0.10   -0.10     0.03    0.48    2.25_*\n108  0.00     0.00     0.00    0.00     0.01   -0.01     0.00   -0.06    1.87_*\n115  0.13    -0.26    -0.31    0.00    -0.01    0.01    -0.16   -1.25    2.70_*\n    cook.d hat    \n1    0.00   0.29  \n3    0.63   0.32  \n7    0.10   0.20  \n13   0.04   0.11  \n20   0.02   0.51  \n21   0.11   0.27  \n23   0.00   0.31  \n43   0.01   0.33  \n83   0.00   0.36  \n89   0.01   0.54_*\n90   0.00   0.29  \n94   0.00   0.37  \n96   0.00   0.27  \n97   0.00   0.42  \n98   0.11   0.66_*\n104  0.00   0.29  \n105  0.01   0.48  \n108  0.00   0.34  \n115  0.06   0.62_*\n\n\n\n\nplot(model2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Shapiro-Wilk Test\n## H0 : normal distribution vs. H1 : not H0\nshapiro.test(resid(model2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model2)\nW = 0.91167, p-value = 5.435e-08\n\n\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\n### 등분산성\n## H0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\nbptest(model2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model2\nBP = 55.489, df = 26, p-value = 0.0006566"
  },
  {
    "objectID": "posts/AS3_3.html#단계적-전진선택법",
    "href": "posts/AS3_3.html#단계적-전진선택법",
    "title": "AS HW3_3(야구 투수)",
    "section": "단계적 전진선택법",
    "text": "단계적 전진선택법\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n# 절편먼저 시작\n\n\nadd1(m0,\n scope = 연봉.2018. ~팀명+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n test = \"F\") \n\n\nA anova: 20 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n144556672039\n3144.305\nNA\nNA\n\n\n팀명\n8\n5733352294\n138823319745\n3154.153\n0.7382310\n6.575975e-01\n\n\n승\n1\n73377399129\n71179272910\n3038.617\n154.6322324\n7.609123e-25\n\n\n패\n1\n31909645038\n112647027001\n3108.394\n42.4906621\n1.018082e-09\n\n\n세\n1\n6446108286\n138110563754\n3139.371\n7.0010303\n9.014876e-03\n\n\n홀드\n1\n43499239\n144513172801\n3146.259\n0.0451508\n8.320155e-01\n\n\n블론\n1\n1381513110\n143175158930\n3144.845\n1.4473668\n2.308464e-01\n\n\n경기\n1\n6371372218\n138185299822\n3139.453\n6.9161180\n9.432762e-03\n\n\n선발\n1\n45409453011\n99147219028\n3088.991\n68.7000404\n5.980872e-14\n\n\n이닝\n1\n62759336897\n81797335142\n3059.752\n115.0881079\n2.775097e-20\n\n\n삼진.9\n1\n1556722941\n142999949098\n3144.659\n1.6329267\n2.032728e-01\n\n\n볼넷.9\n1\n15661166715\n128895505324\n3128.875\n18.2254222\n3.465485e-05\n\n\n홈런.9\n1\n1957529447\n142599142593\n3144.232\n2.0591247\n1.533788e-01\n\n\nBABIP\n1\n1513931514\n143042740525\n3144.704\n1.5875656\n2.096310e-01\n\n\nLOB.\n1\n2283144014\n142273528025\n3143.885\n2.4071351\n1.228906e-01\n\n\nERA\n1\n6733177111\n137823494928\n3139.055\n7.3280435\n7.576445e-03\n\n\nRA9.WAR\n1\n79230247396\n65326424643\n3025.575\n181.9254180\n1.175507e-27\n\n\nFIP\n1\n11402805814\n133153866226\n3133.815\n12.8454466\n4.568071e-04\n\n\nkFIP\n1\n12591370602\n131965301437\n3132.452\n14.3121379\n2.232236e-04\n\n\nWAR\n1\n90535063043\n54021608997\n2996.693\n251.3856901\n7.125176e-34\n\n\n\n\n\n\nm1 &lt;- update(m0, ~ . +WAR)\nsummary(m1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ WAR, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-66769  -5833  -2046   4111 118952 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     6600       1725   3.827  0.00019 ***\nWAR            18519       1168  15.855  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 18980 on 150 degrees of freedom\nMultiple R-squared:  0.6263,    Adjusted R-squared:  0.6238 \nF-statistic: 251.4 on 1 and 150 DF,  p-value: &lt; 2.2e-16\n\n\n\nadd1(m1,\n scope = 연봉.2018. ~팀명+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n test = \"F\") \n\n\nA anova: 19 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n54021608997\n2996.693\nNA\nNA\n\n\n팀명\n8\n3.656082e+09\n50365526927\n3002.041\n1.288490e+00\n0.254059759\n\n\n승\n1\n1.730511e+09\n52291097975\n2993.744\n4.930976e+00\n0.027888729\n\n\n패\n1\n1.519639e+08\n53869645095\n2998.265\n4.203225e-01\n0.517774062\n\n\n세\n1\n3.042965e+09\n50978643633\n2989.880\n8.893957e+00\n0.003343546\n\n\n홀드\n1\n2.526560e+07\n53996343398\n2998.622\n6.971906e-02\n0.792111117\n\n\n블론\n1\n3.000543e+09\n51021066097\n2990.007\n8.762672e+00\n0.003578319\n\n\n경기\n1\n4.286190e+08\n53592990020\n2997.482\n1.191653e+00\n0.276758468\n\n\n선발\n1\n5.471985e+08\n53474410450\n2997.145\n1.524703e+00\n0.218852899\n\n\n이닝\n1\n1.213852e+02\n54021608875\n2998.693\n3.347992e-07\n0.999539103\n\n\n삼진.9\n1\n3.956421e+07\n53982044788\n2998.582\n1.092042e-01\n0.741516150\n\n\n볼넷.9\n1\n5.085840e+07\n53970750594\n2998.550\n1.404076e-01\n0.708408949\n\n\n홈런.9\n1\n3.176050e+08\n53704004008\n2997.797\n8.811846e-01\n0.349396618\n\n\nBABIP\n1\n1.955840e+08\n53826025006\n2998.142\n5.414112e-01\n0.463005189\n\n\nLOB.\n1\n1.973942e+07\n54001869580\n2998.637\n5.446428e-02\n0.815789881\n\n\nERA\n1\n1.219540e+07\n54009413594\n2998.659\n3.364441e-02\n0.854714605\n\n\nRA9.WAR\n1\n1.888461e+08\n53832762923\n2998.161\n5.226941e-01\n0.470828073\n\n\nFIP\n1\n1.426032e+08\n53879005831\n2998.291\n3.943627e-01\n0.530976356\n\n\nkFIP\n1\n1.357816e+08\n53885827432\n2998.310\n3.754504e-01\n0.540982315\n\n\n\n\n\n\n너무 귀찮고… 복잡한데.."
  },
  {
    "objectID": "posts/AS3_3.html#aic이용",
    "href": "posts/AS3_3.html#aic이용",
    "title": "AS HW3_3(야구 투수)",
    "section": "AIC이용",
    "text": "AIC이용\n\nSTEP\n\nmodel_step = step(\n m0,\n scope = 연봉.2018. ~팀명+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"both\")\nsummary(model_step)\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n          Df  Sum of Sq        RSS    AIC\n+ WAR      1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR  1 7.9230e+10 6.5326e+10 3025.6\n+ 승       1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝     1 6.2759e+10 8.1797e+10 3059.8\n+ 선발     1 4.5409e+10 9.9147e+10 3089.0\n+ 패       1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9   1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP     1 1.2591e+10 1.3197e+11 3132.4\n+ FIP      1 1.1403e+10 1.3315e+11 3133.8\n+ ERA      1 6.7332e+09 1.3782e+11 3139.1\n+ 세       1 6.4461e+09 1.3811e+11 3139.4\n+ 경기     1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.     1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9   1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                  1.4456e+11 3144.3\n+ 삼진.9   1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP    1 1.5139e+09 1.4304e+11 3144.7\n+ 블론     1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드     1 4.3499e+07 1.4451e+11 3146.3\n+ 팀명     8 5.7334e+09 1.3882e+11 3154.2\n\nStep:  AIC=2996.69\n연봉.2018. ~ WAR\n\n          Df  Sum of Sq        RSS    AIC\n+ 세       1 3.0430e+09 5.0979e+10 2989.9\n+ 블론     1 3.0005e+09 5.1021e+10 2990.0\n+ 승       1 1.7305e+09 5.2291e+10 2993.7\n&lt;none&gt;                  5.4022e+10 2996.7\n+ 선발     1 5.4720e+08 5.3474e+10 2997.2\n+ 경기     1 4.2862e+08 5.3593e+10 2997.5\n+ 홈런.9   1 3.1760e+08 5.3704e+10 2997.8\n+ BABIP    1 1.9558e+08 5.3826e+10 2998.1\n+ RA9.WAR  1 1.8885e+08 5.3833e+10 2998.2\n+ 패       1 1.5196e+08 5.3870e+10 2998.3\n+ FIP      1 1.4260e+08 5.3879e+10 2998.3\n+ kFIP     1 1.3578e+08 5.3886e+10 2998.3\n+ 볼넷.9   1 5.0858e+07 5.3971e+10 2998.6\n+ 삼진.9   1 3.9564e+07 5.3982e+10 2998.6\n+ 홀드     1 2.5266e+07 5.3996e+10 2998.6\n+ LOB.     1 1.9739e+07 5.4002e+10 2998.6\n+ ERA      1 1.2195e+07 5.4009e+10 2998.7\n+ 이닝     1 1.2100e+02 5.4022e+10 2998.7\n+ 팀명     8 3.6561e+09 5.0366e+10 3002.0\n- WAR      1 9.0535e+10 1.4456e+11 3144.3\n\nStep:  AIC=2989.88\n연봉.2018. ~ WAR + 세\n\n          Df  Sum of Sq        RSS    AIC\n+ 승       1 1.8583e+09 4.9120e+10 2986.2\n+ 블론     1 7.0314e+08 5.0276e+10 2989.8\n&lt;none&gt;                  5.0979e+10 2989.9\n+ kFIP     1 5.6448e+08 5.0414e+10 2990.2\n+ FIP      1 4.9951e+08 5.0479e+10 2990.4\n+ 홈런.9   1 4.4165e+08 5.0537e+10 2990.6\n+ 삼진.9   1 2.3464e+08 5.0744e+10 2991.2\n+ 패       1 1.7676e+08 5.0802e+10 2991.3\n+ BABIP    1 8.9298e+07 5.0889e+10 2991.6\n+ 이닝     1 2.4856e+07 5.0954e+10 2991.8\n+ LOB.     1 1.8541e+07 5.0960e+10 2991.8\n+ ERA      1 1.5240e+07 5.0963e+10 2991.8\n+ 경기     1 1.0175e+07 5.0968e+10 2991.8\n+ 선발     1 9.8418e+06 5.0969e+10 2991.8\n+ 홀드     1 1.9815e+06 5.0977e+10 2991.9\n+ 볼넷.9   1 1.4928e+06 5.0977e+10 2991.9\n+ RA9.WAR  1 1.4282e+06 5.0977e+10 2991.9\n+ 팀명     8 3.5875e+09 4.7391e+10 2994.8\n- 세       1 3.0430e+09 5.4022e+10 2996.7\n- WAR      1 8.7132e+10 1.3811e+11 3139.4\n\nStep:  AIC=2986.24\n연봉.2018. ~ WAR + 세 + 승\n\n          Df  Sum of Sq        RSS    AIC\n+ 패       1 1.3060e+09 4.7814e+10 2984.1\n+ 이닝     1 1.2755e+09 4.7845e+10 2984.2\n+ 경기     1 7.3078e+08 4.8390e+10 2986.0\n&lt;none&gt;                  4.9120e+10 2986.2\n+ 선발     1 5.6173e+08 4.8559e+10 2986.5\n+ kFIP     1 4.9401e+08 4.8626e+10 2986.7\n+ FIP      1 4.4265e+08 4.8678e+10 2986.9\n+ RA9.WAR  1 4.1326e+08 4.8707e+10 2986.9\n+ 홈런.9   1 2.9281e+08 4.8828e+10 2987.3\n+ 삼진.9   1 1.4294e+08 4.8977e+10 2987.8\n+ 블론     1 1.3742e+08 4.8983e+10 2987.8\n+ 홀드     1 1.2820e+08 4.8992e+10 2987.8\n+ ERA      1 7.3246e+07 4.9047e+10 2988.0\n+ 볼넷.9   1 3.2209e+07 4.9088e+10 2988.1\n+ LOB.     1 3.0027e+07 4.9090e+10 2988.1\n+ BABIP    1 2.3777e+06 4.9118e+10 2988.2\n- 승       1 1.8583e+09 5.0979e+10 2989.9\n+ 팀명     8 3.4764e+09 4.5644e+10 2991.1\n- 세       1 3.1707e+09 5.2291e+10 2993.7\n- WAR      1 1.7726e+10 6.6847e+10 3031.1\n\nStep:  AIC=2984.14\n연봉.2018. ~ WAR + 세 + 승 + 패\n\n          Df  Sum of Sq        RSS    AIC\n+ RA9.WAR  1 7.0482e+08 4.7110e+10 2983.9\n&lt;none&gt;                  4.7814e+10 2984.1\n+ 경기     1 5.1652e+08 4.7298e+10 2984.5\n+ kFIP     1 5.1571e+08 4.7299e+10 2984.5\n+ FIP      1 4.5277e+08 4.7362e+10 2984.7\n+ 홈런.9   1 3.5756e+08 4.7457e+10 2985.0\n+ 이닝     1 2.7090e+08 4.7544e+10 2985.3\n+ 블론     1 2.3621e+08 4.7578e+10 2985.4\n+ 삼진.9   1 1.9710e+08 4.7617e+10 2985.5\n+ LOB.     1 1.4784e+08 4.7667e+10 2985.7\n+ ERA      1 8.5522e+07 4.7729e+10 2985.9\n+ 홀드     1 8.0301e+07 4.7734e+10 2985.9\n+ 선발     1 2.0412e+07 4.7794e+10 2986.1\n+ BABIP    1 5.9999e+06 4.7808e+10 2986.1\n+ 볼넷.9   1 3.8294e+05 4.7814e+10 2986.1\n- 패       1 1.3060e+09 4.9120e+10 2986.2\n+ 팀명     8 4.0489e+09 4.3766e+10 2986.7\n- 승       1 2.9875e+09 5.0802e+10 2991.3\n- 세       1 3.3025e+09 5.1117e+10 2992.3\n- WAR      1 1.8530e+10 6.6345e+10 3031.9\n\nStep:  AIC=2983.88\n연봉.2018. ~ WAR + 세 + 승 + 패 + RA9.WAR\n\n          Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                  4.7110e+10 2983.9\n+ kFIP     1 5.5378e+08 4.6556e+10 2984.1\n+ 경기     1 5.4513e+08 4.6564e+10 2984.1\n- RA9.WAR  1 7.0482e+08 4.7814e+10 2984.1\n+ FIP      1 4.6551e+08 4.6644e+10 2984.4\n+ 홈런.9   1 3.3666e+08 4.6773e+10 2984.8\n+ 삼진.9   1 3.0265e+08 4.6807e+10 2984.9\n+ 블론     1 1.2925e+08 4.6980e+10 2985.5\n+ 홀드     1 9.0189e+07 4.7019e+10 2985.6\n+ BABIP    1 7.1631e+07 4.7038e+10 2985.7\n+ 이닝     1 4.2896e+07 4.7067e+10 2985.7\n+ ERA      1 1.6033e+07 4.7094e+10 2985.8\n+ 선발     1 5.9070e+06 4.7104e+10 2985.9\n+ LOB.     1 4.5815e+06 4.7105e+10 2985.9\n+ 볼넷.9   1 6.3618e+04 4.7110e+10 2985.9\n- 패       1 1.5975e+09 4.8707e+10 2986.9\n+ 팀명     8 3.8403e+09 4.3269e+10 2987.0\n- 승       1 3.6878e+09 5.0797e+10 2993.3\n- 세       1 3.9111e+09 5.1021e+10 2994.0\n- WAR      1 1.2311e+10 5.9421e+10 3017.2\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ WAR + 세 + 승 + 패 + RA9.WAR, \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57203  -4481   -518   5092  98660 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4640.9     2190.1   2.119 0.035783 *  \nWAR          17999.2     2913.9   6.177 6.16e-09 ***\n세            1105.3      317.5   3.482 0.000658 ***\n승            2831.9      837.7   3.381 0.000928 ***\n패           -1505.7      676.7  -2.225 0.027609 *  \nRA9.WAR      -3428.2     2319.5  -1.478 0.141573    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 17960 on 146 degrees of freedom\nMultiple R-squared:  0.6741,    Adjusted R-squared:  0.6629 \nF-statistic:  60.4 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\nAIC를 이용하면 최종 모형은 “연봉.2018. ~ WAR + 세 + 승 + 패 + RA9.WAR” 이다.\n\n\n\n후진\n\nmodel_back = step(model2, direction = \"backward\")\nsummary(model_back)\n\nStart:  AIC=3000.21\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 홀드 + 블론 + 경기 + \n    선발 + 이닝 + 삼진.9 + 볼넷.9 + 홈런.9 + BABIP + \n    LOB. + ERA + RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 삼진.9   1    8197091 3.9798e+10 2998.2\n- ERA      1   10387869 3.9800e+10 2998.2\n- LOB.     1   64367797 3.9854e+10 2998.5\n- kFIP     1   77615678 3.9867e+10 2998.5\n- FIP      1   90577046 3.9880e+10 2998.6\n- 볼넷.9   1  132645645 3.9922e+10 2998.7\n- BABIP    1  170471360 3.9960e+10 2998.9\n- 홈런.9   1  184287409 3.9974e+10 2998.9\n- 홀드     1  204952742 3.9994e+10 2999.0\n- 팀명     8 4201080265 4.3991e+10 2999.5\n- 블론     1  392171281 4.0182e+10 2999.7\n- RA9.WAR  1  448716954 4.0238e+10 2999.9\n- 이닝     1  473170227 4.0263e+10 3000.0\n- 선발     1  518773402 4.0308e+10 3000.2\n&lt;none&gt;                  3.9789e+10 3000.2\n- 패       1  842395946 4.0632e+10 3001.4\n- 경기     1 1300498944 4.1090e+10 3003.1\n- 세       1 1918682650 4.1708e+10 3005.4\n- 승       1 2741990179 4.2531e+10 3008.3\n- WAR      1 9184291500 4.8974e+10 3029.8\n\nStep:  AIC=2998.25\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 홀드 + 블론 + 경기 + \n    선발 + 이닝 + 볼넷.9 + 홈런.9 + BABIP + LOB. + ERA + \n    RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- ERA      1   10169383 3.9808e+10 2996.3\n- LOB.     1   65361211 3.9863e+10 2996.5\n- BABIP    1  163630304 3.9961e+10 2996.9\n- 볼넷.9   1  175813214 3.9973e+10 2996.9\n- 홀드     1  205908453 4.0004e+10 2997.0\n- FIP      1  301484311 4.0099e+10 2997.4\n- kFIP     1  324158764 4.0122e+10 2997.5\n- 팀명     8 4212864623 4.4011e+10 2997.5\n- 홈런.9   1  362911144 4.0161e+10 2997.6\n- 블론     1  394851659 4.0193e+10 2997.8\n- RA9.WAR  1  443540025 4.0241e+10 2997.9\n- 이닝     1  465042521 4.0263e+10 2998.0\n- 선발     1  515010811 4.0313e+10 2998.2\n&lt;none&gt;                  3.9798e+10 2998.2\n- 패       1  834201187 4.0632e+10 2999.4\n- 경기     1 1293281645 4.1091e+10 3001.1\n- 세       1 1911988086 4.1710e+10 3003.4\n- 승       1 2773195191 4.2571e+10 3006.5\n- WAR      1 9193295470 4.8991e+10 3027.8\n\nStep:  AIC=2996.28\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 홀드 + 블론 + 경기 + \n    선발 + 이닝 + 볼넷.9 + 홈런.9 + BABIP + LOB. + RA9.WAR + \n    FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- LOB.     1   72922455 3.9881e+10 2994.6\n- 볼넷.9   1  173310646 3.9981e+10 2994.9\n- 홀드     1  200933380 4.0009e+10 2995.1\n- BABIP    1  301616031 4.0109e+10 2995.4\n- FIP      1  318111761 4.0126e+10 2995.5\n- kFIP     1  341386605 4.0149e+10 2995.6\n- 홈런.9   1  366390239 4.0174e+10 2995.7\n- 블론     1  396568615 4.0204e+10 2995.8\n- 팀명     8 4293631610 4.4101e+10 2995.8\n- 이닝     1  460179563 4.0268e+10 2996.0\n- RA9.WAR  1  476388061 4.0284e+10 2996.1\n- 선발     1  505829968 4.0314e+10 2996.2\n&lt;none&gt;                  3.9808e+10 2996.3\n- 패       1  849127946 4.0657e+10 2997.5\n- 경기     1 1293231156 4.1101e+10 2999.1\n- 세       1 1902025740 4.1710e+10 3001.4\n- 승       1 2768648270 4.2576e+10 3004.5\n- WAR      1 9428720510 4.9237e+10 3026.6\n\nStep:  AIC=2994.56\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 홀드 + 블론 + 경기 + \n    선발 + 이닝 + 볼넷.9 + 홈런.9 + BABIP + RA9.WAR + \n    FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 볼넷.9   1 1.3921e+08 4.0020e+10 2993.1\n- 홀드     1 1.9204e+08 4.0073e+10 2993.3\n- BABIP    1 2.2929e+08 4.0110e+10 2993.4\n- FIP      1 2.8027e+08 4.0161e+10 2993.6\n- kFIP     1 3.0644e+08 4.0187e+10 2993.7\n- 홈런.9   1 3.2088e+08 4.0202e+10 2993.8\n- 블론     1 3.7442e+08 4.0255e+10 2994.0\n- 팀명     8 4.2683e+09 4.4149e+10 2994.0\n- 이닝     1 4.8193e+08 4.0363e+10 2994.4\n- 선발     1 5.2257e+08 4.0403e+10 2994.5\n&lt;none&gt;                  3.9881e+10 2994.6\n- RA9.WAR  1 6.8061e+08 4.0561e+10 2995.1\n- 패       1 8.1448e+08 4.0695e+10 2995.6\n- 경기     1 1.2599e+09 4.1141e+10 2997.3\n- 세       1 1.8994e+09 4.1780e+10 2999.6\n- 승       1 2.8139e+09 4.2695e+10 3002.9\n- WAR      1 1.0124e+10 5.0005e+10 3026.9\n\nStep:  AIC=2993.09\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 홀드 + 블론 + 경기 + \n    선발 + 이닝 + 홈런.9 + BABIP + RA9.WAR + FIP + kFIP + \n    WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 홀드     1 1.7588e+08 4.0196e+10 2991.8\n- 팀명     8 4.1448e+09 4.4165e+10 2992.1\n- BABIP    1 2.8736e+08 4.0307e+10 2992.2\n- FIP      1 3.6095e+08 4.0381e+10 2992.5\n- 블론     1 3.7329e+08 4.0393e+10 2992.5\n- kFIP     1 3.7797e+08 4.0398e+10 2992.5\n- 이닝     1 4.8502e+08 4.0505e+10 2992.9\n&lt;none&gt;                  4.0020e+10 2993.1\n- 홈런.9   1 5.5115e+08 4.0571e+10 2993.2\n- 선발     1 5.7475e+08 4.0595e+10 2993.3\n- RA9.WAR  1 6.6202e+08 4.0682e+10 2993.6\n- 패       1 7.4696e+08 4.0767e+10 2993.9\n- 경기     1 1.2608e+09 4.1281e+10 2995.8\n- 세       1 1.8969e+09 4.1917e+10 2998.1\n- 승       1 2.8263e+09 4.2846e+10 3001.5\n- WAR      1 1.0042e+10 5.0062e+10 3025.1\n\nStep:  AIC=2991.76\n연봉.2018. ~ 팀명 + 승 + 패 + 세 + 블론 + 경기 + 선발 + \n    이닝 + 홈런.9 + BABIP + RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 팀명     8 4.1114e+09 4.4307e+10 2990.6\n- BABIP    1 2.6339e+08 4.0459e+10 2990.8\n- FIP      1 3.3802e+08 4.0534e+10 2991.0\n- kFIP     1 3.5713e+08 4.0553e+10 2991.1\n- 이닝     1 3.7841e+08 4.0574e+10 2991.2\n- 블론     1 4.5054e+08 4.0646e+10 2991.4\n- 홈런.9   1 5.1915e+08 4.0715e+10 2991.7\n&lt;none&gt;                  4.0196e+10 2991.8\n- RA9.WAR  1 5.4548e+08 4.0741e+10 2991.8\n- 선발     1 5.8729e+08 4.0783e+10 2992.0\n- 패       1 5.9531e+08 4.0791e+10 2992.0\n- 경기     1 1.1148e+09 4.1311e+10 2993.9\n- 세       1 1.8595e+09 4.2055e+10 2996.6\n- 승       1 2.7664e+09 4.2962e+10 2999.9\n- WAR      1 1.0302e+10 5.0498e+10 3024.4\n\nStep:  AIC=2990.56\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    홈런.9 + BABIP + RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- BABIP    1 1.3853e+08 4.4446e+10 2989.0\n- FIP      1 2.2335e+08 4.4531e+10 2989.3\n- kFIP     1 2.3816e+08 4.4545e+10 2989.4\n- 홈런.9   1 3.3332e+08 4.4641e+10 2989.7\n- 패       1 3.9536e+08 4.4703e+10 2989.9\n- 이닝     1 4.4717e+08 4.4754e+10 2990.1\n- 블론     1 5.5604e+08 4.4863e+10 2990.5\n&lt;none&gt;                  4.4307e+10 2990.6\n- 선발     1 6.3999e+08 4.4947e+10 2990.7\n- RA9.WAR  1 6.8962e+08 4.4997e+10 2990.9\n- 경기     1 1.3361e+09 4.5643e+10 2993.1\n- 세       1 1.9552e+09 4.6262e+10 2995.1\n- 승       1 2.6788e+09 4.6986e+10 2997.5\n- WAR      1 1.0270e+10 5.4577e+10 3020.2\n\nStep:  AIC=2989.04\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    홈런.9 + RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 홈런.9   1 3.5998e+08 4.4806e+10 2988.3\n- 패       1 4.0545e+08 4.4851e+10 2988.4\n- FIP      1 4.1545e+08 4.4861e+10 2988.4\n- 이닝     1 4.4992e+08 4.4896e+10 2988.6\n- kFIP     1 4.7433e+08 4.4920e+10 2988.7\n- 블론     1 5.6820e+08 4.5014e+10 2989.0\n&lt;none&gt;                  4.4446e+10 2989.0\n- RA9.WAR  1 6.0184e+08 4.5048e+10 2989.1\n- 선발     1 6.3237e+08 4.5078e+10 2989.2\n- 경기     1 1.2656e+09 4.5711e+10 2991.3\n- 세       1 1.9450e+09 4.6391e+10 2993.6\n- 승       1 2.6400e+09 4.7086e+10 2995.8\n- WAR      1 1.0158e+10 5.4604e+10 3018.3\n\nStep:  AIC=2988.26\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    RA9.WAR + FIP + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- FIP      1   62772605 4.4869e+10 2986.5\n- kFIP     1  124620325 4.4930e+10 2986.7\n- 패       1  371798316 4.5178e+10 2987.5\n- 블론     1  533889189 4.5340e+10 2988.1\n- 이닝     1  561530976 4.5367e+10 2988.2\n&lt;none&gt;                  4.4806e+10 2988.3\n- RA9.WAR  1  625829790 4.5432e+10 2988.4\n- 선발     1  699410722 4.5505e+10 2988.6\n- 경기     1 1251823806 4.6058e+10 2990.4\n- 세       1 2022180523 4.6828e+10 2993.0\n- 승       1 2613754569 4.7419e+10 2994.9\n- WAR      1 9971984968 5.4778e+10 3016.8\n\nStep:  AIC=2986.47\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    RA9.WAR + kFIP + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- kFIP     1  298981675 4.5167e+10 2985.5\n- 패       1  370858362 4.5239e+10 2985.7\n- 블론     1  516063089 4.5385e+10 2986.2\n- 이닝     1  573519854 4.5442e+10 2986.4\n&lt;none&gt;                  4.4869e+10 2986.5\n- RA9.WAR  1  605724065 4.5474e+10 2986.5\n- 선발     1  697733551 4.5566e+10 2986.8\n- 경기     1 1284322461 4.6153e+10 2988.8\n- 세       1 2006638182 4.6875e+10 2991.1\n- 승       1 2609962086 4.7478e+10 2993.1\n- WAR      1 9969383557 5.4838e+10 3015.0\n\nStep:  AIC=2985.48\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    RA9.WAR + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 패       1  372248481 4.5540e+10 2984.7\n- 블론     1  457113961 4.5625e+10 2985.0\n&lt;none&gt;                  4.5167e+10 2985.5\n- RA9.WAR  1  637686309 4.5805e+10 2985.6\n- 이닝     1  670888796 4.5838e+10 2985.7\n- 선발     1  759346878 4.5927e+10 2986.0\n- 경기     1 1597597737 4.6765e+10 2988.8\n- 세       1 2081882764 4.7249e+10 2990.3\n- 승       1 2810049547 4.7978e+10 2992.7\n- WAR      1 9832711516 5.5000e+10 3013.4\n\nStep:  AIC=2984.73\n연봉.2018. ~ 승 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    RA9.WAR + WAR\n\n          Df  Sum of Sq        RSS    AIC\n- 블론     1  311526554 4.5851e+10 2983.8\n- RA9.WAR  1  409065657 4.5949e+10 2984.1\n&lt;none&gt;                  4.5540e+10 2984.7\n- 이닝     1  608436766 4.6148e+10 2984.8\n- 선발     1 1283431857 4.6823e+10 2987.0\n- 세       1 1938650428 4.7478e+10 2989.1\n- 경기     1 2032045528 4.7572e+10 2989.4\n- 승       1 3013869783 4.8554e+10 2992.5\n- WAR      1 9464923013 5.5005e+10 3011.4\n\nStep:  AIC=2983.77\n연봉.2018. ~ 승 + 세 + 경기 + 선발 + 이닝 + RA9.WAR + \n    WAR\n\n          Df  Sum of Sq        RSS    AIC\n- RA9.WAR  1  530155684 4.6381e+10 2983.5\n&lt;none&gt;                  4.5851e+10 2983.8\n- 이닝     1  609450595 4.6461e+10 2983.8\n- 선발     1 1374006230 4.7225e+10 2986.3\n- 경기     1 1805749334 4.7657e+10 2987.6\n- 세       1 3671562216 4.9523e+10 2993.5\n- 승       1 3747280614 4.9599e+10 2993.7\n- WAR      1 9319958340 5.5171e+10 3009.9\n\nStep:  AIC=2983.52\n연봉.2018. ~ 승 + 세 + 경기 + 선발 + 이닝 + WAR\n\n       Df  Sum of Sq        RSS    AIC\n- 이닝  1 3.6450e+08 4.6746e+10 2982.7\n&lt;none&gt;               4.6381e+10 2983.5\n- 선발  1 1.0824e+09 4.7464e+10 2985.0\n- 경기  1 1.4599e+09 4.7841e+10 2986.2\n- 세    1 3.1641e+09 4.9546e+10 2991.6\n- 승    1 3.3013e+09 4.9683e+10 2992.0\n- WAR   1 1.0769e+10 5.7151e+10 3013.2\n\nStep:  AIC=2982.71\n연봉.2018. ~ 승 + 세 + 경기 + 선발 + WAR\n\n       Df  Sum of Sq        RSS    AIC\n&lt;none&gt;               4.6746e+10 2982.7\n- 선발  1 1.6437e+09 4.8390e+10 2986.0\n- 경기  1 1.8127e+09 4.8559e+10 2986.5\n- 세    1 2.9722e+09 4.9718e+10 2990.1\n- 승    1 4.2089e+09 5.0955e+10 2993.8\n- WAR   1 1.5570e+10 6.2316e+10 3024.4\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ 승 + 세 + 경기 + 선발 + WAR, \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57884  -5379  -1012   6388 100709 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8485.8     2977.2   2.850 0.005001 ** \n승            3343.3      922.1   3.626 0.000397 ***\n세            1074.1      352.5   3.047 0.002746 ** \n경기          -258.2      108.5  -2.379 0.018633 *  \n선발          -695.0      306.7  -2.266 0.024938 *  \nWAR          14803.3     2122.8   6.973 9.94e-11 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 17890 on 146 degrees of freedom\nMultiple R-squared:  0.6766,    Adjusted R-squared:  0.6656 \nF-statistic:  61.1 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nforwrad\n\nmodel_forward = step(\n m0,\n scope = 연봉.2018. ~팀명+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"forward\")\nsummary(model_forward)\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n          Df  Sum of Sq        RSS    AIC\n+ WAR      1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR  1 7.9230e+10 6.5326e+10 3025.6\n+ 승       1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝     1 6.2759e+10 8.1797e+10 3059.8\n+ 선발     1 4.5409e+10 9.9147e+10 3089.0\n+ 패       1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9   1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP     1 1.2591e+10 1.3197e+11 3132.4\n+ FIP      1 1.1403e+10 1.3315e+11 3133.8\n+ ERA      1 6.7332e+09 1.3782e+11 3139.1\n+ 세       1 6.4461e+09 1.3811e+11 3139.4\n+ 경기     1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.     1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9   1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                  1.4456e+11 3144.3\n+ 삼진.9   1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP    1 1.5139e+09 1.4304e+11 3144.7\n+ 블론     1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드     1 4.3499e+07 1.4451e+11 3146.3\n+ 팀명     8 5.7334e+09 1.3882e+11 3154.2\n\nStep:  AIC=2996.69\n연봉.2018. ~ WAR\n\n          Df  Sum of Sq        RSS    AIC\n+ 세       1 3042965364 5.0979e+10 2989.9\n+ 블론     1 3000542900 5.1021e+10 2990.0\n+ 승       1 1730511022 5.2291e+10 2993.7\n&lt;none&gt;                  5.4022e+10 2996.7\n+ 선발     1  547198547 5.3474e+10 2997.2\n+ 경기     1  428618977 5.3593e+10 2997.5\n+ 홈런.9   1  317604988 5.3704e+10 2997.8\n+ BABIP    1  195583991 5.3826e+10 2998.1\n+ RA9.WAR  1  188846074 5.3833e+10 2998.2\n+ 패       1  151963902 5.3870e+10 2998.3\n+ FIP      1  142603166 5.3879e+10 2998.3\n+ kFIP     1  135781565 5.3886e+10 2998.3\n+ 볼넷.9   1   50858403 5.3971e+10 2998.6\n+ 삼진.9   1   39564209 5.3982e+10 2998.6\n+ 홀드     1   25265599 5.3996e+10 2998.6\n+ LOB.     1   19739417 5.4002e+10 2998.6\n+ ERA      1   12195403 5.4009e+10 2998.7\n+ 이닝     1        121 5.4022e+10 2998.7\n+ 팀명     8 3656082069 5.0366e+10 3002.0\n\nStep:  AIC=2989.88\n연봉.2018. ~ WAR + 세\n\n          Df  Sum of Sq        RSS    AIC\n+ 승       1 1858274364 4.9120e+10 2986.2\n+ 블론     1  703141829 5.0276e+10 2989.8\n&lt;none&gt;                  5.0979e+10 2989.9\n+ kFIP     1  564478145 5.0414e+10 2990.2\n+ FIP      1  499506487 5.0479e+10 2990.4\n+ 홈런.9   1  441647752 5.0537e+10 2990.6\n+ 삼진.9   1  234640991 5.0744e+10 2991.2\n+ 패       1  176757884 5.0802e+10 2991.3\n+ BABIP    1   89297926 5.0889e+10 2991.6\n+ 이닝     1   24856278 5.0954e+10 2991.8\n+ LOB.     1   18540947 5.0960e+10 2991.8\n+ ERA      1   15239596 5.0963e+10 2991.8\n+ 경기     1   10174902 5.0968e+10 2991.8\n+ 선발     1    9841794 5.0969e+10 2991.8\n+ 홀드     1    1981457 5.0977e+10 2991.9\n+ 볼넷.9   1    1492812 5.0977e+10 2991.9\n+ RA9.WAR  1    1428169 5.0977e+10 2991.9\n+ 팀명     8 3587506089 4.7391e+10 2994.8\n\nStep:  AIC=2986.24\n연봉.2018. ~ WAR + 세 + 승\n\n          Df  Sum of Sq        RSS    AIC\n+ 패       1 1305968883 4.7814e+10 2984.1\n+ 이닝     1 1275519767 4.7845e+10 2984.2\n+ 경기     1  730779643 4.8390e+10 2986.0\n&lt;none&gt;                  4.9120e+10 2986.2\n+ 선발     1  561726990 4.8559e+10 2986.5\n+ kFIP     1  494014668 4.8626e+10 2986.7\n+ FIP      1  442650045 4.8678e+10 2986.9\n+ RA9.WAR  1  413259850 4.8707e+10 2986.9\n+ 홈런.9   1  292811334 4.8828e+10 2987.3\n+ 삼진.9   1  142938918 4.8977e+10 2987.8\n+ 블론     1  137415116 4.8983e+10 2987.8\n+ 홀드     1  128201381 4.8992e+10 2987.8\n+ ERA      1   73246153 4.9047e+10 2988.0\n+ 볼넷.9   1   32208563 4.9088e+10 2988.1\n+ LOB.     1   30027104 4.9090e+10 2988.1\n+ BABIP    1    2377709 4.9118e+10 2988.2\n+ 팀명     8 3476369083 4.5644e+10 2991.1\n\nStep:  AIC=2984.14\n연봉.2018. ~ WAR + 세 + 승 + 패\n\n          Df  Sum of Sq        RSS    AIC\n+ RA9.WAR  1  704822673 4.7110e+10 2983.9\n&lt;none&gt;                  4.7814e+10 2984.1\n+ 경기     1  516517842 4.7298e+10 2984.5\n+ kFIP     1  515712588 4.7299e+10 2984.5\n+ FIP      1  452768717 4.7362e+10 2984.7\n+ 홈런.9   1  357555270 4.7457e+10 2985.0\n+ 이닝     1  270900219 4.7544e+10 2985.3\n+ 블론     1  236208897 4.7578e+10 2985.4\n+ 삼진.9   1  197097985 4.7617e+10 2985.5\n+ LOB.     1  147844653 4.7667e+10 2985.7\n+ ERA      1   85521777 4.7729e+10 2985.9\n+ 홀드     1   80301276 4.7734e+10 2985.9\n+ 선발     1   20412125 4.7794e+10 2986.1\n+ BABIP    1    5999905 4.7808e+10 2986.1\n+ 볼넷.9   1     382936 4.7814e+10 2986.1\n+ 팀명     8 4048889828 4.3766e+10 2986.7\n\nStep:  AIC=2983.88\n연봉.2018. ~ WAR + 세 + 승 + 패 + RA9.WAR\n\n         Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                 4.7110e+10 2983.9\n+ kFIP    1  553776772 4.6556e+10 2984.1\n+ 경기    1  545131184 4.6564e+10 2984.1\n+ FIP     1  465512200 4.6644e+10 2984.4\n+ 홈런.9  1  336663743 4.6773e+10 2984.8\n+ 삼진.9  1  302647038 4.6807e+10 2984.9\n+ 블론    1  129249402 4.6980e+10 2985.5\n+ 홀드    1   90189291 4.7019e+10 2985.6\n+ BABIP   1   71631104 4.7038e+10 2985.7\n+ 이닝    1   42895844 4.7067e+10 2985.7\n+ ERA     1   16033110 4.7094e+10 2985.8\n+ 선발    1    5907023 4.7104e+10 2985.9\n+ LOB.    1    4581529 4.7105e+10 2985.9\n+ 볼넷.9  1      63618 4.7110e+10 2985.9\n+ 팀명    8 3840270065 4.3269e+10 2987.0\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ WAR + 세 + 승 + 패 + RA9.WAR, \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57203  -4481   -518   5092  98660 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4640.9     2190.1   2.119 0.035783 *  \nWAR          17999.2     2913.9   6.177 6.16e-09 ***\n세            1105.3      317.5   3.482 0.000658 ***\n승            2831.9      837.7   3.381 0.000928 ***\n패           -1505.7      676.7  -2.225 0.027609 *  \nRA9.WAR      -3428.2     2319.5  -1.478 0.141573    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 17960 on 146 degrees of freedom\nMultiple R-squared:  0.6741,    Adjusted R-squared:  0.6629 \nF-statistic:  60.4 on 5 and 146 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "posts/AS3_3.html#regsubsets",
    "href": "posts/AS3_3.html#regsubsets",
    "title": "AS HW3_3(야구 투수)",
    "section": "regsubsets",
    "text": "regsubsets\n\nlibrary(leaps)\n\n\nfit &lt;- regsubsets(연봉.2018.~., data=dt, nbest=1, nvmax=30, method='exhaustive',)\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(연봉.2018. ~ ., data = dt, nbest = 1, nvmax = 30, \n    method = \"exhaustive\", )\n26 Variables  (and intercept)\n         Forced in Forced out\n팀명KT       FALSE      FALSE\n팀명LG       FALSE      FALSE\n팀명NC       FALSE      FALSE\n팀명SK       FALSE      FALSE\n팀명두산     FALSE      FALSE\n팀명롯데     FALSE      FALSE\n팀명삼성     FALSE      FALSE\n팀명한화     FALSE      FALSE\n승           FALSE      FALSE\n패           FALSE      FALSE\n세           FALSE      FALSE\n홀드         FALSE      FALSE\n블론         FALSE      FALSE\n경기         FALSE      FALSE\n선발         FALSE      FALSE\n이닝         FALSE      FALSE\n삼진.9       FALSE      FALSE\n볼넷.9       FALSE      FALSE\n홈런.9       FALSE      FALSE\nBABIP        FALSE      FALSE\nLOB.         FALSE      FALSE\nERA          FALSE      FALSE\nRA9.WAR      FALSE      FALSE\nFIP          FALSE      FALSE\nkFIP         FALSE      FALSE\nWAR          FALSE      FALSE\n1 subsets of each size up to 26\nSelection Algorithm: exhaustive\n          팀명KT 팀명LG 팀명NC 팀명SK 팀명두산 팀명롯데 팀명삼성 팀명한화 승 \n1  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \" \"      \" \"\n2  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \" \"      \" \"\n3  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \"*\"      \" \"\n4  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \"*\"      \"*\"\n5  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \"*\"      \"*\"\n6  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \"*\"      \"*\"\n7  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \" \"      \"*\"      \"*\"\n8  ( 1 )  \" \"    \" \"    \"*\"    \" \"    \" \"      \" \"      \" \"      \"*\"      \"*\"\n9  ( 1 )  \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \"*\"      \"*\"      \"*\"\n10  ( 1 ) \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \"*\"      \"*\"      \"*\"\n11  ( 1 ) \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \"*\"      \"*\"      \"*\"\n12  ( 1 ) \" \"    \" \"    \" \"    \" \"    \"*\"      \" \"      \"*\"      \"*\"      \"*\"\n13  ( 1 ) \" \"    \" \"    \" \"    \" \"    \" \"      \" \"      \"*\"      \"*\"      \"*\"\n14  ( 1 ) \" \"    \" \"    \"*\"    \" \"    \"*\"      \" \"      \" \"      \"*\"      \"*\"\n15  ( 1 ) \" \"    \" \"    \"*\"    \"*\"    \"*\"      \" \"      \" \"      \"*\"      \"*\"\n16  ( 1 ) \"*\"    \" \"    \"*\"    \"*\"    \"*\"      \" \"      \" \"      \"*\"      \"*\"\n17  ( 1 ) \"*\"    \" \"    \"*\"    \"*\"    \"*\"      \" \"      \" \"      \"*\"      \"*\"\n18  ( 1 ) \"*\"    \" \"    \"*\"    \"*\"    \"*\"      \" \"      \" \"      \"*\"      \"*\"\n19  ( 1 ) \"*\"    \" \"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n20  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n21  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n22  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n23  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n24  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n25  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \" \"      \"*\"      \"*\"\n26  ( 1 ) \"*\"    \"*\"    \"*\"    \"*\"    \"*\"      \"*\"      \"*\"      \"*\"      \"*\"\n          패  세  홀드 블론 경기 선발 이닝 삼진.9 볼넷.9 홈런.9 BABIP LOB. ERA\n1  ( 1 )  \" \" \" \" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n2  ( 1 )  \" \" \"*\" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n3  ( 1 )  \" \" \"*\" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n4  ( 1 )  \" \" \"*\" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n5  ( 1 )  \"*\" \"*\" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n6  ( 1 )  \" \" \"*\" \" \"  \" \"  \"*\"  \"*\"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n7  ( 1 )  \" \" \"*\" \" \"  \" \"  \"*\"  \"*\"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n8  ( 1 )  \"*\" \"*\" \" \"  \" \"  \" \"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n9  ( 1 )  \"*\" \"*\" \" \"  \"*\"  \"*\"  \" \"  \" \"  \" \"    \" \"    \" \"    \" \"   \" \"  \" \"\n10  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \" \"  \" \"  \"*\"    \" \"    \"*\"    \" \"   \" \"  \" \"\n11  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \" \"  \" \"  \"*\"    \" \"    \"*\"    \" \"   \" \"  \" \"\n12  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \" \"  \" \"  \"*\"    \" \"    \"*\"    \" \"   \" \"  \" \"\n13  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \" \"   \" \"  \" \"\n14  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \"*\"  \"*\"  \" \"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n15  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \"*\"  \"*\"  \" \"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n16  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \" \"   \" \"  \" \"\n17  ( 1 ) \"*\" \"*\" \" \"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n18  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n19  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n20  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n21  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \" \"    \"*\"    \"*\"   \" \"  \" \"\n22  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \" \"    \"*\"    \"*\"    \"*\"   \" \"  \" \"\n23  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \" \"    \"*\"    \"*\"    \"*\"   \"*\"  \" \"\n24  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \" \"    \"*\"    \"*\"    \"*\"   \"*\"  \"*\"\n25  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \"*\"    \"*\"    \"*\"   \"*\"  \"*\"\n26  ( 1 ) \"*\" \"*\" \"*\"  \"*\"  \"*\"  \"*\"  \"*\"  \"*\"    \"*\"    \"*\"    \"*\"   \"*\"  \"*\"\n          RA9.WAR FIP kFIP WAR\n1  ( 1 )  \" \"     \" \" \" \"  \"*\"\n2  ( 1 )  \" \"     \" \" \" \"  \"*\"\n3  ( 1 )  \" \"     \" \" \" \"  \"*\"\n4  ( 1 )  \" \"     \" \" \" \"  \"*\"\n5  ( 1 )  \" \"     \" \" \" \"  \"*\"\n6  ( 1 )  \" \"     \" \" \" \"  \"*\"\n7  ( 1 )  \" \"     \" \" \"*\"  \"*\"\n8  ( 1 )  \"*\"     \" \" \"*\"  \"*\"\n9  ( 1 )  \" \"     \" \" \"*\"  \"*\"\n10  ( 1 ) \" \"     \" \" \" \"  \"*\"\n11  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n12  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n13  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n14  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n15  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n16  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n17  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n18  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n19  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n20  ( 1 ) \"*\"     \" \" \" \"  \"*\"\n21  ( 1 ) \"*\"     \" \" \"*\"  \"*\"\n22  ( 1 ) \"*\"     \"*\" \"*\"  \"*\"\n23  ( 1 ) \"*\"     \"*\" \"*\"  \"*\"\n24  ( 1 ) \"*\"     \"*\" \"*\"  \"*\"\n25  ( 1 ) \"*\"     \"*\" \"*\"  \"*\"\n26  ( 1 ) \"*\"     \"*\" \"*\"  \"*\"\n\n\n\nwith(summary(fit), round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\n\nA matrix: 26 × 32 of type dbl\n\n\n\n(Intercept)\n팀명KT\n팀명LG\n팀명NC\n팀명SK\n팀명두산\n팀명롯데\n팀명삼성\n팀명한화\n승\n⋯\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n1\n54021608997\n0.626\n0.624\n21.711\n-139.564\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n0\n0\n0\n0\n1\n50978643633\n0.647\n0.643\n14.151\n-143.353\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n⋯\n0\n0\n0\n0\n1\n48968810756\n0.661\n0.654\n9.837\n-144.443\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n⋯\n0\n0\n0\n0\n1\n47054642635\n0.674\n0.666\n5.824\n-145.480\n\n\n5\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n⋯\n0\n0\n0\n0\n1\n45225361341\n0.687\n0.676\n2.077\n-146.483\n\n\n6\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n⋯\n0\n0\n0\n0\n1\n44352191072\n0.693\n0.680\n1.334\n-144.422\n\n\n7\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n⋯\n0\n0\n0\n1\n1\n43873959120\n0.696\n0.682\n1.832\n-141.046\n\n\n8\n1\n0\n0\n1\n0\n0\n0\n0\n1\n1\n⋯\n0\n1\n0\n1\n1\n43490253876\n0.699\n0.682\n2.626\n-137.358\n\n\n9\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n⋯\n0\n0\n0\n1\n1\n43014193386\n0.702\n0.684\n3.131\n-134.007\n\n\n10\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n⋯\n0\n0\n0\n0\n1\n42429881012\n0.706\n0.686\n3.295\n-131.062\n\n\n11\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n⋯\n0\n1\n0\n0\n1\n41942014969\n0.710\n0.687\n3.762\n-127.796\n\n\n12\n1\n0\n0\n0\n0\n1\n0\n1\n1\n1\n⋯\n0\n1\n0\n0\n1\n41744791367\n0.711\n0.686\n5.143\n-123.488\n\n\n13\n1\n0\n0\n0\n0\n0\n0\n1\n1\n1\n⋯\n0\n1\n0\n0\n1\n41540927294\n0.713\n0.686\n6.502\n-119.209\n\n\n14\n1\n0\n0\n1\n0\n1\n0\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n41256884117\n0.715\n0.685\n7.610\n-115.228\n\n\n15\n1\n0\n0\n1\n1\n1\n0\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n41044826480\n0.716\n0.685\n8.944\n-110.987\n\n\n16\n1\n1\n0\n1\n1\n1\n0\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n40671498634\n0.719\n0.685\n9.771\n-107.352\n\n\n17\n1\n1\n0\n1\n1\n1\n0\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n40455404484\n0.720\n0.685\n11.092\n-103.138\n\n\n18\n1\n1\n0\n1\n1\n1\n0\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n40307821620\n0.721\n0.683\n12.628\n-98.669\n\n\n19\n1\n1\n0\n1\n1\n1\n1\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n40204649863\n0.722\n0.682\n14.304\n-94.035\n\n\n20\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n0\n1\n0\n0\n1\n40105370572\n0.723\n0.680\n15.992\n-89.387\n\n\n21\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n0\n1\n0\n1\n1\n39985916879\n0.723\n0.679\n17.617\n-84.817\n\n\n22\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n0\n1\n1\n1\n1\n39886900175\n0.724\n0.677\n19.306\n-80.170\n\n\n23\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n0\n1\n1\n1\n1\n39818646734\n0.725\n0.675\n21.092\n-75.406\n\n\n24\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n1\n1\n1\n1\n1\n39807215204\n0.725\n0.673\n23.056\n-70.426\n\n\n25\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n⋯\n1\n1\n1\n1\n1\n39796108554\n0.725\n0.670\n25.021\n-65.444\n\n\n26\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n⋯\n1\n1\n1\n1\n1\n39789454039\n0.725\n0.667\n27.000\n-60.446"
  },
  {
    "objectID": "posts/AS4.html",
    "href": "posts/AS4.html",
    "title": "AS HW4",
    "section": "",
    "text": "library(car)\nlibrary(MASS)\nlibrary(glmnet)\n\nLoading required package: carData\n\nLoading required package: Matrix\n\nLoaded glmnet 4.1-7"
  },
  {
    "objectID": "posts/AS4.html#section",
    "href": "posts/AS4.html#section",
    "title": "AS HW4",
    "section": "(1)",
    "text": "(1)\n변수들 사이의 산점도행렬을 그리고 설명하여라.\n\npairs(ex4)\n\n\n\n\n\nhipcenter와 대부분 음의 상관관계가 있어보이고, Age는 관계가 없어보인다.\n다른 설명변수들간에도 서로 양의 상관관계가 있어보인다. Age는 다른 변수들과도 관계가 없어 보임."
  },
  {
    "objectID": "posts/AS4.html#section-1",
    "href": "posts/AS4.html#section-1",
    "title": "AS HW4",
    "section": "(2)",
    "text": "(2)\n변수들 사이의 상관계수를 구하여라. (소수점 둘째자리까지 반올림) 다중공선성이 존재한다고 할 수 있는가?\n\nround(cor(ex4),2)\n\n\nA matrix: 9 × 9 of type dbl\n\n\n\nAge\nWeight\nHtShoes\nHt\nSeated\nArm\nThigh\nLeg\nhipcenter\n\n\n\n\nAge\n1.00\n0.08\n-0.08\n-0.09\n-0.17\n0.36\n0.09\n-0.04\n0.21\n\n\nWeight\n0.08\n1.00\n0.83\n0.83\n0.78\n0.70\n0.57\n0.78\n-0.64\n\n\nHtShoes\n-0.08\n0.83\n1.00\n1.00\n0.93\n0.75\n0.72\n0.91\n-0.80\n\n\nHt\n-0.09\n0.83\n1.00\n1.00\n0.93\n0.75\n0.73\n0.91\n-0.80\n\n\nSeated\n-0.17\n0.78\n0.93\n0.93\n1.00\n0.63\n0.61\n0.81\n-0.73\n\n\nArm\n0.36\n0.70\n0.75\n0.75\n0.63\n1.00\n0.67\n0.75\n-0.59\n\n\nThigh\n0.09\n0.57\n0.72\n0.73\n0.61\n0.67\n1.00\n0.65\n-0.59\n\n\nLeg\n-0.04\n0.78\n0.91\n0.91\n0.81\n0.75\n0.65\n1.00\n-0.79\n\n\nhipcenter\n0.21\n-0.64\n-0.80\n-0.80\n-0.73\n-0.59\n-0.59\n-0.79\n1.00\n\n\n\n\n\n\n각 변수들간의 상관관계가 대체적으로 높은 편이며 양의 상관관계를 가지는 경우가 많다."
  },
  {
    "objectID": "posts/AS4.html#section-2",
    "href": "posts/AS4.html#section-2",
    "title": "AS HW4",
    "section": "(3)",
    "text": "(3)\n선형회귀모형을 적합하여라. (계수를 추정. 예 : \\(\\hat y = 3 + 2x\\) 이렇게 적어주기)\n\nfit &lt;- lm(hipcenter~., ex4)\nsummary(fit)\n\n\nCall:\nlm(formula = hipcenter ~ ., data = ex4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-73.827 -22.833  -3.678  25.017  62.337 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) 436.43213  166.57162   2.620   0.0138 *\nAge           0.77572    0.57033   1.360   0.1843  \nWeight        0.02631    0.33097   0.080   0.9372  \nHtShoes      -2.69241    9.75304  -0.276   0.7845  \nHt            0.60134   10.12987   0.059   0.9531  \nSeated        0.53375    3.76189   0.142   0.8882  \nArm          -1.32807    3.90020  -0.341   0.7359  \nThigh        -1.14312    2.66002  -0.430   0.6706  \nLeg          -6.43905    4.71386  -1.366   0.1824  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 37.72 on 29 degrees of freedom\nMultiple R-squared:  0.6866,    Adjusted R-squared:  0.6001 \nF-statistic:  7.94 on 8 and 29 DF,  p-value: 1.306e-05\n\n\n\\(\\widehat {hipcenter}=436.43+0.78 \\widehat{Age} + 0.03 \\widehat{Weight} - 2.69 \\widehat{HtShoes}+0.60\\widehat{Ht}+0.53\\widehat{Seated}-1.33\\widehat{Arm}-1.14\\widehat{Thigh}-6.44\\widehat{Leg}\\)"
  },
  {
    "objectID": "posts/AS4.html#section-3",
    "href": "posts/AS4.html#section-3",
    "title": "AS HW4",
    "section": "(4)",
    "text": "(4)\n위의 선형회귀모형에서 개별 회귀계수의 유의성 검정 결과와 (1),(2)번 결과와 연관지어 설명하여라.\n회귀모형의 p-value값은 1.306e-05으로 유의하고 \\(R^2\\)값도 68%정도로 약간 높은 편이지만 회귀계수는 절편을 제외하고는(절편도..그닥.) 모두 유의하지 않다. 이와 같은 경우는 다중공산성이 존재하기 때문이다."
  },
  {
    "objectID": "posts/AS4.html#section-4",
    "href": "posts/AS4.html#section-4",
    "title": "AS HW4",
    "section": "(5)",
    "text": "(5)\ncar 패키지의 vif 함수를 이용하여 각 설명변수의 vif값을 구하여라.\n\nvif(fit)\n\nAge1.99793147706444Weight3.64703012881283HtShoes307.429378016771Ht333.137832387527Seated8.95105380157058Arm4.49636844261058Thigh2.76288552162407Leg6.69429122446916\n\n\n\nvif의 값이 10이상이면 다중공산성이 있다고 보자.\nHtShoes,Ht의 값이 높은 값을 가지며 다중곤상성이 있다."
  },
  {
    "objectID": "posts/AS4.html#section-5",
    "href": "posts/AS4.html#section-5",
    "title": "AS HW4",
    "section": "(6)",
    "text": "(6)\nglmnet 함수를 이용하여 능형회귀모형을 적합하고자 한다.\n\n(a)\n\nX &lt;- model.matrix(hipcenter~., ex4)[,-1] \ny &lt;- ex4$hipcenter\n\nλ 의 값에 따라 회귀계수가 변하는지 그림으로 보여라. (이 때 0 &lt; λ &lt; 100 으로 설정)\n\nridge.fit&lt;-glmnet(X,y,alpha=0, lambda=seq(0,100,0.1)) ##ridge : alpha=0 \nplot(ridge.fit, label=TRUE)\nabline(h=0, col=\"grey\", lty=2)\n\n\n\n\n\n\n(b)\n교차검증을 수행하였을 때, MSE를 가장 작게 하는 λ 는 무엇인가?\n\nX &lt;- model.matrix(hipcenter~., ex4)[,-1] \ny &lt;- ex4$hipcenter\n\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=length(y))\n\nWarning message:\n“Option grouped=FALSE enforced in cv.glmnet, since &lt; 3 observations per fold”\n\n\n\ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = length(y), alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure    SE Nonzero\nmin  36.41    78    1442 315.7       8\n1se 213.24    59    1750 411.7       8\n\n\n\n\\(\\lambda\\)=36.41일 때 MSE가 가장 작다.\n\n\n\n(c) 이게 맞나…\n위의 λ에 대한 능형회귀 추정량은 무엇인가?\n\nlam&lt;-cv.fit$lambda.min;lam\n\n36.4079887395858\n\n\n\nlog(lam)\n\n3.59478822143023\n\n\n\npredict(ridge.fit,type=\"coefficients\",s=lam)\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept) 400.1966209\nAge           0.4741535\nWeight       -0.1060971\nHtShoes      -0.7390462\nHt           -0.7374872\nSeated       -1.2030446\nArm          -1.3553194\nThigh        -1.3044858\nLeg          -3.1112979\n\n\n\nbeta &lt;- predict(ridge.fit,type=\"coefficients\",s=lam)\n\n\nbeta\n\n9 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept) 400.1966209\nAge           0.4741535\nWeight       -0.1060971\nHtShoes      -0.7390462\nHt           -0.7374872\nSeated       -1.2030446\nArm          -1.3553194\nThigh        -1.3044858\nLeg          -3.1112979"
  },
  {
    "objectID": "posts/AS4.html#section-6",
    "href": "posts/AS4.html#section-6",
    "title": "AS HW4",
    "section": "(1)",
    "text": "(1)\n나이를 예측변수로 하여 척추후만증에 대한 로지스틱 회귀모형을 적합하라. (모형을 정확하게 기재)\n\nmodel &lt;- glm(kyphosis ~ age, data = dt, family = binomial())\n\n\nsummary(model)\n\n\nCall:\nglm(formula = kyphosis ~ age, family = binomial(), data = dt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3126  -1.0907  -0.9482   1.2170   1.4052  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.572693   0.602395  -0.951    0.342\nage          0.004296   0.005849   0.734    0.463\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.051  on 39  degrees of freedom\nResidual deviance: 54.504  on 38  degrees of freedom\nAIC: 58.504\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\widehat {kyphosis} = -0.5727 +0.0043 \\widehat{age}\\)"
  },
  {
    "objectID": "posts/AS4.html#section-7",
    "href": "posts/AS4.html#section-7",
    "title": "AS HW4",
    "section": "(2)",
    "text": "(2)\n나이가 유의한 효과를 갖는지를 검정하라.\nage의 z-value의 값은 0.734로 0.05보다 크므로 유의하지 않다."
  },
  {
    "objectID": "posts/AS4.html#section-8",
    "href": "posts/AS4.html#section-8",
    "title": "AS HW4",
    "section": "(3)",
    "text": "(3)\n자료에 대한 산점도를 그려라. 단, 척추후만증의 두 수준에서 나이의 분포가 차이가 나는 것을 주목하라.(상자그림 확인)\n\nboxplot(age ~ kyphosis, data = dt, cex.axis = 0.8,\n        xlab = \"Kyphosis\", ylab = \"Age\", col = c(\"steelblue\", \"darkorange\"))"
  },
  {
    "objectID": "posts/AS4.html#section-9",
    "href": "posts/AS4.html#section-9",
    "title": "AS HW4",
    "section": "(4)",
    "text": "(4)\n로지스틱 회귀모형\n\\[logit(Pr(x))= log \\dfrac{Pr(y = 1|x)}{1 − Pr(y = 1|x)}= α + β_1x + β_2x^2\\]\n를 적합시킨 후에 나이의 제곱합에 대한 유의성검정을 하여라\n\nmodel2 &lt;- glm(kyphosis ~ age + I(age^2), data = dt, family = binomial())\nsummary(model2)\n\n\nCall:\nglm(formula = kyphosis ~ age + I(age^2), family = binomial(), \n    data = dt)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.482  -1.009  -0.507   1.012   1.788  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -2.0462547  0.9943478  -2.058   0.0396 *\nage          0.0600398  0.0267808   2.242   0.0250 *\nI(age^2)    -0.0003279  0.0001564  -2.097   0.0360 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.051  on 39  degrees of freedom\nResidual deviance: 48.228  on 37  degrees of freedom\nAIC: 54.228\n\nNumber of Fisher Scoring iterations: 4\n\n\n\\(\\widehat {kyphosis} = -2.0462547 + 0.0600398 \\widehat{age} - 0.0003279\\widehat{age}^2\\)"
  },
  {
    "objectID": "posts/AS4.html#section-10",
    "href": "posts/AS4.html#section-10",
    "title": "AS HW4",
    "section": "(5)",
    "text": "(5)\n\\(\\hat β_2\\)의 의미를 서술하시오.\nx가 한단위 증가하면 성공의 오즈는 e^(beta2)배 만큼 증가한다."
  },
  {
    "objectID": "posts/AS4.html#section-11",
    "href": "posts/AS4.html#section-11",
    "title": "AS HW4",
    "section": "(1)",
    "text": "(1)\n사형선고에 대한 추정확률은 피고인이 백인(D=0)이고 피해자가 흑인(V=1)일 때 가장 낮다.\n\nT\n\nTRUE\n\n\nlogit(Pr(y = 1|x)) = -2.06 + 0.87D - 2.4V 에서\nD=0, V=1대입하면\nlogit(Pr(y = 1|x)) = -2.06 + 0.87(0) - 2.4(1) = -2.06 - 2.4 = -4.46\n각 식에 e를 곱해주면\nPr(y = 1|x) = exp(-4.46) / (1 + exp(-4.46))이다.\n피고인이 백인이고 피해자가 흑인일때 사형이 선고될 예상 확률이다.\nD=0, V=0 넣으면 -2.06\nD=1, V=0 넣으면 -1.19\nD=1, V=1 넣으면 -3.59"
  },
  {
    "objectID": "posts/AS4.html#다시",
    "href": "posts/AS4.html#다시",
    "title": "AS HW4",
    "section": "(2)다시",
    "text": "(2)다시\n피해자의 인종이 주어져 있을 떄 백인 피고인(D=0)이 사형선고를 받을 오즈의 추정값은 흑인 피고인(D=1)의 오즈의 0.87배이다.\n만약 백인 피고인일 경우 D = 1이고 흑인일 경우 D = 0이면 D의 회귀계수의 추정값은 0.87이 아니라 1/0.87 = 1.15가 되었을 것이다.\n\nD의 회귀계수 추정값: 0.87\n\n백인 피고인의 오즈가 흑인 피고인의 오즈의 0.87배"
  },
  {
    "objectID": "posts/AS4.html#f",
    "href": "posts/AS4.html#f",
    "title": "AS HW4",
    "section": "(3) f",
    "text": "(3) f\n절편의 추정값 −2.06은 피고인과 피해자가 백인일 경우(D = V = 0)에 사형선고에 대한 추정확률이다.\nD=0,V=0일때의 값이므로 trUE"
  },
  {
    "objectID": "posts/AS4.html#section-12",
    "href": "posts/AS4.html#section-12",
    "title": "AS HW4",
    "section": "(4)",
    "text": "(4)\n만약, 백인피해자와 피고인이 500명일 때, 사형선고를 받는 사람에 대해 모형에서 적합된 도수(즉, 추정되는 기대도수)는 \\(500e^{−2.06}/(1 + e^{−2.06})\\)과 같다.\n\nv=0, D=0이므로 대입해보면, \n\n로짓값\n$logit(\\hat pr(X)=-2.06+0.87 \\times 0 - 2.4 \\times 0$\n       \n확률\n\n       $\\hat pr(y=1|X)=e^{-2.06}\n\n로짓 l 확률 p라고 놓으면\n\\(L=ln\\dfrac{p}{1-p}\\)\n역함수 \\(p=\\dfrac{1}{1+e^{-L}}\\)\n아래위로 \\(e^{-L}\\)을 곱해주면 시그모이드 함수가 나옴\n\\(p=\\dfrac{e^{-L}}{e^{-L}+1}\\)\n기대도수는 여기에다가 인원수 곱하면 됨."
  },
  {
    "objectID": "posts/AS4_5.html",
    "href": "posts/AS4_5.html",
    "title": "AS HW4_5",
    "section": "",
    "text": "python-data-analysis data\nData Source"
  },
  {
    "objectID": "posts/AS4_5.html#model1원본",
    "href": "posts/AS4_5.html#model1원본",
    "title": "AS HW4_5",
    "section": "model1(원본)",
    "text": "model1(원본)\n\nmodel1 &lt;- lm(연봉.2018. ~ ., dt)\nsummary(model1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ ., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46529  -2418    424   2649  47773 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.513e+04  1.826e+04   0.829   0.4087    \n승           1.004e+03  5.375e+02   1.869   0.0639 .  \n패          -1.836e+02  5.504e+02  -0.334   0.7392    \n세          -2.112e+01  2.713e+02  -0.078   0.9381    \n홀드        -1.817e+01  3.161e+02  -0.057   0.9542    \n블론         4.535e+02  7.610e+02   0.596   0.5522    \n경기        -1.760e+02  1.456e+02  -1.209   0.2289    \n선발        -6.719e+02  4.616e+02  -1.456   0.1479    \n이닝         7.425e+01  1.156e+02   0.642   0.5217    \n삼진.9      -4.603e+02  2.349e+03  -0.196   0.8449    \n볼넷.9       1.194e+03  2.256e+03   0.529   0.5976    \n홈런.9       4.874e+03  1.413e+04   0.345   0.7306    \nBABIP       -9.997e+03  1.486e+04  -0.673   0.5022    \nLOB.        -4.350e+01  1.299e+02  -0.335   0.7382    \nERA         -7.413e+01  5.693e+02  -0.130   0.8966    \nRA9.WAR     -7.584e+02  1.487e+03  -0.510   0.6109    \nFIP         -6.436e+03  4.477e+04  -0.144   0.8859    \nkFIP         3.805e+03  3.593e+04   0.106   0.9158    \nWAR          8.559e+03  1.789e+03   4.783 4.55e-06 ***\n연봉.2017.   8.755e-01  4.444e-02  19.698  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9198 on 132 degrees of freedom\nMultiple R-squared:  0.9228,    Adjusted R-squared:  0.9116 \nF-statistic: 82.99 on 19 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodel1 &lt;- lm(연봉.2018. ~ +WAR+연봉.2017., dt)\nsummary(model1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ +WAR + 연봉.2017., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-50442  -1849    758   2050  56166 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -576.58811  889.09610  -0.649    0.518    \nWAR         7007.17364  761.83979   9.198 3.03e-16 ***\n연봉.2017.     0.89926    0.04022  22.360  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9124 on 149 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.913 \nF-statistic: 793.8 on 2 and 149 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model1)\n\n승7.69934669638176패5.30032799820878세3.03718565968123홀드3.63605161357374블론2.75956034802692경기14.2011271199637선발36.1601878733279이닝60.3244538179009삼진.978.7161704892493볼넷.950.7257985016107홈런.9368.399308168573BABIP3.11936893312554LOB.4.04602533224649ERA10.0441738232824RA9.WAR13.4198973515016FIP12525.2424059086kFIP9046.04880487446WAR9.99177856816849연봉.2017.2.21186942564398\n\n\n\nthreshold &lt;- 10\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\n [1] \"경기\"    \"선발\"    \"이닝\"    \"삼진.9\"  \"볼넷.9\"  \"홈런.9\"  \"ERA\"    \n [8] \"RA9.WAR\" \"FIP\"     \"kFIP\"   \n\n\n\nthreshold &lt;- 15\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\n[1] \"선발\"   \"이닝\"   \"삼진.9\" \"볼넷.9\" \"홈런.9\" \"FIP\"    \"kFIP\"  \n\n\n\npairs(dt,panel=panel.smooth)\n\n\n\n\n\n수치형 데이터들끼리의 상관계수 확인..\n\n\ndt_numeric &lt;- dt[, sapply(dt, is.numeric)]\ncor_matrix &lt;- cor(dt_numeric)\nprint(round(cor_matrix,2))\n\n              승    패    세  홀드  블론  경기  선발  이닝 삼진.9 볼넷.9 홈런.9\n승          1.00  0.71  0.05  0.09  0.11  0.40  0.77  0.91   0.08  -0.40  -0.12\n패          0.71  1.00  0.07  0.10  0.12  0.34  0.77  0.83   0.03  -0.39  -0.06\n세          0.05  0.07  1.00  0.11  0.61  0.43 -0.18  0.02   0.17  -0.13  -0.07\n홀드        0.09  0.10  0.11  1.00  0.49  0.72 -0.29  0.02   0.19  -0.15  -0.08\n블론        0.11  0.12  0.61  0.49  1.00  0.63 -0.26  0.01   0.19  -0.14  -0.06\n경기        0.40  0.34  0.43  0.72  0.63  1.00 -0.04  0.38   0.19  -0.36  -0.11\n선발        0.77  0.77 -0.18 -0.29 -0.26 -0.04  1.00  0.89  -0.06  -0.31  -0.06\n이닝        0.91  0.83  0.02  0.02  0.01  0.38  0.89  1.00   0.04  -0.45  -0.11\n삼진.9      0.08  0.03  0.17  0.19  0.19  0.19 -0.06  0.04   1.00   0.11   0.22\n볼넷.9     -0.40 -0.39 -0.13 -0.15 -0.14 -0.36 -0.31 -0.45   0.11   1.00   0.30\n홈런.9     -0.12 -0.06 -0.07 -0.08 -0.06 -0.11 -0.06 -0.11   0.22   0.30   1.00\nBABIP      -0.17 -0.13 -0.09 -0.10 -0.11 -0.24 -0.10 -0.19   0.46   0.28   0.36\nLOB.        0.13 -0.02  0.17  0.05  0.10  0.11  0.04  0.10  -0.07  -0.15  -0.27\nERA        -0.27 -0.19 -0.15 -0.16 -0.16 -0.32 -0.16 -0.29   0.26   0.52   0.63\nRA9.WAR     0.85  0.60  0.17  0.00  0.01  0.28  0.74  0.85   0.10  -0.40  -0.19\nFIP        -0.30 -0.23 -0.20 -0.21 -0.21 -0.35 -0.15 -0.30  -0.15   0.63   0.83\nkFIP       -0.31 -0.24 -0.23 -0.24 -0.24 -0.37 -0.14 -0.30  -0.32   0.61   0.74\nWAR         0.82  0.63  0.08 -0.04 -0.06  0.20  0.76  0.83   0.15  -0.39  -0.21\n연봉.2017.  0.63  0.43  0.26  0.00  0.15  0.23  0.49  0.59   0.10  -0.33  -0.10\n연봉.2018.  0.71  0.47  0.21 -0.02  0.10  0.21  0.56  0.66   0.10  -0.33  -0.12\n           BABIP  LOB.   ERA RA9.WAR   FIP  kFIP   WAR 연봉.2017. 연봉.2018.\n승         -0.17  0.13 -0.27    0.85 -0.30 -0.31  0.82       0.63       0.71\n패         -0.13 -0.02 -0.19    0.60 -0.23 -0.24  0.63       0.43       0.47\n세         -0.09  0.17 -0.15    0.17 -0.20 -0.23  0.08       0.26       0.21\n홀드       -0.10  0.05 -0.16    0.00 -0.21 -0.24 -0.04       0.00      -0.02\n블론       -0.11  0.10 -0.16    0.01 -0.21 -0.24 -0.06       0.15       0.10\n경기       -0.24  0.11 -0.32    0.28 -0.35 -0.37  0.20       0.23       0.21\n선발       -0.10  0.04 -0.16    0.74 -0.15 -0.14  0.76       0.49       0.56\n이닝       -0.19  0.10 -0.29    0.85 -0.30 -0.30  0.83       0.59       0.66\n삼진.9      0.46 -0.07  0.26    0.10 -0.15 -0.32  0.15       0.10       0.10\n볼넷.9      0.28 -0.15  0.52   -0.40  0.63  0.61 -0.39      -0.33      -0.33\n홈런.9      0.36 -0.27  0.63   -0.19  0.83  0.74 -0.21      -0.10      -0.12\nBABIP       1.00 -0.51  0.73   -0.19  0.25  0.17 -0.08      -0.09      -0.10\nLOB.       -0.51  1.00 -0.72    0.29 -0.29 -0.27  0.14       0.11       0.13\nERA         0.73 -0.72  1.00   -0.34  0.65  0.58 -0.26      -0.20      -0.22\nRA9.WAR    -0.19  0.29 -0.34    1.00 -0.37 -0.38  0.92       0.64       0.74\nFIP         0.25 -0.29  0.65   -0.37  1.00  0.98 -0.39      -0.27      -0.28\nkFIP        0.17 -0.27  0.58   -0.38  0.98  1.00 -0.41      -0.28      -0.30\nWAR        -0.08  0.14 -0.26    0.92 -0.39 -0.41  1.00       0.68       0.79\n연봉.2017. -0.09  0.11 -0.20    0.64 -0.27 -0.28  0.68       1.00       0.93\n연봉.2018. -0.10  0.13 -0.22    0.74 -0.28 -0.30  0.79       0.93       1.00"
  },
  {
    "objectID": "posts/AS4_5.html#vif계수가-높은-변수-제거",
    "href": "posts/AS4_5.html#vif계수가-높은-변수-제거",
    "title": "AS HW4_5",
    "section": "VIF계수가 높은 변수 제거",
    "text": "VIF계수가 높은 변수 제거\n\nmodel2(Vif 10 이상인 변수 제거)\n\nmodel2 &lt;- lm(연봉.2018. ~ .-경기-선발-이닝-삼진.9-볼넷.9-홈런.9-ERA-RA9.WAR-FIP-kFIP, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - 경기 - 선발 - 이닝 - 삼진.9 - \n    볼넷.9 - 홈런.9 - ERA - RA9.WAR - FIP - kFIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48657  -1981    511   2303  51073 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.432e+03  7.893e+03   0.815   0.4165    \n승           4.770e+02  4.061e+02   1.175   0.2421    \n패          -7.851e+02  3.525e+02  -2.227   0.0275 *  \n세          -1.172e+02  2.150e+02  -0.545   0.5865    \n홀드        -1.229e+02  1.973e+02  -0.623   0.5344    \n블론         6.340e+02  7.188e+02   0.882   0.3792    \nBABIP       -7.810e+03  9.994e+03  -0.781   0.4358    \nLOB.        -4.979e+01  7.793e+01  -0.639   0.5239    \nWAR          7.298e+03  1.169e+03   6.243 4.67e-09 ***\n연봉.2017.   8.846e-01  4.322e-02  20.469  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9149 on 142 degrees of freedom\nMultiple R-squared:  0.9178,    Adjusted R-squared:  0.9126 \nF-statistic: 176.1 on 9 and 142 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model2)\n\n승4.44133840452701패2.19787784118271세1.9291576101908홀드1.43155944990414블론2.48814143828698BABIP1.42670331782564LOB.1.47225296358887WAR4.30937396392511연봉.2017.2.11357639082776\n\n\n\nmodel1에서 다중공산성이 높았던 변수들을 제외하고 lm을 돌렸더니, 회귀모형은 유의하게 나왔고 R^2값도 91%로 높게 나왔지만 model1보다는 R^2값이 조금 적게 나왔다.\n다중공산성이 높은 변수를 제외하는 것은 다른 것들도 확인을 해보아야 한다.\n\n\n\nVIF제거시 고려사항\n\n\nVIF계수가 높은 피처 우선 제거하되, FIP, kFIP와 같이 유사한 변수들은 두개 중에서 하나만 제거해보자.\n\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP, dt)\nsummary(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46688  -2466    423   2597  47710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.406e+04  1.660e+04   0.847    0.399    \n승           1.007e+03  5.352e+02   1.882    0.062 .  \n패          -1.723e+02  5.427e+02  -0.317    0.751    \n세          -2.263e+01  2.701e+02  -0.084    0.933    \n홀드        -1.779e+01  3.149e+02  -0.056    0.955    \n블론         4.563e+02  7.579e+02   0.602    0.548    \n경기        -1.738e+02  1.443e+02  -1.205    0.230    \n선발        -6.701e+02  4.598e+02  -1.458    0.147    \n이닝         7.216e+01  1.142e+02   0.632    0.529    \n삼진.9      -7.714e+02  9.085e+02  -0.849    0.397    \n볼넷.9       8.998e+02  9.504e+02   0.947    0.346    \n홈런.9       2.904e+03  3.404e+03   0.853    0.395    \nBABIP       -9.797e+03  1.474e+04  -0.665    0.507    \nLOB.        -4.465e+01  1.292e+02  -0.346    0.730    \nERA         -8.076e+01  5.654e+02  -0.143    0.887    \nRA9.WAR     -7.473e+02  1.480e+03  -0.505    0.614    \nkFIP        -1.347e+03  2.371e+03  -0.568    0.571    \nWAR          8.560e+03  1.783e+03   4.802 4.17e-06 ***\n연봉.2017.   8.757e-01  4.426e-02  19.787  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9164 on 133 degrees of freedom\nMultiple R-squared:  0.9227,    Adjusted R-squared:  0.9123 \nF-statistic: 88.25 on 18 and 133 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model3)\n\n승7.68840921316788패5.19140274673014세3.03263975401923홀드3.63578951116975블론2.75775305712261경기14.0426530671348선발36.1331990754777이닝59.3709458069269삼진.911.8666574529657볼넷.99.0682604275144홈런.921.5595297493918BABIP3.09205217740503LOB.4.03056643053091ERA9.9781711774582RA9.WAR13.3837520395074kFIP39.6977412189025WAR9.99134587181084연봉.2017.2.20945320867407\n\n\n\nVIF계수가 가장 높았떤 FIP를 제거하니 전체적으로 VIF값들이 많이 감소했다. 볼넷의 경우 50에서 9로 감소함\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47170  -2539    292   2603  47529 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.425e+04  1.656e+04   0.860   0.3912    \n승           1.053e+03  5.292e+02   1.989   0.0487 *  \n패          -1.258e+02  5.365e+02  -0.234   0.8150    \n세          -7.264e+01  2.576e+02  -0.282   0.7784    \n홀드        -7.025e+01  3.031e+02  -0.232   0.8171    \n블론         4.745e+02  7.557e+02   0.628   0.5312    \n경기        -1.021e+02  8.877e+01  -1.150   0.2523    \n선발        -4.306e+02  2.595e+02  -1.659   0.0994 .  \n삼진.9      -7.892e+02  9.060e+02  -0.871   0.3853    \n볼넷.9       8.829e+02  9.479e+02   0.931   0.3533    \n홈런.9       2.956e+03  3.396e+03   0.871   0.3855    \nBABIP       -1.004e+04  1.470e+04  -0.683   0.4957    \nLOB.        -4.506e+01  1.289e+02  -0.350   0.7272    \nERA         -6.838e+01  5.637e+02  -0.121   0.9036    \nRA9.WAR     -4.551e+02  1.402e+03  -0.325   0.7460    \nkFIP        -1.349e+03  2.366e+03  -0.570   0.5696    \nWAR          8.733e+03  1.758e+03   4.968 2.03e-06 ***\n연봉.2017.   8.784e-01  4.395e-02  19.984  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9143 on 134 degrees of freedom\nMultiple R-squared:  0.9225,    Adjusted R-squared:  0.9127 \nF-statistic: 93.84 on 17 and 134 DF,  p-value: &lt; 2.2e-16\n\n\n승7.54995904402493패5.09588128549648세2.77188079102657홀드3.38288542173628블론2.75379743632109경기5.34096225137479선발11.5652288817222삼진.911.8553164987874볼넷.99.06115155090058홈런.921.5464901061541BABIP3.0899457520436LOB.4.03046844648891ERA9.96618149145026RA9.WAR12.0759322801392kFIP39.6977185280435WAR9.75717291579296연봉.2017.2.18906326524158\n\n\n\n그 다음 vif계수값이 높은 ’이닝’을 제거했다.\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47261  -2379    309   2742  47813 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.978e+03  1.054e+04   0.662   0.5090    \n승           1.055e+03  5.278e+02   1.999   0.0476 *  \n패          -1.135e+02  5.347e+02  -0.212   0.8323    \n세          -7.382e+01  2.569e+02  -0.287   0.7743    \n홀드        -7.661e+01  3.021e+02  -0.254   0.8002    \n블론         5.038e+02  7.521e+02   0.670   0.5040    \n경기        -9.923e+01  8.841e+01  -1.122   0.2637    \n선발        -4.402e+02  2.583e+02  -1.704   0.0906 .  \n삼진.9      -3.109e+02  3.413e+02  -0.911   0.3639    \n볼넷.9       4.082e+02  4.514e+02   0.904   0.3675    \n홈런.9       1.129e+03  1.118e+03   1.010   0.3143    \nBABIP       -9.576e+03  1.464e+04  -0.654   0.5141    \nLOB.        -3.779e+01  1.279e+02  -0.295   0.7681    \nERA         -8.963e+01  5.611e+02  -0.160   0.8733    \nRA9.WAR     -4.669e+02  1.399e+03  -0.334   0.7390    \nWAR          8.800e+03  1.749e+03   5.030 1.53e-06 ***\n연봉.2017.   8.779e-01  4.384e-02  20.027  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9120 on 135 degrees of freedom\nMultiple R-squared:  0.9223,    Adjusted R-squared:  0.9131 \nF-statistic: 100.2 on 16 and 135 DF,  p-value: &lt; 2.2e-16\n\n\n승7.54945354936935패5.08758327243023세2.77170233465349홀드3.37829494203411블론2.74099514800148경기5.32417495720081선발11.5165551404735삼진.91.691074402967볼넷.92.06526070907551홈런.92.34713984614222BABIP3.08046557773165LOB.3.99101474806593ERA9.92261602645989RA9.WAR12.0732896169765WAR9.71340804685137연봉.2017.2.18834997395971\n\n\n\nKFIP제거\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP-RA9.WAR, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP - RA9.WAR, \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47256  -2340    228   2820  48394 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.002e+03  1.005e+04   0.796   0.4273    \n승           1.005e+03  5.044e+02   1.993   0.0483 *  \n패          -6.188e+01  5.102e+02  -0.121   0.9036    \n세          -1.005e+02  2.434e+02  -0.413   0.6805    \n홀드        -8.969e+01  2.986e+02  -0.300   0.7643    \n블론         5.293e+02  7.457e+02   0.710   0.4790    \n경기        -1.027e+02  8.749e+01  -1.174   0.2424    \n선발        -4.671e+02  2.447e+02  -1.909   0.0584 .  \n삼진.9      -3.052e+02  3.398e+02  -0.898   0.3707    \n볼넷.9       4.218e+02  4.481e+02   0.941   0.3482    \n홈런.9       1.154e+03  1.112e+03   1.037   0.3013    \nBABIP       -9.059e+03  1.451e+04  -0.624   0.5334    \nLOB.        -5.310e+01  1.190e+02  -0.446   0.6562    \nERA         -1.249e+02  5.493e+02  -0.227   0.8205    \nWAR          8.406e+03  1.289e+03   6.523 1.25e-09 ***\n연봉.2017.   8.790e-01  4.358e-02  20.168  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9090 on 136 degrees of freedom\nMultiple R-squared:  0.9223,    Adjusted R-squared:  0.9137 \nF-statistic: 107.6 on 15 and 136 DF,  p-value: &lt; 2.2e-16\n\n\n승6.94042688100102패4.66294914095306세2.50425253772256홀드3.32146243586634블론2.71278267026247경기5.24904766467952선발10.3991198289636삼진.91.68672132907842볼넷.92.04837389574494홈런.92.33709850623971BABIP3.046073880605LOB.3.47761526760098ERA9.57142193166993WAR5.30623103354661연봉.2017.2.17720905024618\n\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP-RA9.WAR-선발, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP - RA9.WAR - \n    선발, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46776  -2395    374   2597  50018 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7221.2085 10138.7800   0.712   0.4775    \n승            558.2422   451.0986   1.238   0.2180    \n패           -758.5773   359.9930  -2.107   0.0369 *  \n세            -12.1963   241.3026  -0.051   0.9598    \n홀드          106.5326   283.0186   0.376   0.7072    \n블론          843.4665   734.3349   1.149   0.2527    \n경기          -69.6278    86.5803  -0.804   0.4227    \n삼진.9       -270.1732   342.5480  -0.789   0.4316    \n볼넷.9        431.0859   452.3679   0.953   0.3423    \n홈런.9        983.4449  1119.0004   0.879   0.3810    \nBABIP       -8863.3423 14648.1787  -0.605   0.5461    \nLOB.          -57.5239   120.1320  -0.479   0.6328    \nERA           -88.2397   554.2171  -0.159   0.8737    \nWAR          7825.6419  1264.4051   6.189 6.57e-09 ***\n연봉.2017.      0.8792     0.0440  19.981  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9178 on 137 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.912 \nF-statistic: 112.8 on 14 and 137 DF,  p-value: &lt; 2.2e-16\n\n\n승5.44563629770577패2.27744620091112세2.41391273581397홀드2.9278580003899블론2.58069224802437경기5.04287223750904삼진.91.68181308100372볼넷.92.04813245831198홈런.92.32207970272157BABIP3.04592151858475LOB.3.47629848428834ERA9.55974332977644WAR5.01053061622992연봉.2017.2.17719632600132\n\n\n\n유의미한 변수는 ’WAR’과 ’연봉(2017)’이다."
  },
  {
    "objectID": "posts/AS4_5.html#정규화",
    "href": "posts/AS4_5.html#정규화",
    "title": "AS HW4_5",
    "section": "정규화",
    "text": "정규화\n\nnormalize\n\nnormalize &lt;- function(x) {\n  return((x - mean(x)) / sd(x))\n}\n\n\ndf_normalized &lt;- as.data.frame(lapply(dt, normalize))\n\n\nhead(df_normalized)\n\n\nA data.frame: 6 × 20\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2018.\n연봉.2017.\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3.313623\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n0.05943348\n2.452068\n2.645175\n0.6720988\n-0.8689998\n-0.44238194\n0.01678276\n0.4466146\n-0.5870557\n3.174630\n-0.9710297\n-1.0581252\n4.503142\n3.912893\n2.7347053\n\n\n2\n2.019505\n2.5047212\n-0.0985024\n-0.5857052\n-0.5435919\n0.05943348\n2.349505\n2.547755\n0.1345315\n-0.9875023\n-0.66852133\n-0.24168646\n-0.1227637\n-0.5198553\n3.114968\n-1.0618879\n-1.0732645\n4.094734\n3.266495\n1.3373033\n\n\n3\n4.348918\n0.9077513\n-0.3064519\n-0.5857052\n-0.5435919\n0.11105570\n2.554632\n2.706808\n0.1097751\n-0.8859287\n-0.41288550\n-0.09559517\n0.3085835\n-0.6254559\n2.973948\n-0.8374147\n-0.8663606\n3.761956\n6.821679\n5.3298806\n\n\n4\n1.760682\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n-0.04381097\n2.246942\n2.350927\n0.3502657\n-0.9451800\n-0.18674611\n-0.47768010\n0.5587649\n-0.6278559\n2.740722\n-0.6984550\n-0.7603854\n2.998081\n2.620098\n3.3335919\n\n\n5\n2.537153\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n0.05943348\n2.452068\n2.587518\n0.1557512\n-0.8774643\n-0.29489973\n-0.19673529\n0.4811224\n-0.5390554\n2.751570\n-0.6129414\n-0.6190851\n2.809003\n2.975617\n2.7347053\n\n\n6\n1.243035\n2.1853272\n-0.3064519\n-0.5857052\n-0.5435919\n-0.14705541\n2.041816\n2.048726\n0.1309948\n-1.0340569\n-0.08842464\n-0.57882022\n0.6536613\n-0.7214564\n2.963100\n-0.5808738\n-0.6140386\n2.476226\n2.135301\n0.7384167\n\n\n\n\n\n\nmodel4 &lt;- lm(연봉.2018. ~ ., df_normalized)\nsummary(model4)\n\n\nCall:\nlm(formula = 연봉.2018. ~ ., data = df_normalized)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.50382 -0.07816  0.01372  0.08561  1.54402 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.061e-16  2.411e-02   0.000   1.0000    \n승           1.254e-01  6.712e-02   1.869   0.0639 .  \n패          -1.858e-02  5.569e-02  -0.334   0.7392    \n세          -3.282e-03  4.216e-02  -0.078   0.9381    \n홀드        -2.652e-03  4.613e-02  -0.057   0.9542    \n블론         2.395e-02  4.019e-02   0.596   0.5522    \n경기        -1.102e-01  9.116e-02  -1.209   0.2289    \n선발        -2.117e-01  1.455e-01  -1.456   0.1479    \n이닝         1.207e-01  1.879e-01   0.642   0.5217    \n삼진.9      -4.207e-02  2.146e-01  -0.196   0.8449    \n볼넷.9       9.115e-02  1.723e-01   0.529   0.5976    \n홈런.9       1.602e-01  4.643e-01   0.345   0.7306    \nBABIP       -2.875e-02  4.273e-02  -0.673   0.5022    \nLOB.        -1.630e-02  4.866e-02  -0.335   0.7382    \nERA         -9.982e-03  7.667e-02  -0.130   0.8966    \nRA9.WAR     -4.519e-02  8.862e-02  -0.510   0.6109    \nFIP         -3.892e-01  2.707e+00  -0.144   0.8859    \nkFIP         2.437e-01  2.301e+00   0.106   0.9158    \nWAR          3.657e-01  7.647e-02   4.783 4.55e-06 ***\n연봉.2017.   7.087e-01  3.598e-02  19.698  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.2973 on 132 degrees of freedom\nMultiple R-squared:  0.9228,    Adjusted R-squared:  0.9116 \nF-statistic: 82.99 on 19 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n\nthreshold &lt;- 10\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model4, threshold)\nprint(high_vif_vars)\n\n [1] \"경기\"    \"선발\"    \"이닝\"    \"삼진.9\"  \"볼넷.9\"  \"홈런.9\"  \"ERA\"    \n [8] \"RA9.WAR\" \"FIP\"     \"kFIP\"   \n\n\n\nvif(model4)\n\n승7.6993466963604패5.30032799820294세3.03718565967898홀드3.63605161357941블론2.75956034802382경기14.2011271199764선발36.160187873294이닝60.3244538179135삼진.978.7161704890434볼넷.950.7257985014939홈런.9368.399308167005BABIP3.11936893312485LOB.4.04602533224442ERA10.044173823258RA9.WAR13.4198973514472FIP12525.2424058262kFIP9046.04880481494WAR9.99177856815799연봉.2017.2.2118694256429\n\n\n\n###"
  },
  {
    "objectID": "posts/AS4_5.html#변수선택",
    "href": "posts/AS4_5.html#변수선택",
    "title": "AS HW4_5",
    "section": "변수선택",
    "text": "변수선택\n\nmodel3(AIC)\n- AIC(Step)\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n\n\nmodel3 = step(\n m0,\n scope = 연봉.2018. ~연봉.2017.+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"both\")\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n             Df  Sum of Sq        RSS    AIC\n+ 연봉.2017.  1 1.2511e+11 1.9445e+10 2841.4\n+ WAR         1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR     1 7.9230e+10 6.5326e+10 3025.6\n+ 승          1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝        1 6.2759e+10 8.1797e+10 3059.8\n+ 선발        1 4.5409e+10 9.9147e+10 3089.0\n+ 패          1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9      1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP        1 1.2591e+10 1.3197e+11 3132.4\n+ FIP         1 1.1403e+10 1.3315e+11 3133.8\n+ ERA         1 6.7332e+09 1.3782e+11 3139.1\n+ 세          1 6.4461e+09 1.3811e+11 3139.4\n+ 경기        1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.        1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9      1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                     1.4456e+11 3144.3\n+ 삼진.9      1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP       1 1.5139e+09 1.4304e+11 3144.7\n+ 블론        1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드        1 4.3499e+07 1.4451e+11 3146.3\n\nStep:  AIC=2841.38\n연봉.2018. ~ 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n+ WAR         1 7.0421e+09 1.2403e+10 2775.0\n+ RA9.WAR     1 4.9589e+09 1.4486e+10 2798.6\n+ 승          1 3.8414e+09 1.5604e+10 2809.9\n+ 이닝        1 2.8118e+09 1.6633e+10 2819.6\n+ 선발        1 2.1318e+09 1.7313e+10 2825.7\n+ 패          1 8.8114e+08 1.8564e+10 2836.3\n&lt;none&gt;                     1.9445e+10 2841.4\n+ 블론        1 2.2022e+08 1.9225e+10 2841.7\n+ 세          1 1.7105e+08 1.9274e+10 2842.0\n+ kFIP        1 1.6254e+08 1.9283e+10 2842.1\n+ FIP         1 1.5483e+08 1.9290e+10 2842.2\n+ ERA         1 1.0735e+08 1.9338e+10 2842.5\n+ LOB.        1 7.7049e+07 1.9368e+10 2842.8\n+ 홈런.9      1 7.3957e+07 1.9371e+10 2842.8\n+ 볼넷.9      1 6.4565e+07 1.9381e+10 2842.9\n+ BABIP       1 5.6938e+07 1.9388e+10 2842.9\n+ 홀드        1 3.8024e+07 1.9407e+10 2843.1\n+ 삼진.9      1 5.5081e+06 1.9440e+10 2843.3\n+ 경기        1 1.2651e+04 1.9445e+10 2843.4\n- 연봉.2017.  1 1.2511e+11 1.4456e+11 3144.3\n\nStep:  AIC=2775.03\n연봉.2018. ~ 연봉.2017. + WAR\n\n             Df  Sum of Sq        RSS    AIC\n+ 패          1 2.1336e+08 1.2190e+10 2774.4\n+ kFIP        1 1.8769e+08 1.2215e+10 2774.7\n+ 선발        1 1.7153e+08 1.2232e+10 2774.9\n+ FIP         1 1.6877e+08 1.2234e+10 2774.9\n+ 볼넷.9      1 1.6419e+08 1.2239e+10 2775.0\n&lt;none&gt;                     1.2403e+10 2775.0\n+ 이닝        1 1.4704e+08 1.2256e+10 2775.2\n+ 홈런.9      1 5.1612e+07 1.2351e+10 2776.4\n+ 삼진.9      1 4.8349e+07 1.2355e+10 2776.4\n+ 승          1 3.0076e+07 1.2373e+10 2776.7\n+ 경기        1 2.7246e+07 1.2376e+10 2776.7\n+ BABIP       1 2.4182e+07 1.2379e+10 2776.7\n+ ERA         1 1.7077e+07 1.2386e+10 2776.8\n+ 블론        1 1.1153e+07 1.2392e+10 2776.9\n+ RA9.WAR     1 6.6509e+06 1.2396e+10 2776.9\n+ 세          1 4.3325e+06 1.2399e+10 2777.0\n+ 홀드        1 3.4824e+06 1.2400e+10 2777.0\n+ LOB.        1 6.6018e+05 1.2402e+10 2777.0\n- WAR         1 7.0421e+09 1.9445e+10 2841.4\n- 연봉.2017.  1 4.1619e+10 5.4022e+10 2996.7\n\nStep:  AIC=2774.4\n연봉.2018. ~ 연봉.2017. + WAR + 패\n\n             Df  Sum of Sq        RSS    AIC\n+ kFIP        1 1.9738e+08 1.1992e+10 2773.9\n+ 승          1 1.8072e+08 1.2009e+10 2774.1\n+ FIP         1 1.7496e+08 1.2015e+10 2774.2\n&lt;none&gt;                     1.2190e+10 2774.4\n- 패          1 2.1336e+08 1.2403e+10 2775.0\n+ 볼넷.9      1 1.0330e+08 1.2086e+10 2775.1\n+ 홈런.9      1 7.1015e+07 1.2119e+10 2775.5\n+ 삼진.9      1 6.6895e+07 1.2123e+10 2775.6\n+ 블론        1 4.2173e+07 1.2148e+10 2775.9\n+ BABIP       1 4.1954e+07 1.2148e+10 2775.9\n+ 선발        1 3.1474e+07 1.2158e+10 2776.0\n+ ERA         1 1.3441e+07 1.2176e+10 2776.2\n+ 이닝        1 5.8966e+06 1.2184e+10 2776.3\n+ 세          1 3.4705e+06 1.2186e+10 2776.3\n+ RA9.WAR     1 2.4143e+06 1.2187e+10 2776.4\n+ LOB.        1 1.7129e+06 1.2188e+10 2776.4\n+ 경기        1 1.1252e+06 1.2189e+10 2776.4\n+ 홀드        1 1.8992e+05 1.2190e+10 2776.4\n- WAR         1 6.3743e+09 1.8564e+10 2836.3\n- 연봉.2017.  1 4.1680e+10 5.3870e+10 2998.3\n\nStep:  AIC=2773.92\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP\n\n             Df  Sum of Sq        RSS    AIC\n+ 승          1 1.6741e+08 1.1825e+10 2773.8\n&lt;none&gt;                     1.1992e+10 2773.9\n+ 블론        1 1.2836e+08 1.1864e+10 2774.3\n- kFIP        1 1.9738e+08 1.2190e+10 2774.4\n+ 선발        1 1.1764e+08 1.1875e+10 2774.4\n- 패          1 2.2305e+08 1.2215e+10 2774.7\n+ BABIP       1 7.5190e+07 1.1917e+10 2775.0\n+ ERA         1 2.1818e+07 1.1971e+10 2775.6\n+ 홀드        1 2.1404e+07 1.1971e+10 2775.6\n+ 삼진.9      1 1.9275e+07 1.1973e+10 2775.7\n+ 경기        1 1.7028e+07 1.1975e+10 2775.7\n+ 이닝        1 1.3041e+07 1.1979e+10 2775.8\n+ FIP         1 9.3610e+06 1.1983e+10 2775.8\n+ 볼넷.9      1 8.8432e+06 1.1983e+10 2775.8\n+ 홈런.9      1 8.7223e+06 1.1984e+10 2775.8\n+ LOB.        1 4.0316e+06 1.1988e+10 2775.9\n+ RA9.WAR     1 2.0131e+06 1.1990e+10 2775.9\n+ 세          1 1.4454e+06 1.1991e+10 2775.9\n- WAR         1 6.4941e+09 1.8486e+10 2837.7\n- 연봉.2017.  1 4.1735e+10 5.3727e+10 2999.9\n\nStep:  AIC=2773.78\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승\n\n             Df  Sum of Sq        RSS    AIC\n+ 이닝        1 2.1565e+08 1.1609e+10 2773.0\n+ 선발        1 1.9668e+08 1.1628e+10 2773.2\n&lt;none&gt;                     1.1825e+10 2773.8\n- 승          1 1.6741e+08 1.1992e+10 2773.9\n- kFIP        1 1.8408e+08 1.2009e+10 2774.1\n+ 블론        1 8.3012e+07 1.1742e+10 2774.7\n+ RA9.WAR     1 6.3182e+07 1.1762e+10 2775.0\n+ BABIP       1 4.5875e+07 1.1779e+10 2775.2\n+ 볼넷.9      1 1.7921e+07 1.1807e+10 2775.6\n+ 삼진.9      1 1.4564e+07 1.1810e+10 2775.6\n+ 홈런.9      1 1.2160e+07 1.1813e+10 2775.6\n+ ERA         1 8.8026e+06 1.1816e+10 2775.7\n+ FIP         1 8.1221e+06 1.1817e+10 2775.7\n+ 세          1 5.8214e+06 1.1819e+10 2775.7\n+ 홀드        1 5.2671e+06 1.1820e+10 2775.7\n+ LOB.        1 3.9758e+05 1.1825e+10 2775.8\n+ 경기        1 3.3176e+05 1.1825e+10 2775.8\n- 패          1 3.6648e+08 1.2191e+10 2776.4\n- WAR         1 3.6353e+09 1.5460e+10 2812.5\n- 연봉.2017.  1 3.9188e+10 5.1013e+10 2994.0\n\nStep:  AIC=2772.98\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n- 패          1 3.1923e+07 1.1641e+10 2771.4\n&lt;none&gt;                     1.1609e+10 2773.0\n- kFIP        1 2.1496e+08 1.1824e+10 2773.8\n- 이닝        1 2.1565e+08 1.1825e+10 2773.8\n+ BABIP       1 8.7592e+07 1.1522e+10 2773.8\n+ 선발        1 5.0414e+07 1.1559e+10 2774.3\n+ 블론        1 3.9472e+07 1.1570e+10 2774.5\n+ 삼진.9      1 3.3863e+07 1.1575e+10 2774.5\n+ ERA         1 3.3525e+07 1.1576e+10 2774.5\n+ FIP         1 1.8310e+07 1.1591e+10 2774.7\n+ 홈런.9      1 1.2031e+07 1.1597e+10 2774.8\n+ RA9.WAR     1 1.0398e+07 1.1599e+10 2774.8\n+ LOB.        1 3.1362e+06 1.1606e+10 2774.9\n+ 경기        1 1.9500e+06 1.1607e+10 2775.0\n+ 볼넷.9      1 1.2880e+06 1.1608e+10 2775.0\n+ 세          1 2.2726e+05 1.1609e+10 2775.0\n+ 홀드        1 9.3003e+04 1.1609e+10 2775.0\n- 승          1 3.7002e+08 1.1979e+10 2775.8\n- WAR         1 3.7546e+09 1.5364e+10 2813.6\n- 연봉.2017.  1 3.8723e+10 5.0333e+10 2993.9\n\nStep:  AIC=2771.4\n연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                     1.1641e+10 2771.4\n+ BABIP       1 9.5915e+07 1.1545e+10 2772.1\n- kFIP        1 2.2291e+08 1.1864e+10 2772.3\n+ 선발        1 6.0820e+07 1.1580e+10 2772.6\n+ ERA         1 4.1760e+07 1.1599e+10 2772.8\n+ 삼진.9      1 3.6788e+07 1.1604e+10 2772.9\n+ 패          1 3.1923e+07 1.1609e+10 2773.0\n+ 블론        1 2.3680e+07 1.1618e+10 2773.1\n+ FIP         1 2.0239e+07 1.1621e+10 2773.1\n+ 홈런.9      1 1.4166e+07 1.1627e+10 2773.2\n+ LOB.        1 7.7349e+06 1.1633e+10 2773.3\n+ 경기        1 1.5351e+06 1.1640e+10 2773.4\n+ RA9.WAR     1 1.4350e+06 1.1640e+10 2773.4\n+ 볼넷.9      1 1.4095e+06 1.1640e+10 2773.4\n+ 홀드        1 2.5525e+05 1.1641e+10 2773.4\n+ 세          1 6.8181e+04 1.1641e+10 2773.4\n- 승          1 4.0011e+08 1.2041e+10 2774.5\n- 이닝        1 5.5021e+08 1.2191e+10 2776.4\n- WAR         1 3.9604e+09 1.5602e+10 2813.9\n- 연봉.2017.  1 3.8795e+10 5.0436e+10 2992.2\n\n\n\nsummary\n\nsummary(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + \n    이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48717  -2879    204   3083  48961 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.691e+03  2.658e+03  -1.012  0.31310    \n연봉.2017.   8.862e-01  4.018e-02  22.058  &lt; 2e-16 ***\nWAR          8.118e+03  1.152e+03   7.048 6.68e-11 ***\nkFIP         6.737e+02  4.029e+02   1.672  0.09666 .  \n승           1.059e+03  4.727e+02   2.240  0.02659 *  \n이닝        -9.701e+01  3.693e+01  -2.627  0.00954 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8929 on 146 degrees of freedom\nMultiple R-squared:  0.9195,    Adjusted R-squared:  0.9167 \nF-statistic: 333.4 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\nAIC를 이용하면 최종 모형은 “연봉.2018. ~ 연봉.2017. + WAR + kFIP+승+이닝” 이다.\n\n\nvif(model3)\n\n연봉.2017.1.91752518192932WAR4.3927072113068kFIP1.20722030237251승6.3165168650018이닝6.53436057823665\n\n\n연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + 이닝\n\n\n\n후진\n\nmodel_back = step(model1, direction = \"backward\")\nsummary(model_back)\n\nStart:  AIC=2793.07\n연봉.2018. ~ 승 + 패 + 세 + 홀드 + 블론 + 경기 + 선발 + \n    이닝 + 삼진.9 + 볼넷.9 + 홈런.9 + BABIP + LOB. + \n    ERA + RA9.WAR + FIP + kFIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 홀드        1 2.7964e+05 1.1167e+10 2791.1\n- 세          1 5.1274e+05 1.1167e+10 2791.1\n- kFIP        1 9.4914e+05 1.1168e+10 2791.1\n- ERA         1 1.4342e+06 1.1168e+10 2791.1\n- FIP         1 1.7480e+06 1.1168e+10 2791.1\n- 삼진.9      1 3.2496e+06 1.1170e+10 2791.1\n- 패          1 9.4169e+06 1.1176e+10 2791.2\n- LOB.        1 9.4883e+06 1.1176e+10 2791.2\n- 홈런.9      1 1.0071e+07 1.1177e+10 2791.2\n- RA9.WAR     1 2.1998e+07 1.1189e+10 2791.4\n- 볼넷.9      1 2.3679e+07 1.1190e+10 2791.4\n- 블론        1 3.0047e+07 1.1197e+10 2791.5\n- 이닝        1 3.4909e+07 1.1201e+10 2791.6\n- BABIP       1 3.8305e+07 1.1205e+10 2791.6\n- 경기        1 1.2360e+08 1.1290e+10 2792.7\n&lt;none&gt;                     1.1167e+10 2793.1\n- 선발        1 1.7922e+08 1.1346e+10 2793.5\n- 승          1 2.9538e+08 1.1462e+10 2795.0\n- WAR         1 1.9353e+09 1.3102e+10 2815.4\n- 연봉.2017.  1 3.2824e+10 4.3991e+10 2999.5\n\nStep:  AIC=2791.07\n연봉.2018. ~ 승 + 패 + 세 + 블론 + 경기 + 선발 + 이닝 + \n    삼진.9 + 볼넷.9 + 홈런.9 + BABIP + LOB. + ERA + RA9.WAR + \n    FIP + kFIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 세          1 2.6213e+05 1.1167e+10 2789.1\n- kFIP        1 9.3814e+05 1.1168e+10 2789.1\n- ERA         1 1.5378e+06 1.1168e+10 2789.1\n- FIP         1 1.7362e+06 1.1169e+10 2789.1\n- 삼진.9      1 3.2908e+06 1.1170e+10 2789.1\n- LOB.        1 9.8104e+06 1.1177e+10 2789.2\n- 홈런.9      1 1.0084e+07 1.1177e+10 2789.2\n- 패          1 1.1289e+07 1.1178e+10 2789.2\n- 볼넷.9      1 2.3701e+07 1.1191e+10 2789.4\n- RA9.WAR     1 2.3959e+07 1.1191e+10 2789.4\n- 블론        1 2.9856e+07 1.1197e+10 2789.5\n- BABIP       1 3.8334e+07 1.1205e+10 2789.6\n- 이닝        1 3.9219e+07 1.1206e+10 2789.6\n&lt;none&gt;                     1.1167e+10 2791.1\n- 선발        1 1.7952e+08 1.1346e+10 2791.5\n- 경기        1 1.8031e+08 1.1347e+10 2791.5\n- 승          1 2.9677e+08 1.1464e+10 2793.1\n- WAR         1 1.9405e+09 1.3107e+10 2813.4\n- 연봉.2017.  1 3.2987e+10 4.4154e+10 2998.0\n\nStep:  AIC=2789.08\n연봉.2018. ~ 승 + 패 + 블론 + 경기 + 선발 + 이닝 + \n    삼진.9 + 볼넷.9 + 홈런.9 + BABIP + LOB. + ERA + RA9.WAR + \n    FIP + kFIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- kFIP        1 9.9324e+05 1.1168e+10 2787.1\n- ERA         1 1.6052e+06 1.1169e+10 2787.1\n- FIP         1 1.8109e+06 1.1169e+10 2787.1\n- 삼진.9      1 3.2005e+06 1.1170e+10 2787.1\n- LOB.        1 9.9171e+06 1.1177e+10 2787.2\n- 홈런.9      1 1.0260e+07 1.1177e+10 2787.2\n- 패          1 1.1827e+07 1.1179e+10 2787.2\n- 볼넷.9      1 2.3993e+07 1.1191e+10 2787.4\n- RA9.WAR     1 2.8604e+07 1.1196e+10 2787.5\n- 블론        1 3.3423e+07 1.1201e+10 2787.5\n- BABIP       1 3.8245e+07 1.1205e+10 2787.6\n- 이닝        1 4.1983e+07 1.1209e+10 2787.7\n&lt;none&gt;                     1.1167e+10 2789.1\n- 선발        1 1.8297e+08 1.1350e+10 2789.6\n- 경기        1 1.9004e+08 1.1357e+10 2789.6\n- 승          1 3.2363e+08 1.1491e+10 2791.4\n- WAR         1 1.9440e+09 1.3111e+10 2811.5\n- 연봉.2017.  1 3.4986e+10 4.6153e+10 3002.8\n\nStep:  AIC=2787.09\n연봉.2018. ~ 승 + 패 + 블론 + 경기 + 선발 + 이닝 + \n    삼진.9 + 볼넷.9 + 홈런.9 + BABIP + LOB. + ERA + RA9.WAR + \n    FIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- ERA         1 1.8214e+06 1.1170e+10 2785.1\n- LOB.        1 1.0407e+07 1.1179e+10 2785.2\n- 패          1 1.1087e+07 1.1179e+10 2785.2\n- FIP         1 2.8081e+07 1.1196e+10 2785.5\n- RA9.WAR     1 2.8212e+07 1.1196e+10 2785.5\n- 블론        1 3.3436e+07 1.1202e+10 2785.6\n- BABIP       1 3.7443e+07 1.1206e+10 2785.6\n- 이닝        1 4.1025e+07 1.1209e+10 2785.7\n- 홈런.9      1 5.5367e+07 1.1223e+10 2785.8\n- 삼진.9      1 6.7478e+07 1.1236e+10 2786.0\n- 볼넷.9      1 7.1385e+07 1.1239e+10 2786.1\n&lt;none&gt;                     1.1168e+10 2787.1\n- 선발        1 1.8242e+08 1.1351e+10 2787.6\n- 경기        1 1.8950e+08 1.1358e+10 2787.7\n- 승          1 3.2639e+08 1.1494e+10 2789.5\n- WAR         1 1.9444e+09 1.3112e+10 2809.5\n- 연봉.2017.  1 3.5014e+10 4.6182e+10 3000.9\n\nStep:  AIC=2785.12\n연봉.2018. ~ 승 + 패 + 블론 + 경기 + 선발 + 이닝 + \n    삼진.9 + 볼넷.9 + 홈런.9 + BABIP + LOB. + RA9.WAR + \n    FIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- LOB.        1 1.0813e+07 1.1181e+10 2783.3\n- 패          1 1.1584e+07 1.1181e+10 2783.3\n- FIP         1 2.9304e+07 1.1199e+10 2783.5\n- 블론        1 3.2749e+07 1.1203e+10 2783.6\n- RA9.WAR     1 3.2883e+07 1.1203e+10 2783.6\n- 이닝        1 4.0932e+07 1.1211e+10 2783.7\n- 홈런.9      1 5.3929e+07 1.1224e+10 2783.8\n- BABIP       1 6.8000e+07 1.1238e+10 2784.0\n- 삼진.9      1 6.9211e+07 1.1239e+10 2784.1\n- 볼넷.9      1 6.9566e+07 1.1239e+10 2784.1\n&lt;none&gt;                     1.1170e+10 2785.1\n- 선발        1 1.8070e+08 1.1351e+10 2785.6\n- 경기        1 1.8826e+08 1.1358e+10 2785.7\n- 승          1 3.2533e+08 1.1495e+10 2787.5\n- WAR         1 1.9871e+09 1.3157e+10 2808.0\n- 연봉.2017.  1 3.5061e+10 4.6231e+10 2999.0\n\nStep:  AIC=2783.26\n연봉.2018. ~ 승 + 패 + 블론 + 경기 + 선발 + 이닝 + \n    삼진.9 + 볼넷.9 + 홈런.9 + BABIP + RA9.WAR + FIP + \n    WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 패          1 1.0204e+07 1.1191e+10 2781.4\n- FIP         1 2.3196e+07 1.1204e+10 2781.6\n- 블론        1 3.0781e+07 1.1212e+10 2781.7\n- 이닝        1 4.3368e+07 1.1224e+10 2781.8\n- 홈런.9      1 4.6793e+07 1.1228e+10 2781.9\n- RA9.WAR     1 5.5284e+07 1.1236e+10 2782.0\n- BABIP       1 5.7524e+07 1.1238e+10 2782.0\n- 볼넷.9      1 6.1767e+07 1.1242e+10 2782.1\n- 삼진.9      1 6.4118e+07 1.1245e+10 2782.1\n&lt;none&gt;                     1.1181e+10 2783.3\n- 선발        1 1.8271e+08 1.1363e+10 2783.7\n- 경기        1 1.8379e+08 1.1365e+10 2783.7\n- 승          1 3.3073e+08 1.1511e+10 2785.7\n- WAR         1 2.1202e+09 1.3301e+10 2807.7\n- 연봉.2017.  1 3.5097e+10 4.6278e+10 2997.2\n\nStep:  AIC=2781.4\n연봉.2018. ~ 승 + 블론 + 경기 + 선발 + 이닝 + 삼진.9 + \n    볼넷.9 + 홈런.9 + BABIP + RA9.WAR + FIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- FIP         1 2.2440e+07 1.1213e+10 2779.7\n- 블론        1 2.3243e+07 1.1214e+10 2779.7\n- 이닝        1 4.1401e+07 1.1232e+10 2780.0\n- 홈런.9      1 4.5529e+07 1.1236e+10 2780.0\n- RA9.WAR     1 4.6219e+07 1.1237e+10 2780.0\n- BABIP       1 5.8410e+07 1.1249e+10 2780.2\n- 볼넷.9      1 6.1274e+07 1.1252e+10 2780.2\n- 삼진.9      1 6.2728e+07 1.1254e+10 2780.2\n&lt;none&gt;                     1.1191e+10 2781.4\n- 경기        1 2.1328e+08 1.1404e+10 2782.3\n- 선발        1 2.3948e+08 1.1430e+10 2782.6\n- 승          1 3.4606e+08 1.1537e+10 2784.0\n- WAR         1 2.1342e+09 1.3325e+10 2805.9\n- 연봉.2017.  1 3.5334e+10 4.6525e+10 2996.0\n\nStep:  AIC=2779.71\n연봉.2018. ~ 승 + 블론 + 경기 + 선발 + 이닝 + 삼진.9 + \n    볼넷.9 + 홈런.9 + BABIP + RA9.WAR + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 블론        1 2.8280e+07 1.1242e+10 2778.1\n- 이닝        1 4.0706e+07 1.1254e+10 2778.3\n- RA9.WAR     1 4.3431e+07 1.1257e+10 2778.3\n- BABIP       1 6.5327e+07 1.1279e+10 2778.6\n- 삼진.9      1 6.9757e+07 1.1283e+10 2778.7\n- 볼넷.9      1 8.4569e+07 1.1298e+10 2778.8\n- 홈런.9      1 1.0518e+08 1.1319e+10 2779.1\n&lt;none&gt;                     1.1213e+10 2779.7\n- 경기        1 2.0863e+08 1.1422e+10 2780.5\n- 선발        1 2.4225e+08 1.1456e+10 2781.0\n- 승          1 3.4542e+08 1.1559e+10 2782.3\n- WAR         1 2.1544e+09 1.3368e+10 2804.4\n- 연봉.2017.  1 3.5313e+10 4.6526e+10 2994.0\n\nStep:  AIC=2778.09\n연봉.2018. ~ 승 + 경기 + 선발 + 이닝 + 삼진.9 + 볼넷.9 + \n    홈런.9 + BABIP + RA9.WAR + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 이닝        1 3.5553e+07 1.1277e+10 2776.6\n- RA9.WAR     1 4.1773e+07 1.1283e+10 2776.7\n- 삼진.9      1 6.2788e+07 1.1304e+10 2776.9\n- BABIP       1 6.6047e+07 1.1308e+10 2777.0\n- 볼넷.9      1 8.6190e+07 1.1328e+10 2777.2\n- 홈런.9      1 9.9781e+07 1.1341e+10 2777.4\n&lt;none&gt;                     1.1242e+10 2778.1\n- 경기        1 1.8104e+08 1.1423e+10 2778.5\n- 선발        1 2.4212e+08 1.1484e+10 2779.3\n- 승          1 3.7616e+08 1.1618e+10 2781.1\n- WAR         1 2.1262e+09 1.3368e+10 2802.4\n- 연봉.2017.  1 3.7489e+10 4.8731e+10 2999.0\n\nStep:  AIC=2776.57\n연봉.2018. ~ 승 + 경기 + 선발 + 삼진.9 + 볼넷.9 + \n    홈런.9 + BABIP + RA9.WAR + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- RA9.WAR     1 2.7446e+07 1.1305e+10 2774.9\n- BABIP       1 6.5912e+07 1.1343e+10 2775.5\n- 삼진.9      1 7.3754e+07 1.1351e+10 2775.6\n- 볼넷.9      1 7.9195e+07 1.1356e+10 2775.6\n- 홈런.9      1 1.1893e+08 1.1396e+10 2776.2\n&lt;none&gt;                     1.1277e+10 2776.6\n- 경기        1 2.5407e+08 1.1531e+10 2778.0\n- 승          1 4.4284e+08 1.1720e+10 2780.4\n- 선발        1 7.1295e+08 1.1990e+10 2783.9\n- WAR         1 2.3107e+09 1.3588e+10 2802.9\n- 연봉.2017.  1 3.7540e+10 4.8818e+10 2997.3\n\nStep:  AIC=2774.94\n연봉.2018. ~ 승 + 경기 + 선발 + 삼진.9 + 볼넷.9 + \n    홈런.9 + BABIP + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- BABIP       1 5.1810e+07 1.1356e+10 2773.6\n- 삼진.9      1 7.6555e+07 1.1381e+10 2774.0\n- 볼넷.9      1 7.6805e+07 1.1381e+10 2774.0\n- 홈런.9      1 1.1502e+08 1.1420e+10 2774.5\n&lt;none&gt;                     1.1305e+10 2774.9\n- 경기        1 2.6687e+08 1.1572e+10 2776.5\n- 승          1 4.1542e+08 1.1720e+10 2778.4\n- 선발        1 7.2929e+08 1.2034e+10 2782.4\n- WAR         1 3.6036e+09 1.4908e+10 2815.0\n- 연봉.2017.  1 3.7533e+10 4.8837e+10 2995.4\n\nStep:  AIC=2773.63\n연봉.2018. ~ 승 + 경기 + 선발 + 삼진.9 + 볼넷.9 + \n    홈런.9 + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 볼넷.9      1 7.2317e+07 1.1429e+10 2772.6\n- 홈런.9      1 8.5360e+07 1.1442e+10 2772.8\n&lt;none&gt;                     1.1356e+10 2773.6\n- 삼진.9      1 1.8112e+08 1.1538e+10 2774.0\n- 경기        1 2.2627e+08 1.1583e+10 2774.6\n- 승          1 4.2114e+08 1.1778e+10 2777.2\n- 선발        1 7.1482e+08 1.2071e+10 2780.9\n- WAR         1 3.5773e+09 1.4934e+10 2813.3\n- 연봉.2017.  1 3.7607e+10 4.8964e+10 2993.8\n\nStep:  AIC=2772.6\n연봉.2018. ~ 승 + 경기 + 선발 + 삼진.9 + 홈런.9 + \n    WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 홈런.9      1 1.2421e+08 1.1553e+10 2772.2\n- 삼진.9      1 1.4867e+08 1.1577e+10 2772.6\n&lt;none&gt;                     1.1429e+10 2772.6\n- 경기        1 3.5597e+08 1.1785e+10 2775.3\n- 승          1 4.5207e+08 1.1881e+10 2776.5\n- 선발        1 7.9752e+08 1.2226e+10 2780.8\n- WAR         1 3.5112e+09 1.4940e+10 2811.3\n- 연봉.2017.  1 3.7614e+10 4.9042e+10 2992.0\n\nStep:  AIC=2772.24\n연봉.2018. ~ 승 + 경기 + 선발 + 삼진.9 + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n- 삼진.9      1 8.2804e+07 1.1636e+10 2771.3\n&lt;none&gt;                     1.1553e+10 2772.2\n- 경기        1 4.0633e+08 1.1959e+10 2775.5\n- 승          1 4.9123e+08 1.2044e+10 2776.6\n- 선발        1 7.3028e+08 1.2283e+10 2779.6\n- WAR         1 3.4409e+09 1.4994e+10 2809.9\n- 연봉.2017.  1 3.8020e+10 4.9573e+10 2991.6\n\nStep:  AIC=2771.33\n연봉.2018. ~ 승 + 경기 + 선발 + WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                     1.1636e+10 2771.3\n- 경기        1 4.4893e+08 1.2085e+10 2775.1\n- 승          1 5.0693e+08 1.2143e+10 2775.8\n- 선발        1 6.7070e+08 1.2306e+10 2777.8\n- WAR         1 3.3777e+09 1.5014e+10 2808.1\n- 연봉.2017.  1 3.8082e+10 4.9718e+10 2990.1\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ 승 + 경기 + 선발 + WAR + 연봉.2017., \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48420  -1976    -56   2829  48859 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1969.51173 1500.85489   1.312   0.1915    \n승          1174.23883  465.58916   2.522   0.0127 *  \n경기        -120.60886   50.81729  -2.373   0.0189 *  \n선발        -440.87646  151.97589  -2.901   0.0043 ** \nWAR         7220.36610 1109.09601   6.510 1.13e-09 ***\n연봉.2017.     0.88247    0.04037  21.860  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8927 on 146 degrees of freedom\nMultiple R-squared:  0.9195,    Adjusted R-squared:  0.9168 \nF-statistic: 333.6 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model_back)\n\n승6.13100501411465경기1.83604531782849선발4.16007029774887WAR4.07445827346373연봉.2017.1.93705640010866\n\n\n연봉.2018. ~ 승 + 경기 + 선발 + WAR + 연봉.2017.\n\n\n전진\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n\n\nmodel_forward = step(\n m0,\n scope = 연봉.2018. ~연봉.2017.+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"forward\")\nsummary(model_forward)\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n             Df  Sum of Sq        RSS    AIC\n+ 연봉.2017.  1 1.2511e+11 1.9445e+10 2841.4\n+ WAR         1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR     1 7.9230e+10 6.5326e+10 3025.6\n+ 승          1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝        1 6.2759e+10 8.1797e+10 3059.8\n+ 선발        1 4.5409e+10 9.9147e+10 3089.0\n+ 패          1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9      1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP        1 1.2591e+10 1.3197e+11 3132.4\n+ FIP         1 1.1403e+10 1.3315e+11 3133.8\n+ ERA         1 6.7332e+09 1.3782e+11 3139.1\n+ 세          1 6.4461e+09 1.3811e+11 3139.4\n+ 경기        1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.        1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9      1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                     1.4456e+11 3144.3\n+ 삼진.9      1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP       1 1.5139e+09 1.4304e+11 3144.7\n+ 블론        1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드        1 4.3499e+07 1.4451e+11 3146.3\n\nStep:  AIC=2841.38\n연봉.2018. ~ 연봉.2017.\n\n          Df  Sum of Sq        RSS    AIC\n+ WAR      1 7042094427 1.2403e+10 2775.0\n+ RA9.WAR  1 4958914952 1.4486e+10 2798.6\n+ 승       1 3841387936 1.5604e+10 2809.9\n+ 이닝     1 2811807174 1.6633e+10 2819.6\n+ 선발     1 2131826098 1.7313e+10 2825.7\n+ 패       1  881138122 1.8564e+10 2836.3\n&lt;none&gt;                  1.9445e+10 2841.4\n+ 블론     1  220224080 1.9225e+10 2841.7\n+ 세       1  171052899 1.9274e+10 2842.0\n+ kFIP     1  162536872 1.9283e+10 2842.1\n+ FIP      1  154825743 1.9290e+10 2842.2\n+ ERA      1  107350094 1.9338e+10 2842.5\n+ LOB.     1   77049296 1.9368e+10 2842.8\n+ 홈런.9   1   73957140 1.9371e+10 2842.8\n+ 볼넷.9   1   64564811 1.9381e+10 2842.9\n+ BABIP    1   56938420 1.9388e+10 2842.9\n+ 홀드     1   38023685 1.9407e+10 2843.1\n+ 삼진.9   1    5508109 1.9440e+10 2843.3\n+ 경기     1      12651 1.9445e+10 2843.4\n\nStep:  AIC=2775.03\n연봉.2018. ~ 연봉.2017. + WAR\n\n          Df Sum of Sq        RSS    AIC\n+ 패       1 213356827 1.2190e+10 2774.4\n+ kFIP     1 187694356 1.2215e+10 2774.7\n+ 선발     1 171531569 1.2232e+10 2774.9\n+ FIP      1 168772833 1.2234e+10 2774.9\n+ 볼넷.9   1 164189202 1.2239e+10 2775.0\n&lt;none&gt;                 1.2403e+10 2775.0\n+ 이닝     1 147039192 1.2256e+10 2775.2\n+ 홈런.9   1  51612430 1.2351e+10 2776.4\n+ 삼진.9   1  48348966 1.2355e+10 2776.4\n+ 승       1  30075743 1.2373e+10 2776.7\n+ 경기     1  27245510 1.2376e+10 2776.7\n+ BABIP    1  24181791 1.2379e+10 2776.7\n+ ERA      1  17077047 1.2386e+10 2776.8\n+ 블론     1  11153112 1.2392e+10 2776.9\n+ RA9.WAR  1   6650871 1.2396e+10 2776.9\n+ 세       1   4332494 1.2399e+10 2777.0\n+ 홀드     1   3482363 1.2400e+10 2777.0\n+ LOB.     1    660176 1.2402e+10 2777.0\n\nStep:  AIC=2774.4\n연봉.2018. ~ 연봉.2017. + WAR + 패\n\n          Df Sum of Sq        RSS    AIC\n+ kFIP     1 197383620 1.1992e+10 2773.9\n+ 승       1 180715640 1.2009e+10 2774.1\n+ FIP      1 174958135 1.2015e+10 2774.2\n&lt;none&gt;                 1.2190e+10 2774.4\n+ 볼넷.9   1 103300993 1.2086e+10 2775.1\n+ 홈런.9   1  71014626 1.2119e+10 2775.5\n+ 삼진.9   1  66895356 1.2123e+10 2775.6\n+ 블론     1  42172679 1.2148e+10 2775.9\n+ BABIP    1  41953578 1.2148e+10 2775.9\n+ 선발     1  31473684 1.2158e+10 2776.0\n+ ERA      1  13441234 1.2176e+10 2776.2\n+ 이닝     1   5896647 1.2184e+10 2776.3\n+ 세       1   3470456 1.2186e+10 2776.3\n+ RA9.WAR  1   2414340 1.2187e+10 2776.4\n+ LOB.     1   1712854 1.2188e+10 2776.4\n+ 경기     1   1125166 1.2189e+10 2776.4\n+ 홀드     1    189917 1.2190e+10 2776.4\n\nStep:  AIC=2773.92\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP\n\n          Df Sum of Sq        RSS    AIC\n+ 승       1 167413120 1.1825e+10 2773.8\n&lt;none&gt;                 1.1992e+10 2773.9\n+ 블론     1 128359041 1.1864e+10 2774.3\n+ 선발     1 117641927 1.1875e+10 2774.4\n+ BABIP    1  75190355 1.1917e+10 2775.0\n+ ERA      1  21818455 1.1971e+10 2775.6\n+ 홀드     1  21403854 1.1971e+10 2775.6\n+ 삼진.9   1  19275489 1.1973e+10 2775.7\n+ 경기     1  17028183 1.1975e+10 2775.7\n+ 이닝     1  13040981 1.1979e+10 2775.8\n+ FIP      1   9361041 1.1983e+10 2775.8\n+ 볼넷.9   1   8843181 1.1983e+10 2775.8\n+ 홈런.9   1   8722328 1.1984e+10 2775.8\n+ LOB.     1   4031616 1.1988e+10 2775.9\n+ RA9.WAR  1   2013063 1.1990e+10 2775.9\n+ 세       1   1445393 1.1991e+10 2775.9\n\nStep:  AIC=2773.78\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승\n\n          Df Sum of Sq        RSS    AIC\n+ 이닝     1 215650124 1.1609e+10 2773.0\n+ 선발     1 196677432 1.1628e+10 2773.2\n&lt;none&gt;                 1.1825e+10 2773.8\n+ 블론     1  83011867 1.1742e+10 2774.7\n+ RA9.WAR  1  63182313 1.1762e+10 2775.0\n+ BABIP    1  45874866 1.1779e+10 2775.2\n+ 볼넷.9   1  17920561 1.1807e+10 2775.6\n+ 삼진.9   1  14563944 1.1810e+10 2775.6\n+ 홈런.9   1  12160231 1.1813e+10 2775.6\n+ ERA      1   8802604 1.1816e+10 2775.7\n+ FIP      1   8122100 1.1817e+10 2775.7\n+ 세       1   5821352 1.1819e+10 2775.7\n+ 홀드     1   5267064 1.1820e+10 2775.7\n+ LOB.     1    397579 1.1825e+10 2775.8\n+ 경기     1    331762 1.1825e+10 2775.8\n\nStep:  AIC=2772.98\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝\n\n          Df Sum of Sq        RSS    AIC\n&lt;none&gt;                 1.1609e+10 2773.0\n+ BABIP    1  87591503 1.1522e+10 2773.8\n+ 선발     1  50413708 1.1559e+10 2774.3\n+ 블론     1  39472232 1.1570e+10 2774.5\n+ 삼진.9   1  33863019 1.1575e+10 2774.5\n+ ERA      1  33524887 1.1576e+10 2774.5\n+ FIP      1  18310110 1.1591e+10 2774.7\n+ 홈런.9   1  12031455 1.1597e+10 2774.8\n+ RA9.WAR  1  10397930 1.1599e+10 2774.8\n+ LOB.     1   3136209 1.1606e+10 2774.9\n+ 경기     1   1950014 1.1607e+10 2775.0\n+ 볼넷.9   1   1288038 1.1608e+10 2775.0\n+ 세       1    227255 1.1609e+10 2775.0\n+ 홀드     1     93003 1.1609e+10 2775.0\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + \n    승 + 이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48378  -2526    133   2563  48361 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.653e+03  2.664e+03  -0.996   0.3209    \n연봉.2017.   8.856e-01  4.027e-02  21.992  &lt; 2e-16 ***\nWAR          8.003e+03  1.169e+03   6.848 1.97e-10 ***\n패          -2.708e+02  4.288e+02  -0.631   0.5287    \nkFIP         6.622e+02  4.042e+02   1.639   0.1035    \n승           1.025e+03  4.767e+02   2.150   0.0332 *  \n이닝        -7.811e+01  4.760e+01  -1.641   0.1029    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8948 on 145 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9164 \nF-statistic: 276.8 on 6 and 145 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model_forward)\n\n연봉.2017.1.9185269678088WAR4.50275401281816패3.39937428417761kFIP1.20965408072733승6.3982769107728이닝10.8086888355271\n\n\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝"
  },
  {
    "objectID": "posts/AS4_5.html#pca주성분분석",
    "href": "posts/AS4_5.html#pca주성분분석",
    "title": "AS HW4_5",
    "section": "PCA(주성분분석)",
    "text": "PCA(주성분분석)\n\n서로 상관성이 높은 변수들의 선형 결합으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법\n\n\ndt2 &lt;- dt[,1:19] # 설명변수\n\n\ndt3 &lt;- dt[,20] # 종속변수\n\n\nprocomp.result2 &lt;- prcomp(dt2, center=T, scale=T)\nsummary(procomp.result2)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.6109 1.8528 1.5587 1.27735 1.07673 0.94635 0.77437\nProportion of Variance 0.3588 0.1807 0.1279 0.08588 0.06102 0.04714 0.03156\nCumulative Proportion  0.3588 0.5395 0.6673 0.75322 0.81424 0.86137 0.89293\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.75305 0.60013 0.57118 0.51280 0.43707 0.31874 0.28648\nProportion of Variance 0.02985 0.01896 0.01717 0.01384 0.01005 0.00535 0.00432\nCumulative Proportion  0.92278 0.94173 0.95890 0.97274 0.98280 0.98815 0.99247\n                          PC15    PC16   PC17    PC18     PC19\nStandard deviation     0.27007 0.21161 0.1236 0.10061 0.006742\nProportion of Variance 0.00384 0.00236 0.0008 0.00053 0.000000\nCumulative Proportion  0.99630 0.99866 0.9995 1.00000 1.000000\n\n\n\ndt.pca &lt;- princomp(dt2, cor=TRUE)\n\n\nsummary(dt.pca)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5\nStandard deviation     2.6109027 1.8528422 1.5587351 1.2773530 1.07672785\nProportion of Variance 0.3587796 0.1806855 0.1278766 0.0858753 0.06101805\nCumulative Proportion  0.3587796 0.5394651 0.6673417 0.7532170 0.81423504\n                           Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     0.94635475 0.77436931 0.75305028 0.60012648 0.57117946\nProportion of Variance 0.04713617 0.03156041 0.02984656 0.01895536 0.01717084\nCumulative Proportion  0.86137122 0.89293163 0.92277819 0.94173355 0.95890439\n                          Comp.11   Comp.12    Comp.13     Comp.14     Comp.15\nStandard deviation     0.51280325 0.4370697 0.31874103 0.286476368 0.270069813\nProportion of Variance 0.01384038 0.0100542 0.00534715 0.004319406 0.003838827\nCumulative Proportion  0.97274477 0.9827990 0.98814612 0.992465529 0.996304355\n                           Comp.16      Comp.17      Comp.18      Comp.19\nStandard deviation     0.211606675 0.1235839263 0.1006053051 6.741848e-03\nProportion of Variance 0.002356704 0.0008038414 0.0005327067 2.392237e-06\nCumulative Proportion  0.998661060 0.9994649011 0.9999976078 1.000000e+00\n\n\n\n제 1주성분과 제6주성분까지의 누적 분산비율은 대략 85.71%로 6개의 주성분 변수를 활용해 전체 데이터의 85.71%를 설명할 수 있다.\n\n\nscreeplot(dt.pca, npcs=8, type=\"lines\")\n\n\n\n\n\n주성분들에 의해 설명되는 변동 비율\n\n\nloadings(dt.pca)\n\n\nLoadings:\n           Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\n승          0.322  0.201                              0.105              \n패          0.272  0.198         0.108  0.236 -0.108        -0.477 -0.294\n세                -0.202  0.289  0.126 -0.512 -0.426        -0.192  0.360\n홀드              -0.253  0.336  0.141  0.417  0.343  0.154  0.296       \n블론              -0.274  0.396  0.204        -0.208  0.116 -0.183 -0.354\n경기        0.191 -0.220  0.372  0.222  0.227                       0.236\n선발        0.261  0.350 -0.123                             -0.250 -0.105\n이닝        0.329  0.235                0.109               -0.134       \n삼진.9                    0.393 -0.404 -0.229  0.455 -0.171 -0.204 -0.211\n볼넷.9     -0.250  0.116               -0.236  0.289  0.752 -0.178       \n홈런.9     -0.173  0.286  0.276  0.268         0.154 -0.535              \nBABIP      -0.142  0.207  0.292 -0.444                              0.146\nLOB.        0.131 -0.209 -0.230  0.228 -0.416  0.495 -0.174              \nERA        -0.235  0.294  0.288 -0.156        -0.133  0.106         0.107\nRA9.WAR     0.327  0.176               -0.181  0.124         0.119  0.391\nFIP        -0.259  0.284         0.378                                   \nkFIP       -0.257  0.268         0.423                                   \nWAR         0.318  0.213               -0.126         0.113  0.165  0.235\n연봉.2017.  0.252  0.130  0.103        -0.296 -0.181         0.629 -0.535\n           Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\n승                  0.383   0.237   0.542   0.550   0.102                 \n패                 -0.478  -0.137           0.321  -0.270  -0.229         \n세         -0.134  -0.328           0.242           0.206   0.105         \n홀드        0.128  -0.339  -0.335   0.264  -0.113   0.240                 \n블론        0.363   0.513  -0.251  -0.150  -0.121                         \n경기       -0.183           0.560  -0.322          -0.240                 \n선발                                0.154  -0.463   0.328   0.249         \n이닝                        0.213          -0.370           0.151         \n삼진.9     -0.380                                                   0.376 \n볼넷.9     -0.105           0.128                                  -0.359 \n홈런.9                                                             -0.614 \nBABIP       0.665  -0.171   0.296  -0.149           0.187                 \nLOB.        0.414  -0.155           0.112          -0.328   0.216         \nERA                        -0.141   0.336  -0.206  -0.663   0.296         \nRA9.WAR             0.123  -0.243          -0.247  -0.163  -0.683         \nFIP                                                                 0.323 \nkFIP                                                                0.492 \nWAR                        -0.398  -0.499   0.305           0.473         \n연봉.2017.         -0.229   0.174                                         \n           Comp.18 Comp.19\n승                        \n패                        \n세                        \n홀드                      \n블론                      \n경기        0.318         \n선발        0.553         \n이닝       -0.758         \n삼진.9                    \n볼넷.9                    \n홈런.9             -0.125 \nBABIP                     \nLOB.                      \nERA                       \nRA9.WAR                   \nFIP                 0.754 \nkFIP               -0.641 \nWAR                       \n연봉.2017.                \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.053  0.053  0.053  0.053  0.053  0.053  0.053  0.053  0.053\nCumulative Var  0.053  0.105  0.158  0.211  0.263  0.316  0.368  0.421  0.474\n               Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\nSS loadings      1.000   1.000   1.000   1.000   1.000   1.000   1.000   1.000\nProportion Var   0.053   0.053   0.053   0.053   0.053   0.053   0.053   0.053\nCumulative Var   0.526   0.579   0.632   0.684   0.737   0.789   0.842   0.895\n               Comp.18 Comp.19\nSS loadings      1.000   1.000\nProportion Var   0.053   0.053\nCumulative Var   0.947   1.000"
  },
  {
    "objectID": "posts/AS4_5.html#glm",
    "href": "posts/AS4_5.html#glm",
    "title": "AS HW4_5",
    "section": "glm",
    "text": "glm\n\n\nX &lt;- model.matrix(연봉.2018.~., dt)[,-1] \ny &lt;- dt$연봉.2018.\n\n\nhead(X)\n\n\nA matrix: 6 × 19 of type dbl\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2017.\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n85000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n50000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n150000\n\n\n4\n10\n7\n0\n0\n0\n28\n28\n175.2\n8.04\n1.95\n1.02\n0.298\n75.0\n3.43\n6.11\n4.20\n4.03\n4.63\n100000\n\n\n5\n13\n7\n0\n0\n0\n30\n30\n187.1\n7.49\n2.11\n0.91\n0.323\n74.1\n3.80\n6.13\n4.36\n4.31\n4.38\n85000\n\n\n6\n8\n10\n0\n0\n0\n26\n26\n160.0\n7.42\n1.74\n1.12\n0.289\n76.1\n3.04\n6.52\n4.42\n4.32\n3.94\n35000\n\n\n\n\n\n\nridge.fit&lt;-glmnet(X,y,alpha=0, lambda=seq(0,100,10)) ##ridge : alpha=0 \nplot(ridge.fit, label=TRUE)\nabline(h=0, col=\"grey\", lty=2)\n\n\n\n\n\nsummary(ridge.fit)\n\n          Length Class     Mode   \na0         11    -none-    numeric\nbeta      209    dgCMatrix S4     \ndf         11    -none-    numeric\ndim         2    -none-    numeric\nlambda     11    -none-    numeric\ndev.ratio  11    -none-    numeric\nnulldev     1    -none-    numeric\nnpasses     1    -none-    numeric\njerr        1    -none-    numeric\noffset      1    -none-    logical\ncall        5    -none-    call   \nnobs        1    -none-    numeric\n\n\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=length(y))\n\nWarning message:\n“Option grouped=FALSE enforced in cv.glmnet, since &lt; 3 observations per fold”\n\n\n\ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = length(y), alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index   Measure       SE Nonzero\nmin   2869   100 117171048 49876335      19\n1se  12711    84 166795504 78214517      19\n\n\n\nplot(cv.fit)"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html",
    "href": "posts/01. Simple Linear Regression.html",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#산점도시각화-상관-계수-두-변수-사이-관계-요약",
    "href": "posts/01. Simple Linear Regression.html#산점도시각화-상관-계수-두-변수-사이-관계-요약",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "1. 산점도(시각화), 상관 계수 (두 변수 사이 관계 요약)",
    "text": "1. 산점도(시각화), 상관 계수 (두 변수 사이 관계 요약)"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#회귀모형-적합-widehat-eyx-widehat-beta_0-widehat-beta_1-x-epsilon-sim-n0-widehat-sigma2",
    "href": "posts/01. Simple Linear Regression.html#회귀모형-적합-widehat-eyx-widehat-beta_0-widehat-beta_1-x-epsilon-sim-n0-widehat-sigma2",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "2. 회귀모형 적합: \\(\\widehat E(y|x) = \\widehat \\beta_0 + \\widehat \\beta_1 x, \\epsilon \\sim N(0, \\widehat \\sigma^2)\\)",
    "text": "2. 회귀모형 적합: \\(\\widehat E(y|x) = \\widehat \\beta_0 + \\widehat \\beta_1 x, \\epsilon \\sim N(0, \\widehat \\sigma^2)\\)\n- 모수추정\n\n\\(\\widehat\\beta_0, \\widehat\\beta_1\\) : 최소제곱추정량(LSE)\n\\(\\widehat\\sigma^2\\) : MSE"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#통계적-유의성-검정",
    "href": "posts/01. Simple Linear Regression.html#통계적-유의성-검정",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "3. 통계적 유의성 검정",
    "text": "3. 통계적 유의성 검정\n- 회귀직선의 유의성 검정\n\nF검정\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta_1 \\neq 0\\)\n\n- 개별 회귀 계수의 유의성 검정\n\nt검정\n\\(H_0 : \\beta_0 = 0 \\ vs \\  H_1 : \\beta _{0}=\\begin{cases} &gt;0\\\\ \\neq 0\\\\ &lt;0\\end{cases}\\)\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta _{1}=\\begin{cases} &gt;0\\\\ \\neq 0\\\\ &lt;0\\end{cases}\\)\n\n\nNOTE: 단순회귀모형에서는 회귀직선이 유의성검정, 개별회귀 계수의 유의성 검정이 같다. 중회귀모형에서는 다르다."
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#모형의-적합도",
    "href": "posts/01. Simple Linear Regression.html#모형의-적합도",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "4. 모형의 적합도",
    "text": "4. 모형의 적합도\n\n\\(\\mathbb{R} ^{2}\\), MSE, \\(\\dots\\)"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#회귀진단",
    "href": "posts/01. Simple Linear Regression.html#회귀진단",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "5. 회귀진단",
    "text": "5. 회귀진단\n\n잔차분석(오차항에 대한 가정 검토)\n이상점(leverage point)\n변수선택, 다중곤산성"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#beta_1에-대한-추론",
    "href": "posts/01. Simple Linear Regression.html#beta_1에-대한-추론",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "\\(\\beta_1\\)에 대한 추론",
    "text": "\\(\\beta_1\\)에 대한 추론\n\\[\\widehat \\beta_1 = \\dfrac{S_{(xy)}}{S_{(xx)}}\\]\n분자 \\(S_{(xy)}= \\sum(x_i - \\bar x)(y_i - \\bar y) = \\sum(x_i - \\bar x)y_i - \\sum(x_i- \\bar x)\\bar y= \\sum(x_i - \\bar x)y_i\\)\n\\(\\widehat \\beta_1 = \\dfrac{S_{(xy)}}{S_{(xx)}}=\\dfrac{\\sum(x_i-\\bar x)y_i}{S_{(xx)}}=\\sum \\dfrac{x_i- \\bar x}{S_{(xx)}} y_i= \\sum a_i y_i\\)\n\\[\\widehat \\beta_1 \\sim N(\\beta_1, \\dfrac{\\sigma^2}{S_{(xx)}})\\]\n- \\(E(\\widehat \\beta_1)=\\beta_1\\) : 불편추정량(unbiase-)\n\n\\(E(\\widehat \\beta_1)\\)\n\n= \\(\\sum a_i E(y_i)\\)\n= \\(\\sum \\dfrac{(x_i - \\bar x)}{S_{xx}}(\\beta_0 + \\beta_1 x_i)\\)\n= \\(\\dfrac{1}{S_{xx}}[\\beta_0 \\sum(x_i - \\bar x) + \\beta_1 \\sum(x_i - \\bar x) x_i]\\)\n\\(\\because \\sum(x_i - \\bar x)=0\\)\n\\(\\because \\sum(x_i - \\bar x)(x_i -\\bar x + \\bar x) = \\sum(x_i- \\bar x)^2 + \\bar x \\sum(x_i - \\bar x) =\\sum(x_i- \\bar x)^2= S_{(xx)}\\)\n= \\(\\dfrac{\\beta_1 S_{xx}}{S_{xx}} = \\beta_1\\)\n- \\(Var(\\widehat \\beta_1)\\)\n\n\\(Var(\\widehat \\beta_1)\\)\n\n= \\(Var(\\sum a_i y_i)\\)\n= \\(\\sum a_i^2 Var(y_i)\\)\n\\(\\because Var(y_i)=\\sigma^2\\)\n= \\(\\sigma^2 \\sum a_i^2\\)\n= \\(\\sigma^2 \\sum \\dfrac{(x_i- \\bar x)^2}{S_{xx}^2}\\)\n= \\(\\sigma^2 \\dfrac{S_{xx}}{S_{xx}^2}\\)\n= \\(\\dfrac{\\sigma^2}{S_{xx}}\\)\n- BLUE\n\nBest Linear Unbiased Estimation\n\n\\[\\widehat{Var}(\\widehat \\beta_1) = \\dfrac{MSE}{S_{xx}}\\]\n\\[\\widehat \\sigma_{\\widehat \\beta_1} = \\sqrt{\\dfrac{MSE}{S_{xx}}}\\]\n- stuendtized \\(\\widehat \\beta_1\\)의 분포\n\\[\\dfrac{\\widehat \\beta_1 - \\beta_1}{\\widehat \\sigma / \\sqrt{S_{xx}}} \\sim t(n-2), \\widehat \\sigma = \\sqrt{MSE}\\]\n- \\(\\widehat \\beta_1\\)의 \\(100(1-\\alpha)\\)% 신뢰구간\n\\[\\widehat \\beta_1 \\pm t_{\\alpha/2}(n-2) \\dfrac{\\widehat \\sigma}{\\sqrt{S_{xx}}}\\]\n\n\\(\\sigma\\)를 몰라 추정하므로 t분포로 바뀌고 분산이 더 커진다.\n\n- 모회귀계수(기울기) \\(\\beta_1\\)에 대한 추론\n\nt검정\n\\(H_0 : \\beta_1 = 0 \\ vs \\  H_1 : \\beta _{1}=\\begin{cases} &gt;0\\\\ \\neq 0\\\\ &lt;0\\end{cases}\\)\n검정통계량\n\n\\[\\dfrac{\\widehat \\beta_1 - \\beta_1}{\\widehat \\sigma / \\sqrt{S_{xx}}} \\sim t(n-2)\\]"
  },
  {
    "objectID": "posts/01. Simple Linear Regression.html#beta_0에-대한-추론",
    "href": "posts/01. Simple Linear Regression.html#beta_0에-대한-추론",
    "title": "01. SLR(Simple Linear Regression)",
    "section": "\\(\\beta_0\\)에 대한 추론",
    "text": "\\(\\beta_0\\)에 대한 추론\n- \\(\\beta_0\\)의 최소제곱추정량\n\\[\\widehat \\beta_0 = \\bar y - \\widehat \\beta_1 \\bar x\\]\n\\[\\widehat \\beta_0 \\sim N(\\beta_0, \\sigma^2(\\dfrac{1}{n} + \\dfrac{\\bar x^2}{S_{xx}}))\\]\n- stuendtized \\(\\widehat \\beta_0\\)의 분포\n\\[\\dfrac{\\widehat \\beta_0 - \\beta_0}{\\widehat \\sigma_{\\widehat \\beta_0}} \\sim t(n-2)   ,  \\widehat \\sigma_{\\widehat \\beta_0} = \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]\n- \\(\\widehat \\beta_1\\)의 \\(100(1-\\alpha)\\)% 신뢰구간\n\\[\\widehat \\beta_0 \\pm t_{\\alpha/2}(n-2) \\widehat \\sigma \\sqrt{\\dfrac{1}{n}+\\dfrac{\\bar x^2}{S_{xx}}}\\]"
  },
  {
    "objectID": "posts/08. 다항회귀실습.html",
    "href": "posts/08. 다항회귀실습.html",
    "title": "08. 다항회귀 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임\n\n\n실습1\n\n\nhard &lt;- read.csv(\"hardwood.csv\")\nhead(hard)\n\n\nA data.frame: 6 × 2\n\n\n\nx\ny\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1.0\n6.3\n\n\n2\n1.5\n11.1\n\n\n3\n2.0\n20.0\n\n\n4\n3.0\n24.0\n\n\n5\n4.0\n26.1\n\n\n6\n4.5\n30.0\n\n\n\n\n\n\\(y = β_0 + β_1x + ϵ\\)\n\npar(mfrow=c(1,1))\nplot(y~x, hard, pch=16, main=\"scatter plot : hardwood\")\n\n\n\n\n\\(y = β_0 + β_1(x − \\bar x) + ϵ\\)\n\nhard$cx &lt;- hard$x - mean(hard$x)\nplot(y~cx, hard, pch=16, main=\"scatter plot : hardwood\")\n\n\n\n\n\nhard_fit &lt;- lm(y~x, hard)\nsummary(hard_fit)\n\n\nCall:\nlm(formula = y ~ x, data = hard)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-25.986  -3.749   2.938   7.675  15.840 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  21.3213     5.4302   3.926  0.00109 **\nx             1.7710     0.6478   2.734  0.01414 * \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.82 on 17 degrees of freedom\nMultiple R-squared:  0.3054,    Adjusted R-squared:  0.2645 \nF-statistic: 7.474 on 1 and 17 DF,  p-value: 0.01414\n\n\n\\(y= \\beta_0+\\beta_1x + \\beta_2 x^2 + \\epsilon\\)\n\nhard_fit_2 &lt;- lm(y~x+I(x^2), hard)\nsummary(hard_fit_2)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = hard)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8503 -3.2482 -0.7267  4.1350  6.5506 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6.67419    3.39971  -1.963   0.0673 .  \nx           11.76401    1.00278  11.731 2.85e-09 ***\nI(x^2)      -0.63455    0.06179 -10.270 1.89e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.42 on 16 degrees of freedom\nMultiple R-squared:  0.9085,    Adjusted R-squared:  0.8971 \nF-statistic: 79.43 on 2 and 16 DF,  p-value: 4.912e-09\n\n\n\\(y= \\beta_0+\\beta_1cx + \\beta_2 cx^2 + \\epsilon\\)\n\nhard_fit_c_2 &lt;- lm(y~cx+I(cx^2), hard)\nsummary(hard_fit_c_2)\n\n\nCall:\nlm(formula = y ~ cx + I(cx^2), data = hard)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8503 -3.2482 -0.7267  4.1350  6.5506 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.29497    1.48287   30.55 1.29e-15 ***\ncx           2.54634    0.25384   10.03 2.63e-08 ***\nI(cx^2)     -0.63455    0.06179  -10.27 1.89e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.42 on 16 degrees of freedom\nMultiple R-squared:  0.9085,    Adjusted R-squared:  0.8971 \nF-statistic: 79.43 on 2 and 16 DF,  p-value: 4.912e-09\n\n\n\n모형의 적합도는 동일한 값이 나온다.\n기울기가 조금 다르게 나온다.\n\\(\\beta_2\\)는 동일하게 나오지만, \\(\\beta_0,\\beta_1\\)이 조금 다르게 나온다.\nstandard error가 좀 다르게 나온다.\n\n\nprint(paste0(\"corr(x, x^2) = \", round(cor(hard$x, hard$x^2),3)))\n\n[1] \"corr(x, x^2) = 0.97\"\n\n\n\nprint(paste0(\"corr(cx, cx^2) = \", round(cor(hard$cx, hard$cx^2),3)))\n\n[1] \"corr(cx, cx^2) = 0.297\"\n\n\n\nplot(y~x, hard, pch=16, main=\"scatter plot : hardwood\")\nabline(hard_fit, col='blue', lwd=2)\nlines(hard$x, fitted(hard_fit_c_2),col='red', lwd=2)\nlegend(\"topleft\", c(\"x\",\"cx+cx^2\"), col=c('blue','red'), lwd=2, lty=1)\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(fitted(hard_fit), rstandard(hard_fit), pch =16, \n     xlab = expression(hat(y)), \n     ylab = \"Standardized Residuals\", \n     main =\"Fitted vs. Residuals\")\nabline(h =0, col =\"grey\", lwd =2, lty=2)\n\nplot(fitted(hard_fit_c_2), rstandard(hard_fit_c_2), pch =16,\n     xlab = expression(hat(y)),\n     ylab = \"Standardized Residuals\",\n     main = \"Fitted vs. Residuals\")\nabline(h = 0, col =\"grey\", lwd =2, lty=2)\n\n\n\n\n\npar(mfrow=c(1,1))\nplot(y~x, hard, pch=16, main=\"scatter plot : hardwood\")\nlines(hard$x, fitted(hard_fit_c_2),col='red', lwd=2)\nlines(hard$x, fitted(lm(y~cx+I(cx^2)+I(cx^3), hard)),col='blue', lwd=2) \nlegend(\"topleft\",c(expression(ldots+cx^2), \n            expression(ldots+cx^3)), col=c('blue', 'red'), lwd=2, lty=1, bty = \"n\")                  \n\n\n\n\n\nplot(y~x, hard, pch=16, main=\"scatter plot : hardwood\")\nlines(hard$x, fitted(lm(y~cx+I(cx^2)+I(cx^3), hard)),col='blue', lwd=2)\nlines(hard$x, fitted(lm(y~cx+I(cx^2)+I(cx^3)+I(cx^4)+I(cx^5), hard)),col='red', lwd=2) \nlegend(\"topleft\", c(expression(ldots+cx^3), \n                    expression(ldots+cx^5)), col=c('blue', 'red'), lwd=2, lty=1, bty = \"n\")                                                                                       \n\n\n\n\n\n5승 들어가면 잔차가 더 작아진다. -&gt; 과적합\n\n\nsummary(lm(y~cx+I(cx^2)+I(cx^3)+I(cx^4)+I(cx^5), hard))\n\n\nCall:\nlm(formula = y ~ cx + I(cx^2) + I(cx^3) + I(cx^4) + I(cx^5), \n    data = hard)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65167 -0.91159 -0.03811  0.96396  2.56865 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 43.6187788  0.7309210  59.676  &lt; 2e-16 ***\ncx           5.3479308  0.3896655  13.724 4.11e-09 ***\nI(cx^2)     -0.1378567  0.1059263  -1.301 0.215700    \nI(cx^3)     -0.1630817  0.0289147  -5.640 8.06e-05 ***\nI(cx^4)     -0.0114448  0.0026525  -4.315 0.000840 ***\nI(cx^5)      0.0021978  0.0005163   4.257 0.000935 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.703 on 13 degrees of freedom\nMultiple R-squared:  0.989, Adjusted R-squared:  0.9847 \nF-statistic: 233.1 on 5 and 13 DF,  p-value: 3.022e-12\n\n\n\n\n실습2\n\nset.seed(12)\nx &lt;- seq(1,2,0.05) # 21개의 데이터\nx2 &lt;- x^2\ny &lt;- 3*x+1+4*x2 + rnorm(21)\n\n\n\\(y=1+3x+4x^2+N(0,1)\\)\n\n\npar(mfrow=c(1,2)) \nplot(x, y, pch=16) \nplot(x2,y, pch=16)\n\n\n\n\n\n1~2까지의 X의 범위 일부분\n커브는 보이지 않고 직선 형태로만 보인다.\n\n\ncor(x,x2)\n\n0.995978098018456\n\n\n\nplot(x,x2,pch=16)\n\n\n\n\n\nm &lt;- lm(y~x+x2)\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x + x2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.40039 -0.44786 -0.07384  0.31824  2.14916 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   2.5112     4.8813   0.514   0.6132  \nx            -0.5728     6.6908  -0.086   0.9327  \nx2            5.5131     2.2213   2.482   0.0232 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8317 on 18 degrees of freedom\nMultiple R-squared:  0.9755,    Adjusted R-squared:  0.9727 \nF-statistic: 357.8 on 2 and 18 DF,  p-value: 3.226e-15\n\n\n\n\\(y=\\beta_0+\\beta_1x+\\beta_2x^2+\\epsilon\\)\n모형자체는 유의하게 나오지만 x와 계수는 유의하지 않고, x2는 적게 나옴 -&gt; 다중공산성\n\n\ncx &lt;- x-mean(x)\ncx2 &lt;- cx^2\n\n\ncor(cx, cx2)\n\n2.28352723203241e-16\n\n\n\nplot(cx,cx2, pch=16)\n\n\n\n\n\nm2 &lt;- lm(y~cx+I(cx^2))\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ cx + I(cx^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.40039 -0.44786 -0.07384  0.31824  2.14916 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  14.0565     0.2728  51.533  &lt; 2e-16 ***\ncx           15.9665     0.5995  26.634 6.53e-16 ***\nI(cx^2)       5.5131     2.2213   2.482   0.0232 *  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8317 on 18 degrees of freedom\nMultiple R-squared:  0.9755,    Adjusted R-squared:  0.9727 \nF-statistic: 357.8 on 2 and 18 DF,  p-value: 3.226e-15\n\n\n\\(y=\\beta_0+\\beta_1(x-\\bar x) + \\beta_2 (x-\\bar x)^2 + \\epsilon\\)\n\\(\\because x- \\bar x = cx\\)\n\\(lm(y\\)~\\(x+x\\)^\\(2)\\) 이건 성립이 안된다.\n\\(lm(y\\)~ \\(x+I(x\\)^\\(2))\\)이렇게 써야해\n\\(\\hat \\beta_2\\)는 위의 m1모형의 값과 같다.\n\\(\\hat \\beta_1\\)과 \\(\\hat \\beta_0\\)은 m1모형의 값과 아주 다르다.\n\npar(mfrow=c(1,2))\nplot(x,y, pch=16)\nlines(x, fitted(m), col='red', lwd=2) \nplot(x,y, pch=16)\nlines(x, fitted(m2), col='blue', lwd=2)\n\n\n\n\n\\(y=\\beta_0+\\beta_1(x_1 - \\bar x) +\\epsilon\\)\n\\(= \\beta_0+\\beta_1x_1 - \\beta_1 \\bar x + \\epsilon\\)\n\\(=(\\beta_0 - \\beta_1 \\bar x) +\\beta_1 x_1 +\\epsilon\\)\n\\(= \\beta^`_0 + \\beta_1x_1 + \\epsilon\\)\n\\(y=\\beta_0+\\beta_1(x-\\bar x)+\\beta_2(x-\\bar x)^2\\)\n\\(=\\beta_0+\\beta_1x - \\beta_1 \\bar x + \\beta_2 x^2 - 2x \\bar x \\beta + \\beta_2 (\\bar x)^2\\)\n\\(=((\\beta_0-\\beta_1 \\bar x) + \\beta_2 (\\bar x)^2)+ (\\beta_1 - 2 \\bar x \\beta_2) x + \\beta_2 x^2\\)\n\n센터링을 하게 되면 회귀계수들의 다중공산성을 조금 제거할 수 있다."
  },
  {
    "objectID": "posts/alpha.html",
    "href": "posts/alpha.html",
    "title": "alpha",
    "section": "",
    "text": ": 제 1종 오류를 범할 확률\n즉, \\(H_0\\)가 참임에도 \\(H_0\\)를 기각할 확률\n1 귀무가설 \\(H_0\\)와 대립가설 \\(H_1\\)를 설정\n2 검정통계랑 구하기\n3 검정통계량의 기각역 구하기\n4 검정통계량 값이 기각역에 있으면 귀무가설을 기각\n\n\n임곘값: 주어진 유의수준을 검정통계량의 값으로 환산한 값\nF검정시 R에서 쓰는 코드\nqf(100(1-𝛼),df1,df2)\n검정통계량 \\(F_0\\)\n\\(F_0 &gt; F_𝛼\\) 이면 귀무가설을 기각\n만약 df=1, 8, 𝛼=0.05이면\n\nqf(0.05,1,8)\n\n0.00418615505332731\n\n\n\nqf(0.95,1,8)\n\n5.31765507157871\n\n\n\n오시 pf라는것도 있는데..\n\n\n\n\nt검정시 R에서 쓰는 코드\nqt(𝛼,df)\nt분포에서는 \\(𝛼/2\\)를 해줘야 한다.\n검정통계량 \\(t_0\\)\n\\(t_0 &gt; t_{𝛼/2}\\) 이면 귀무가설을 기각\n만약 df=8, 𝛼=0.05이면, t분포는 𝛼/2=0.025\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\nqt(0.025,8)\n\n-2.30600413520417\n\n\n\n\n\n잔차의 독립성 검정을 위한 test\n\\(H_0\\):독립이다. \\(H_1\\): not \\(H_0\\)\nDW값이 2에 가까울수록 양의 상관관계, 4에 가까울수록 음의 상관관계, 2를 기준으로 2이면 보류\np-value의 값이 유의수준(𝛼) 보다 크다면 귀무가설을 기각할 수 없음. 즉 독립\npvalue &gt; 𝛼 이면 귀무가설을 채택한다.\n\n\n\n정규성을 검증\n\\(H_0\\):정규분포이다. \\(H_1\\): not \\(H_0\\)\npvalue &gt; 𝛼 이면 귀무가설을 채택한다."
  },
  {
    "objectID": "posts/alpha.html#f검정",
    "href": "posts/alpha.html#f검정",
    "title": "alpha",
    "section": "",
    "text": "임곘값: 주어진 유의수준을 검정통계량의 값으로 환산한 값\nF검정시 R에서 쓰는 코드\nqf(100(1-𝛼),df1,df2)\n검정통계량 \\(F_0\\)\n\\(F_0 &gt; F_𝛼\\) 이면 귀무가설을 기각\n만약 df=1, 8, 𝛼=0.05이면\n\nqf(0.05,1,8)\n\n0.00418615505332731\n\n\n\nqf(0.95,1,8)\n\n5.31765507157871\n\n\n\n오시 pf라는것도 있는데.."
  },
  {
    "objectID": "posts/alpha.html#t검정",
    "href": "posts/alpha.html#t검정",
    "title": "alpha",
    "section": "",
    "text": "t검정시 R에서 쓰는 코드\nqt(𝛼,df)\nt분포에서는 \\(𝛼/2\\)를 해줘야 한다.\n검정통계량 \\(t_0\\)\n\\(t_0 &gt; t_{𝛼/2}\\) 이면 귀무가설을 기각\n만약 df=8, 𝛼=0.05이면, t분포는 𝛼/2=0.025\n\nqt(0.975,8)\n\n2.30600413520417\n\n\n\nqt(0.025,8)\n\n-2.30600413520417"
  },
  {
    "objectID": "posts/alpha.html#dw-test",
    "href": "posts/alpha.html#dw-test",
    "title": "alpha",
    "section": "",
    "text": "잔차의 독립성 검정을 위한 test\n\\(H_0\\):독립이다. \\(H_1\\): not \\(H_0\\)\nDW값이 2에 가까울수록 양의 상관관계, 4에 가까울수록 음의 상관관계, 2를 기준으로 2이면 보류\np-value의 값이 유의수준(𝛼) 보다 크다면 귀무가설을 기각할 수 없음. 즉 독립\npvalue &gt; 𝛼 이면 귀무가설을 채택한다."
  },
  {
    "objectID": "posts/alpha.html#shapiro-test",
    "href": "posts/alpha.html#shapiro-test",
    "title": "alpha",
    "section": "",
    "text": "정규성을 검증\n\\(H_0\\):정규분포이다. \\(H_1\\): not \\(H_0\\)\npvalue &gt; 𝛼 이면 귀무가설을 채택한다."
  },
  {
    "objectID": "posts/11. 편의추정 실습.html",
    "href": "posts/11. 편의추정 실습.html",
    "title": "11. 편의추정 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임\nlibrary(MASS) #lm.ridge \nlibrary(car) #vif\nlibrary(glmnet) #Ridge, Lasso\n\nLoading required package: Matrix\n\nLoaded glmnet 4.1-7"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#완전한-다중공산성을-갖는-데이터-생성",
    "href": "posts/11. 편의추정 실습.html#완전한-다중공산성을-갖는-데이터-생성",
    "title": "11. 편의추정 실습",
    "section": "완전한 다중공산성을 갖는 데이터 생성",
    "text": "완전한 다중공산성을 갖는 데이터 생성\n\ngen_perfect_collin_data = function(num_samples = 100) {\n     x1 = rnorm(n = num_samples, mean = 80, sd = 10)\n     x2 = rnorm(n = num_samples, mean = 70, sd = 5)\n     x3 = 2 * x1 + 4 * x2 + 3\n     y = 3 + x1 + x2 + rnorm(n = num_samples, mean = 0, sd = 1)  \n    data.frame(y, x1, x2, x3)\n    }\n\n\nx1과 x2는 independent\nx3는 선형결합\n\n\nset.seed(42)\nperfect_collin_data = gen_perfect_collin_data() \nhead(perfect_collin_data)\n\n\nA data.frame: 6 × 4\n\n\n\ny\nx1\nx2\nx3\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n170.7135\n93.70958\n76.00483\n494.4385\n\n\n2\n152.9106\n74.35302\n75.22376\n452.6011\n\n\n3\n152.7866\n83.63128\n64.98396\n430.1984\n\n\n4\n170.6306\n86.32863\n79.24241\n492.6269\n\n\n5\n152.3320\n84.04268\n66.66613\n437.7499\n\n\n6\n151.3155\n78.93875\n70.52757\n442.9878\n\n\n\n\n\n\nround(cor(perfect_collin_data),4)\n\n\nA matrix: 4 × 4 of type dbl\n\n\n\ny\nx1\nx2\nx3\n\n\n\n\ny\n1.0000\n0.9112\n0.4307\n0.9558\n\n\nx1\n0.9112\n1.0000\n0.0313\n0.7639\n\n\nx2\n0.4307\n0.0313\n1.0000\n0.6690\n\n\nx3\n0.9558\n0.7639\n0.6690\n1.0000\n\n\n\n\n\n\nperfect_collin_fit = lm(y ~ x1 + x2 + x3, data = perfect_collin_data) \nsummary(perfect_collin_fit)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = perfect_collin_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.57662 -0.66188 -0.08253  0.63706  2.52057 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.957336   1.735165   1.704   0.0915 .  \nx1          0.985629   0.009788 100.702   &lt;2e-16 ***\nx2          1.017059   0.022545  45.112   &lt;2e-16 ***\nx3                NA         NA      NA       NA    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.014 on 97 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9921 \nF-statistic:  6236 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\np-value값 유의하게 나오고, \\(R^2\\)값도 크다.\n\\(rank(X)=3\\)(x3가 x1과 x2의 선형결합으로 이루어진 변수이므로)\n\\(\\rightarrow (X^TX)^{-1}\\)값을 구할 수 없으므로 \\(\\hat \\beta\\)값을 구할 수 없다. 그래서 R에서는 한 개의 변수를 빼고 lm을 돌린다.\n\n\nfit1 = lm(y ~ x1 + x2, data = perfect_collin_data) \nfit2 = lm(y ~ x1 + x3, data = perfect_collin_data) \nfit3 = lm(y ~ x2 + x3, data = perfect_collin_data)\n\n\n\nx3는 선형결합으로 이루어진 변수이므로 위 예시에서 두 개의 변수만 안다는 것은 세 개의 변수를 안다는 것과 동치\n\n\nsummary(fit1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = perfect_collin_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.57662 -0.66188 -0.08253  0.63706  2.52057 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.957336   1.735165   1.704   0.0915 .  \nx1          0.985629   0.009788 100.702   &lt;2e-16 ***\nx2          1.017059   0.022545  45.112   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.014 on 97 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9921 \nF-statistic:  6236 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\nsummary(fit2)\n\n\nCall:\nlm(formula = y ~ x1 + x3, data = perfect_collin_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.57662 -0.66188 -0.08253  0.63706  2.52057 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.194542   1.750225   1.254    0.213    \nx1          0.477100   0.015158  31.475   &lt;2e-16 ***\nx3          0.254265   0.005636  45.112   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.014 on 97 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9921 \nF-statistic:  6236 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(y=\\beta_0 + \\beta_1x_1 + \\beta_2 x_3 + \\epsilon\\)\n\\(x_3 = 2x_1+4x_2+3\\) 대입\n\\(y=\\beta_0 +\\beta_1x_1 + 2\\beta_2x_1 + 4\\beta_2 x_2 + 3\\beta_2 = (\\beta_0 + 3\\beta_2)+(\\beta_1+2\\beta_2)x_1 + 4\\beta_2 x_2\\)\n계수가 좀 달라지지만 x1과 x2의 식으로 다시 써질 수 있음\n\n\nsummary(fit3)\n\n\nCall:\nlm(formula = y ~ x2 + x3, data = perfect_collin_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.57662 -0.66188 -0.08253  0.63706  2.52057 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.478892   1.741452   0.849    0.398    \nx2          -0.954200   0.030316 -31.475   &lt;2e-16 ***\nx3           0.492815   0.004894 100.702   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.014 on 97 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9921 \nF-statistic:  6236 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\n\nfit1, fit2, fit3의 \\(R^2\\)가 다 똑같다. 모형의 적합도가 동일하다.\n\n\nall.equal(fitted(fit1), fitted(fit2))\n\nTRUE\n\n\n\nall.equal(fitted(fit2), fitted(fit3))\n\nTRUE\n\n\n\nall.equal: 동일한 함수이냐?\n\n\ncoef(fit1)\n\n(Intercept)2.95733574182587x10.985629075384886x21.01705863569558\n\n\n\ncoef(fit2)\n\n(Intercept)2.19454176505435x10.477099757537094x30.254264658923896\n\n\n\ncoef(fit3)\n\n(Intercept)1.47889212874879x2-0.954199515074186x30.492814537692442"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#완전에-가까운-다중공선성을-갖는-데이터-생성",
    "href": "posts/11. 편의추정 실습.html#완전에-가까운-다중공선성을-갖는-데이터-생성",
    "title": "11. 편의추정 실습",
    "section": "완전에 가까운 다중공선성을 갖는 데이터 생성",
    "text": "완전에 가까운 다중공선성을 갖는 데이터 생성\n\ngen_almost_collin_data = function(num_samples = 100) {\n    x1 = rnorm(n = num_samples, mean = 0, sd = 2)\n    x2 = rnorm(n = num_samples, mean = 0, sd = 3)\n    x3 = 3*x1 + 1*x2 + rnorm(num_samples, mean=0, sd=0.5)\n    y = 3 + x1 + x2 + rnorm(n = num_samples, mean = 0, sd = 1)  \n    data.frame(y, x1, x2, x3)\n    }\n\n\nrank(X)=4가 된다. 약간의 noise가 있음\n\n\nset.seed(42)\nalmost_collin_data = gen_almost_collin_data() \nhead(almost_collin_data)\n\n\nA data.frame: 6 × 4\n\n\n\ny\nx1\nx2\nx3\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n9.3401923\n2.7419169\n3.6028961\n10.82818219\n\n\n2\n5.7650991\n-1.1293963\n3.1342533\n-0.08704717\n\n\n3\n0.7556218\n0.7262568\n-3.0096259\n-0.24519291\n\n\n4\n10.5462431\n1.2657252\n5.5454457\n10.37239096\n\n\n5\n1.6617438\n0.8085366\n-2.0003202\n-0.26314109\n\n\n6\n3.0464051\n-0.2122490\n0.3165414\n-0.89563344\n\n\n\n\n\n\nround(cor(almost_collin_data),3)\n\n\nA matrix: 4 × 4 of type dbl\n\n\n\ny\nx1\nx2\nx3\n\n\n\n\ny\n1.000\n0.616\n0.769\n0.863\n\n\nx1\n0.616\n1.000\n0.031\n0.913\n\n\nx2\n0.769\n0.031\n1.000\n0.429\n\n\nx3\n0.863\n0.913\n0.429\n1.000\n\n\n\n\n\n\nm &lt;- lm(y~., almost_collin_data) \n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = almost_collin_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7944 -0.5867 -0.1038  0.6188  2.3280 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.03150    0.08914  34.007  &lt; 2e-16 ***\nx1           1.21854    0.52829   2.307   0.0232 *  \nx2           1.06616    0.18314   5.821 7.71e-08 ***\nx3          -0.06322    0.17765  -0.356   0.7227    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.8867 on 96 degrees of freedom\nMultiple R-squared:  0.9419,    Adjusted R-squared:  0.9401 \nF-statistic:   519 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\n모형은 유의하고, \\(R^2\\)값도 큰 편\n설명변수들은 별로 유의하지 않다.(x1과 x3).. 다중공산성이 있기 떄문에\n위의 fit3와 비교해보면, std.Error값이 많이 커진 것을 볼 수 있음. x2의 std.Error값 (0.03 -&gt; 0.183) x3의 std.Error값 (0.005 -&gt; 0.1777)\n\n\nvif(m)\n\nx1152.426839030633x231.0734919101693x3186.719994280627\n\n\n\nvif&gt;10 이면 다중공산성이 있다.\nStd. Error 값이 커진다. -&gt; 불안정해짐. 과녁중앙에서 벗어나게..\n\n- y에 noise\n\nset.seed(1000)\nnoise &lt;- rnorm(n = 100, mean = 0, sd =0.5) \nm_noise &lt;- lm(y+noise~., almost_collin_data) \nsummary(m_noise)\n\n\nCall:\nlm(formula = y + noise ~ ., data = almost_collin_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0962 -0.6998 -0.0891  0.7726  2.8462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.0403     0.1020  29.815  &lt; 2e-16 ***\nx1            0.9894     0.6043   1.637    0.105    \nx2            0.9898     0.2095   4.725 7.88e-06 ***\nx3            0.0158     0.2032   0.078    0.938    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 1.014 on 96 degrees of freedom\nMultiple R-squared:  0.9259,    Adjusted R-squared:  0.9236 \nF-statistic:   400 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\n\n\nround(coef(m),3)\n\n(Intercept)3.032x11.219x21.066x3-0.063\n\n\n\nround(coef(m_noise),3)\n\n(Intercept)3.04x10.989x20.99x30.016\n\n\n\nm과 m_noise를 비교해보기\n다중공산성이 없는 경우와 비교해보았을때, 위 coef의 값은 서로 차이가 많이 있음"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#다중공선성이-없는-경우-비교",
    "href": "posts/11. 편의추정 실습.html#다중공선성이-없는-경우-비교",
    "title": "11. 편의추정 실습",
    "section": "다중공선성이 없는 경우 비교",
    "text": "다중공선성이 없는 경우 비교\n\nm1 &lt;- lm(y~x1+x2, almost_collin_data)\nm1_noise &lt;- lm(y+noise~x1+x2, almost_collin_data)\n\n\nvif(m1)\n\nx11.00097938645275x21.00097938645275\n\n\n\nround(coef(m1),3)\n\n(Intercept)3.031x11.031x21.002\n\n\n\n\\(y\\)~\\(x_1+x_2\\)\n\n\nround(coef(m1_noise),3)\n\n(Intercept)3.04x11.036x21.006\n\n\n\n\\(y+noise\\)~\\(x_1+x_2\\)\n다중공산성이 없는 경우 noise를 추가해도 회귀계수가 별 차이가 업승ㅁ\n\n\\[VIF = \\dfrac{1}{1-R_j^2}\\]\n- VIF구하는 식\n\nm_sub &lt;- lm(x3~x1+x2,almost_collin_data)\n\n\nc33 &lt;- 1/(1-summary(m_sub)$r.sq);c33 ##vif\n\n186.719994280622\n\n\n\nvif(m)\n\nx1152.426839030633x231.0734919101693x3186.719994280627"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#lm.ridge-함수-이용",
    "href": "posts/11. 편의추정 실습.html#lm.ridge-함수-이용",
    "title": "11. 편의추정 실습",
    "section": "lm.ridge 함수 이용",
    "text": "lm.ridge 함수 이용\n\nrfit &lt;- lm.ridge(mpg~., dt, lambda=seq(0.01,20,0.1))\n\n\n\\(\\hat\\beta(\\lambda) = (X^TX+\\lambda I_p)^{-1} X^Ty\\)\n람다 범위는 0.01~0.1 범위에서 정해줘.\n\n\nselect(rfit) \n\nmodified HKB estimator is 2.58585 \nmodified L-W estimator is 1.837435 \nsmallest value of GCV  at 14.91 \n\n\n\n람다 값 몇개 추전해주는 함수. smallest value of GCV 이것을 많이 씀\nMSE를 가장 작게 하는 \\(\\lambda\\)값을 찾아줘! GCV\n\n\nround(rfit$coef[,rfit$lam=='0.21'],3)\n\ncyl-0.032disp0.185hp-0.211drat0.074wt-0.52qsec0.207vs0.027am0.201gear0.082carb-0.093\n\n\n\n람다=0.21일 때\n\n\nround(rfit$coef[,rfit$lam=='3.21'],3)\n\ncyl-0.078disp-0.049hp-0.144drat0.086wt-0.291qsec0.085vs0.041am0.169gear0.075carb-0.175\n\n\n\nround(rfit$coef[,rfit$lam=='14.91'],3)\n\ncyl-0.109disp-0.107hp-0.13drat0.092wt-0.197qsec0.047vs0.063am0.132gear0.066carb-0.144\n\n\n\n\\(\\hat \\beta(14.91)\\)\n\\(\\hat \\beta^{Ridge}(\\lambda) = argmin\\{\\sum \\epsilon_i^2\\}\\)\n\\(\\sum \\beta_j^2 \\leq t\\)\n\\(min\\{\\sum \\epsilon_i^2 + \\lambda \\sum \\beta_j^2 \\}\\)\n\n- 람다의 값이 커질 때, \\(\\sum \\hat \\beta_j^2\\)의 계수 값은 작아진다.\n\nsum(rfit$coef[,rfit$lam=='0.21']^2)\n\n0.45567046636623\n\n\n\n람다=0.21 일 때 \\(\\sum \\hat \\beta_j^2\\)의 계수값\n\n\nsum(rfit$coef[,rfit$lam=='3.21']^2)\n\n0.195026451544005\n\n\n\n람다=3.21 일 때 \\(\\sum \\hat \\beta_j^2\\)의 계수값\n\n\nsum(rfit$coef[,rfit$lam=='14.91']^2)\n\n0.135989355255867\n\n\n\n람다=14.91 일 때 \\(\\sum \\hat \\beta_j^2\\)의 계수값\n\n\nmatplot(rfit$lambda, t(rfit$coef), type='l',\n        xlab=expression(lambda),\n        ylab=expression(bold(beta)(lambda)), lwd=2) \nabline(h=0, col=\"grey\", lty=2)\nabline(v=14.91, col=\"black\", lty=2)"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#glmnet-함수-이용",
    "href": "posts/11. 편의추정 실습.html#glmnet-함수-이용",
    "title": "11. 편의추정 실습",
    "section": "glmnet 함수 이용",
    "text": "glmnet 함수 이용\n\n매트릭스를 만들어 줘야 함!\n그런데 X하뭇에서 1열은 필요 없음(1 1 1 1 .. 이렇게 되어있는) 그래서 아래와 같이 [,-1]로 X(상수항 빠짐)를 가져옴\n\n\nX &lt;- model.matrix(mpg~., dt)[,-1] \ny &lt;- dt$mpg\n\n\nX &lt;- as.matrix(dt[,-1]) 해도 같은 결과 나올듯\n\n\nhead(X)\n\n\nA matrix: 6 × 10 of type dbl\n\n\n\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n-0.1049878\n-0.57061982\n-0.5350928\n0.5675137\n-0.610399567\n-0.7771651\n-0.8680278\n1.1899014\n0.4235542\n0.7352031\n\n\nMazda RX4 Wag\n-0.1049878\n-0.57061982\n-0.5350928\n0.5675137\n-0.349785269\n-0.4637808\n-0.8680278\n1.1899014\n0.4235542\n0.7352031\n\n\nDatsun 710\n-1.2248578\n-0.99018209\n-0.7830405\n0.4739996\n-0.917004624\n0.4260068\n1.1160357\n1.1899014\n0.4235542\n-1.1221521\n\n\nHornet 4 Drive\n-0.1049878\n0.22009369\n-0.5350928\n-0.9661175\n-0.002299538\n0.8904872\n1.1160357\n-0.8141431\n-0.9318192\n-1.1221521\n\n\nHornet Sportabout\n1.0148821\n1.04308123\n0.4129422\n-0.8351978\n0.227654255\n-0.4637808\n-0.8680278\n-0.8141431\n-0.9318192\n-0.5030337\n\n\nValiant\n-0.1049878\n-0.04616698\n-0.6080186\n-1.5646078\n0.248094592\n1.3269868\n1.1160357\n-0.8141431\n-0.9318192\n-1.1221521\n\n\n\n\n\n\nhead(y)  #mpg\n\n\n0.1508848246476570.1508848246476570.4495434466306470.217253407310543-0.230734525663942-0.330287399658272"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#lidge",
    "href": "posts/11. 편의추정 실습.html#lidge",
    "title": "11. 편의추정 실습",
    "section": "Lidge",
    "text": "Lidge\n\nridge.fit&lt;-glmnet(X,y,alpha=0, lambda=seq(0.01,20,0.1)) ##ridge : alpha=0 \nplot(ridge.fit, label=TRUE)\nabline(h=0, col=\"grey\", lty=2)\n\n\n\n\n\n위 plot에서 xlabel이 L1 Norm아님. \\(\\sum |\\beta_j|\\)\n\n- 최소제곱추정량을 구할 때의 제약 조건\n\n릿지 \\(\\sum \\beta_j^2 \\leq t\\) : \\(L_2\\)-norm\n라쏘 \\(\\sum |\\beta_j| \\leq t\\) : \\(L_1\\)-norm\n그런데 릿지 라쏘의 중간 정도를 제약 조건을 주면 어쩔까? 싶어서 나온…\n\n\\[(1-\\alpha)\\sum \\beta_j^2 + \\alpha \\sum|\\beta_j| \\leq t\\]\n\n\\(\\alpha=0\\)이라면 뒷 부분으 날라가니까 \\(Ridge\\)를 쓴다는 것이고, \\(\\alpha=1\\)이라면 앞부분 날라가서 \\(Rasso\\)를 쓴다는 뜻\n\n- 람다를 크게하면서 MSE값이 작아지려면?\nLoocv\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=length(y))\n\nWarning message:\n“Option grouped=FALSE enforced in cv.glmnet, since &lt; 3 observations per fold”\n\n\n\ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = length(y), alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure      SE Nonzero\nmin 0.4558    82  0.2020 0.05006      10\n1se 1.8399    67  0.2465 0.07432      10\n\n\n\ncross validatiom\nMSE기준으로 람다가 0.4558일때 MSE가 가장 작음\n\n\nplot(cv.fit)\n\n\n\n\n\n가장 작아지는 값 log(0.4558)=-0.7857\nlog(1.8399)=0.6097\n\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=10) \ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = 10, alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure      SE Nonzero\nmin 0.5002    81  0.2169 0.05967      10\n1se 2.2161    65  0.2764 0.10178      10\n\n\n\nplot(cv.fit)\n\n\n\n\n\nlam&lt;-cv.fit$lambda.min;lam\n\n0.500186412336802\n\n\n\nlog(lam)\n\n-0.692774425368192\n\n\n- \\(\\hat \\beta(\\lambda)\\)의 추정량\n\npredict(ridge.fit,type=\"coefficients\",s=lam)\n\n11 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  8.350503e-17\ncyl         -1.111861e-01\ndisp        -1.093455e-01\nhp          -1.307062e-01\ndrat         9.342524e-02\nwt          -1.954335e-01\nqsec         4.746741e-02\nvs           6.573455e-02\nam           1.317846e-01\ngear         6.643998e-02\ncarb        -1.434548e-01\n\n\n\n옵션 타입을 ‘coefficients’ 주어야 \\(\\hat \\beta(\\lambda)\\)의 추정량을 구할 수 있다.\n옵션 타입을 ’response’를 쓰게 되면 \\(\\hat y\\)값을 구할 수 있다.\n\n\npredict(ridge.fit,type=\"response\",s=lam)\n\nERROR: Error in predict.glmnet(ridge.fit, type = \"response\", s = lam): You need to supply a value for 'newx'\n\n\n\ns1: \\(\\hat \\beta (0.5)\\)\n\\(\\hat y = X \\hat \\beta (0.5)\\)"
  },
  {
    "objectID": "posts/11. 편의추정 실습.html#lasso",
    "href": "posts/11. 편의추정 실습.html#lasso",
    "title": "11. 편의추정 실습",
    "section": "Lasso",
    "text": "Lasso\n\nlasso.fit&lt;-glmnet(X,y,alpha=1, lambda=seq(0.01,20,0.1)) ##lasso : alpha=1 \nplot(lasso.fit, label=TRUE)\n\nWarning message in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n“collapsing to unique 'x' values”\n\n\n\n\n\n\n8번 보면 가다가 0이 됨. (릿지는 0으로 수렴하긴 했지만 아예 0은 아니였는데..)\n제약조건을 걸면 변수를 포기해버림 (9개 -&gt; 9개 -&gt; 4개 -&gt; 3개 -&gt; 2개 -&gt;)\n\n\ncv.lasso.fit&lt;-cv.glmnet(X,y,alpha=1,nfolds=10) \ncv.lasso.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = 10, alpha = 1) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index Measure      SE Nonzero\nmin 0.1005    24  0.2252 0.05178       5\n1se 0.2322    15  0.2697 0.07727       3\n\n\n\nplot(cv.lasso.fit)"
  },
  {
    "objectID": "posts/12. 로지스틱 회귀분석.html",
    "href": "posts/12. 로지스틱 회귀분석.html",
    "title": "12. 로지스틱 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임\nlibrary(ISLR)"
  },
  {
    "objectID": "posts/12. 로지스틱 회귀분석.html#predict",
    "href": "posts/12. 로지스틱 회귀분석.html#predict",
    "title": "12. 로지스틱 실습",
    "section": "Predict",
    "text": "Predict\n\npredict(glm.fits, data.frame(balance=2000)) #type='link' : default\n\n1: 0.346503247961301\n\n\n\n위의 값은 \\(log\\dfrac{p(x)}{1-p(x)}\\)값\n\n\npredict(glm.fits, data.frame(balance=2000), type='response') \n\n1: 0.585769369615381\n\n\n\ntype = 'response' 해야 \\(p(x)\\)값을 구할 수 있음.\n\n\ny_2000 &lt;- predict(glm.fits, newdata = data.frame(balance=2000), type='response');y_2000\n\n1: 0.585769369615381\n\n\n\n$P(Default = “Yes” | Balance = 2000) = 0.585769369615381 $\n\n\ny_1000 &lt;- predict(glm.fits, newdata = data.frame(balance=1000), type='response');y_1000\n\n1: 0.00575214508582045\n\n\n\npar(mfrow=c(1,1))\nplot(as.numeric(default)-1 ~ balance,data = Default, \n     ylab = \"Probability of Default\" ,  \n     pch=16, cex=0.8, type='n')\n\ncurve(predict(glm.fits,data.frame(balance=x), type = \"response\"), \n       add = TRUE, col = \"dodgerblue\", lwd=2)\n\nlines(c(0, 2000),c(y_2000, y_2000), lty=2) \nlines(c(2000, 2000),c(0, y_2000), lty=2) \npoints(2000, y_2000, col='red', cex=1, pch=16)\n\nlines(c(0, 1000),c(y_1000, y_1000), lty=2) \nlines(c(1000, 1000),c(0, y_1000), lty=2) \npoints(1000, y_1000, col='red', cex=1, pch=16)"
  },
  {
    "objectID": "posts/12. 로지스틱 회귀분석.html#confidence-interval",
    "href": "posts/12. 로지스틱 회귀분석.html#confidence-interval",
    "title": "12. 로지스틱 실습",
    "section": "Confidence Interval",
    "text": "Confidence Interval\n\nsummary(glm.fits)\n\n\nCall:\nglm(formula = default ~ balance, family = binomial, data = Default)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2697  -0.1465  -0.0589  -0.0221   3.7589  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.065e+01  3.612e-01  -29.49   &lt;2e-16 ***\nbalance      5.499e-03  2.204e-04   24.95   &lt;2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2920.6  on 9999  degrees of freedom\nResidual deviance: 1596.5  on 9998  degrees of freedom\nAIC: 1600.5\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n95% CI : \\(\\hat \\beta_1 \\pm z_{0.025} s.e(\\hat \\beta_1)\\)\n\n- 방법1\n\nconfint(glm.fits, level = 0.95)\n\nWaiting for profiling to be done...\n\n\n\n\nA matrix: 2 × 2 of type dbl\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n(Intercept)\n-11.383288936\n-9.966565064\n\n\nbalance\n0.005078926\n0.005943365\n\n\n\n\n\n- 방법2\n\nsummary(glm.fits)$coef\n\n\nA matrix: 2 × 4 of type dbl\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n-10.651330614\n0.3611573721\n-29.49221\n3.623124e-191\n\n\nbalance\n0.005498917\n0.0002203702\n24.95309\n1.976602e-137\n\n\n\n\n\n\nsummary(glm.fits)$coef[2,1] + qnorm(0.975)*summary(glm.fits)$coef[2,2]\n\n0.00593083451913757\n\n\n\nsummary(glm.fits)$coef[2,1] - qnorm(0.975)*summary(glm.fits)$coef[2,2]\n\n0.00506699934268553"
  },
  {
    "objectID": "posts/12. 로지스틱 회귀분석.html#classification",
    "href": "posts/12. 로지스틱 회귀분석.html#classification",
    "title": "12. 로지스틱 실습",
    "section": "Classification",
    "text": "Classification\n\n로지스틱은 분류모형에 해당\n\\(P(y=1|x) &gt; 0.5 \\rightarrow\\) Default \\(\\hat y=1\\)\n\\(P(y=1|x) \\leq 0.5 \\rightarrow\\) Default \\(\\hat y=0\\)\n로 바꾸면 classification이 된다. 그럼 저렇게 0.5와 같은 기준값 (\\(cut-off\\))를 뭘로 정할까?\n\n\nfitted.default.prob &lt;- predict(glm.fits, type='response') \nhead(fitted.default.prob)\n\n10.0013056796742163320.0021125949135847530.0085947405390064540.00043443681930493650.0017769573781442560.00370415282222879\n\n\n\nclass.default &lt;- ifelse(fitted.default.prob &gt; 0.5, 'Yes', \"No\") \nhead(class.default)\n\n1'No'2'No'3'No'4'No'5'No'6'No'\n\n\n\ntable(Default$default, class.default)\n\n     class.default\n        No  Yes\n  No  9625   42\n  Yes  233  100\n\n\n\n왼쪽이 실제!\n정분류율은 0.9725\n\n\nclass.default &lt;- ifelse(fitted.default.prob &gt; 0.3, 'Yes', \"No\") \nhead(class.default)\n\n1'No'2'No'3'No'4'No'5'No'6'No'\n\n\n\ntable(Default$default, class.default)\n\n     class.default\n        No  Yes\n  No  9520  147\n  Yes  166  167"
  },
  {
    "objectID": "posts/AS3.html",
    "href": "posts/AS3.html",
    "title": "AS HW3",
    "section": "",
    "text": "library(ggplot2)"
  },
  {
    "objectID": "posts/AS3.html#section",
    "href": "posts/AS3.html#section",
    "title": "AS HW3",
    "section": "(1)",
    "text": "(1)\n\\(x_3\\)를 설명변수로 하고 \\(y\\)를 반응변수로 하여 회귀직선을 적합시키고, 이상치가 있는가, 어떤 것이 영향을 크게 주는 측정값인가를 판정하시오.\n\nmodel1 &lt;- lm(y~x3,dt)\nsummary(model1)\n\n\nCall:\nlm(formula = y ~ x3, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.453 -1.727 -0.042  1.068  6.396 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13.20655    3.15279   4.189 0.001061 ** \nx3           0.28767    0.06774   4.247 0.000953 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.692 on 13 degrees of freedom\nMultiple R-squared:  0.5811,    Adjusted R-squared:  0.5489 \nF-statistic: 18.04 on 1 and 13 DF,  p-value: 0.0009527\n\n\n\nplot(y~x3, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model1, col='steelblue', lwd=2)\n\n\n\n\n\nleverage\n\nHmatrix\n\n\nX = cbind(rep(1, nrow(dt)), dt$x3)\nH = X %*% solve(t(X) %*% X) %*% t(X)\ndiag(H)\n\n\n0.1489406600827220.1518527897357980.1013336709715540.2583354435722130.1113362032582090.08006246307081940.1134886469148310.08512703638051840.2411158943192370.09259728201232380.06689457246560310.1134886469148310.1518527897357980.1032328859626910.180341014602853\n\n\n\nhatvalues 함수\n\n\nwhich.max(hatvalues(model1))\nhatvalues(model1)[which.max(hatvalues(model1))]\n\n4: 4\n\n\n4: 0.258335443572212\n\n\n\n2*(1+1)/nrow(dt)\n\n0.266666666666667\n\n\n\n\\(h_{4} &lt; 2\\bar h\\)이므로 leverage포인트로 고려할 점은 없다.\n\n\n\n이상치\n\n잔차\n\n\nresidual &lt;- model1$residuals\nhead(residual)\n\n1-0.98725415717059926.396387271039083-0.13792521313412640.73875242677471353.1497425508567661.41006161897527\n\n\n\nhist(residual)\n\n\n\n\n\n내적표준화된 잔차\n\n\ns_residual &lt;- rstandard(model1)\nhead(s_residual)\n\n1-0.39751713781480122.579918210950283-0.054044466675073340.31864122629465351.241118131605860.546091891647763\n\n\n\nhist(s_residual)\n\n\n\n\n\ns_residual&gt;2\n\n1FALSE2TRUE3FALSE4FALSE5FALSE6FALSE7FALSE8FALSE9FALSE10FALSE11FALSE12FALSE13FALSE14FALSE15FALSE\n\n\n\n외적표준화된 잔차\n\n\ns_residual_i &lt;- rstudent(model1)\nhead(s_residual_i)\n\n1-0.38426469353097123.54825007062973-0.051930078113902540.30734314173941151.2700432930987360.530791544243437\n\n\n\nhist(s_residual_i)\n\n\n\n\n\nwhich.max(s_residual_i)\ns_residual_i[which.max(s_residual_i)]\n\n2: 2\n\n\n2: 3.5482500706297\n\n\n\nqt(0.975, 15-1-2)\n\n2.17881282966723\n\n\n\n\\(|r_i^*| \\geq t_{\\alpha/2}(12)\\) 이므로 2번째 관측값은 유의수준 0.05에서 이상점이다.\n\n\nplot(y~x3, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model1, col='steelblue', lwd=2)\n\n\n\n\n\n\n영향점\n\ninfluence(model1)\n\n\n    $hat\n        10.14894066008272120.15185278973579830.10133367097155440.25833544357221250.11133620325820960.080062463070819670.11348864691483180.085127036380518390.241115894319237100.0925972820123238110.0668945724656031120.113488646914831130.151852789735798140.103232885962691150.180341014602853\n\n    $coefficients\n        \n\nA matrix: 15 × 2 of type dbl\n\n\n\n(Intercept)\nx3\n\n\n\n\n1\n-0.45742205\n0.0083719544\n\n\n2\n-2.01160268\n0.0553827321\n\n\n3\n-0.04287451\n0.0007190015\n\n\n4\n0.56454256\n-0.0109721950\n\n\n5\n1.09199851\n-0.0188481935\n\n\n6\n-0.10046455\n0.0044636533\n\n\n7\n0.55803792\n-0.0168311648\n\n\n8\n-0.41549161\n0.0064019062\n\n\n9\n0.02268944\n-0.0005809430\n\n\n10\n0.15864320\n-0.0025647941\n\n\n11\n-0.18216030\n-0.0014000365\n\n\n12\n0.35441974\n-0.0106897699\n\n\n13\n-0.43915046\n0.0120905347\n\n\n14\n0.58437398\n-0.0185257601\n\n\n15\n0.32415484\n-0.0060864196\n\n\n\n\n\n    $sigma\n        12.7849574679319521.9574240107758432.8017246613426442.7910758028094552.6307997274935962.7697139968730272.6730854866991882.7539171937240892.80200497070363102.79662725993432112.60673085648245122.75075026012413132.76764179330928142.59682146853761152.79575824802225\n\n    $wt.res\n        1-0.98725415717059926.396387271039083-0.13792521313412640.73875242677471353.1497425508567661.410061618975277-2.740609436988278-1.713260741115899-0.0419515489153378100.57440702287499111-3.439267325061212-1.74060943698827131.3963872710390814-3.45294167299738150.588081370811178\n\n\n\n\n\ninfluence.measures(model1)\n\nInfluence measures of\n     lm(formula = y ~ x3, data = dt) :\n\n     dfb.1_   dfb.x3    dffit cov.r   cook.d    hat inf\n1  -0.14025  0.11948 -0.16075 1.346 1.38e-02 0.1489    \n2  -0.87752  1.12451  1.50138 0.330 5.96e-01 0.1519   *\n3  -0.01307  0.01020 -0.01744 1.305 1.65e-04 0.1013    \n4   0.17271 -0.15624  0.18139 1.558 1.77e-02 0.2583   *\n5   0.35443 -0.28474  0.44954 1.026 9.65e-02 0.1113    \n6  -0.03097  0.06405  0.15659 1.218 1.30e-02 0.0801    \n7   0.17826 -0.25025 -0.38961 1.096 7.48e-02 0.1135    \n8  -0.12883  0.09239 -0.19840 1.197 2.06e-02 0.0851    \n9   0.00691 -0.00824 -0.00969 1.546 5.08e-05 0.2411   *\n10  0.04844 -0.03645  0.06888 1.283 2.56e-03 0.0926    \n11 -0.05967 -0.02135 -0.36571 0.942 6.27e-02 0.0669    \n12  0.11002 -0.15445 -0.24046 1.230 3.02e-02 0.1135    \n13 -0.13549  0.17362  0.23181 1.317 2.84e-02 0.1519    \n14  0.19215 -0.28354 -0.47641 0.965 1.06e-01 0.1032    \n15  0.09900 -0.08652  0.10898 1.419 6.40e-03 0.1803    \n\n\n\nDFFITS\n\n\ndffits(model1) \n\n1-0.16075230135556121.501377795309863-0.017437998214039240.18138934044736250.44953963128077560.1565882963437357-0.3896065128912718-0.1984017731995239-0.00968760810666192100.068878582081802811-0.3657080048141812-0.240459576775504130.23181212636019314-0.476405218577437150.108981268211392\n\n\n\nwhich(abs(dffits(model1)) &gt; 2*sqrt(2/(15-2)))\n\n2: 2\n\n\n\nCook’s Distance\n\n\ncooks.distance(model1)\n\n10.013827228821435420.59584516209056730.00016467504137607840.017682774327940750.096492856748494160.012976933150712370.074827585188587680.020595695931365295.08340304142156e-05100.00255988931413331110.0626967366405896120.0301835348837091130.0283972104088968140.105589522000774150.00640451964826487\n\n\n\nqf(0.5,2,15-2)\n\n0.731454594627164\n\n\n\nwhich(cooks.distance(model1) &gt;qf(0.5,2,15-2))\n\n\n\n\n없다\n\nCOVRATIO\n\n\ncovratio(model1)\n\n11.3456795499794620.32953051274079631.3053608052344741.5577803759837851.0262209865048961.217891472927371.096463847912881.1969332827362191.54641970673037101.28341023214919110.94206589709893121.22955391198272131.31702952017448140.965419184972857151.41903295690633\n\n\n\nwhich(abs(covratio(model1)-1) &gt; 3*(1+1)/15)\n\n2244991515\n\n\n\nsummary(influence.measures(model1))\n\nPotentially influential observations of\n     lm(formula = y ~ x3, data = dt) :\n\n  dfb.1_ dfb.x3  dffit   cov.r   cook.d hat  \n2 -0.88   1.12_*  1.50_*  0.33_*  0.60   0.15\n4  0.17  -0.16    0.18    1.56_*  0.02   0.26\n9  0.01  -0.01   -0.01    1.55_*  0.00   0.24\n\n\n\n영향점은 2번째 관측값이다.\n\n\n## 2제거 전후\nplot(y~x3, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"2번 제거\")\nabline(model1, col='steelblue', lwd=2)\nabline(lm(y~x3, dt[-2,]), col='red', lwd=2)\ntext(dt[2,], pos=2, \"2\")\nlegend('topright', legend=c(\"full\", \"del(2)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 4제거 전후\nplot(y~x3, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"4번 제거\")\nabline(model1, col='steelblue', lwd=2)\nabline(lm(y~x3, dt[-4,]), col='red', lwd=2)\ntext(dt[-4,], pos=2, \"2\")\nlegend('topright', legend=c(\"full\", \"del(4)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 9제거 전후\nplot(y~x3, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"9번 제거\")\nabline(model1, col='steelblue', lwd=2)\nabline(lm(y~x3, dt[-9,]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(9)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 4,9제거 전후\nplot(y~x3, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"4,9번 제거\")\nabline(model1, col='steelblue', lwd=2)\nabline(lm(y~x3, dt[-c(4,9),]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(4,9)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 2,4,9제거 전후\nplot(y~x3, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"2,4,9번 제거\")\nabline(model1, col='steelblue', lwd=2)\nabline(lm(y~x3, dt[-c(2,4,9),]), col='red', lwd=2)\nlegend('topright', legend=c(\"full\", \"del(2,4,9)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n\n정규성\n\nshapiro.test(model1$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model1$residuals\nW = 0.93332, p-value = 0.3057\n\n\n\n\n회귀진단 그림\n\n\npar(mfrow = c(2, 2))\nplot(model1, pch=16)"
  },
  {
    "objectID": "posts/AS3.html#section-1",
    "href": "posts/AS3.html#section-1",
    "title": "AS HW3",
    "section": "(2)",
    "text": "(2)\n\\(x_1, x_2, x_3\\) 를 모두 사용하여 \\(y\\)에 대한 중회귀모형을 적합시키고, 이상치의 존재유무와 영향을 크게 주는 측정값이 어떤 것인가를 판정하시오.\n\nmodel2 &lt;- lm(y~x1+x2+x3,dt)\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0723 -1.0869 -0.0208  0.9365  3.8745 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -15.78431   14.50462  -1.088   0.2998  \nx1            0.15684    0.11331   1.384   0.1937  \nx2            0.19620    0.10215   1.921   0.0811 .\nx3            0.12948    0.09083   1.425   0.1818  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.331 on 11 degrees of freedom\nMultiple R-squared:  0.7342,    Adjusted R-squared:  0.6617 \nF-statistic: 10.13 on 3 and 11 DF,  p-value: 0.001699\n\n\n\nleverage\n\nwhich.max(hatvalues(model2))\nhatvalues(model2)[which.max(hatvalues(model2))]\n\n6: 6\n\n\n6: 0.467111162145579\n\n\n\n2*(3+1)/nrow(dt)\n\n0.533333333333333\n\n\n\nleverage 고려할 포인트는 업사.\n\n\n\n이상치\n\n잔차\n\n\nresidual2 &lt;- model2$residuals\nhead(residual2)\nhist(residual2)\n\n10.34521486034817323.874522533604423-2.644790869307994-0.36935915630047651.0727144472261660.800293089055777\n\n\n\n\n\n\n내적표준화된 잔차\n\n\ns_residual2 &lt;- rstandard(model2)\nhead(s_residual2)\ns_residual&gt;2\nhist(s_residual2)\n\n10.16869378987261922.040409611918123-1.368462221789944-0.19203860475361450.52746298523962260.470252444990339\n\n\n1FALSE2TRUE3FALSE4FALSE5FALSE6FALSE7FALSE8FALSE9FALSE10FALSE11FALSE12FALSE13FALSE14FALSE15FALSE\n\n\n\n\n\n\n외적표준화된 잔차\n\n\ns_residual_i2 &lt;- rstudent(model2)\nhead(s_residual_i2)\nhist(s_residual_i2)\n\n10.16105167729138222.467704711556573-1.432390344210954-0.18340933439695350.50939946656803660.452944085994544\n\n\n\n\n\n\nwhich.max(s_residual_i2)\ns_residual_i[which.max(s_residual_i2)]\n\n2: 2\n\n\n2: 3.5482500706297\n\n\n\nqt(0.975, 15-3-2)\n\n2.22813885198627\n\n\n\n\\(|r_i^*| \\geq t_{\\alpha/2}(10)\\) 이므로 2번째 관측값은 유의수준 0.05에서 이상점이다.\n\n\n\n영향점\n\ninfluence(model2)\n\n\n    $hat\n        10.22948448298487820.33655904824490130.31274538741397440.3193540418355750.23899957827286360.46711116214557970.17667486836293380.26219942503354490.414083032167575100.19055339953678110.0731668365931882120.174623444618773130.197413593761242140.241923572696132150.365108126332067\n\n    $coefficients\n        \n\nA matrix: 15 × 4 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\n\n\n\n\n1\n0.95354693\n0.0003764930\n-5.561428e-03\n-0.0014501180\n\n\n2\n-13.93952031\n0.0672978312\n8.377466e-02\n-0.0248399264\n\n\n3\n4.69552573\n-0.0704991936\n-3.644734e-02\n0.0705780525\n\n\n4\n0.50943934\n0.0007684142\n-5.873357e-03\n0.0076533361\n\n\n5\n-2.27767961\n0.0105499531\n1.862708e-02\n-0.0200833113\n\n\n6\n-4.18168476\n-0.0332209276\n3.130321e-02\n0.0117532167\n\n\n7\n-2.20897093\n0.0037053698\n1.765648e-02\n-0.0175478830\n\n\n8\n-0.01539851\n0.0005690967\n2.732094e-05\n-0.0002319326\n\n\n9\n-0.51557236\n-0.0263081084\n9.369868e-03\n-0.0028197702\n\n\n10\n0.12758711\n0.0019405994\n-7.974601e-04\n-0.0013464759\n\n\n11\n-1.76717565\n-0.0009954571\n1.149237e-02\n-0.0048224698\n\n\n12\n-0.86864768\n-0.0001014964\n7.037690e-03\n-0.0060239702\n\n\n13\n2.27737859\n-0.0219351895\n-2.102859e-02\n0.0439849174\n\n\n14\n3.66063758\n0.0653541960\n-2.631196e-02\n-0.0454527846\n\n\n15\n13.67495355\n-0.0124352729\n-8.212831e-02\n-0.0017947153\n\n\n\n\n\n    $sigma\n        12.4419318286904321.9276308271956632.2272610408618342.4409944937785652.4139773737757262.4203945219358172.3971591950995182.4450846369875392.42314403118888102.44479643924596112.22712958167362122.43788055286261132.26047895469207142.20980300839363152.16577831202649\n\n    $wt.res\n        10.34521486034817323.874522533604423-2.644790869307994-0.36935915630047651.0727144472261660.8002930890557777-1.382442750794928-0.02084105562919359-0.79131150566298100.10901227531297811-3.0722615270535212-0.539287518287805132.6405170216773914-2.88148775547281152.85950791128479\n\n\n\n\n\ninfluence.measures(model2)\n\nInfluence measures of\n     lm(formula = y ~ x1 + x2 + x3, data = dt) :\n\n     dfb.1_    dfb.x1    dfb.x2   dfb.x3    dffit cov.r   cook.d    hat inf\n1   0.06276  0.003172 -0.051979 -0.01524  0.08789 1.881 2.12e-03 0.2295    \n2  -1.16230  0.718336  0.991882 -0.33074  1.75761 0.329 5.28e-01 0.3366   *\n3   0.33885 -0.651274 -0.373479  0.81332 -0.96627 1.010 2.13e-01 0.3127    \n4   0.03354  0.006477 -0.054915  0.08047 -0.12563 2.122 4.33e-03 0.3194   *\n5  -0.15165  0.089922  0.176110 -0.21353  0.28547 1.737 2.18e-02 0.2390    \n6  -0.27769 -0.282407  0.295171  0.12463  0.42407 2.533 4.85e-02 0.4671   *\n7  -0.14811  0.031804  0.168104 -0.18788 -0.29442 1.518 2.29e-02 0.1767    \n8  -0.00101  0.004789  0.000255 -0.00243 -0.00592 1.984 9.62e-06 0.2622    \n9  -0.03420 -0.223389  0.088252 -0.02987 -0.35865 2.325 3.47e-02 0.4141   *\n10  0.00839  0.016332 -0.007445 -0.01414  0.02405 1.807 1.59e-04 0.1906    \n11 -0.12753 -0.009197  0.117770 -0.05558 -0.40259 0.748 3.70e-02 0.0732    \n12 -0.05727 -0.000857  0.065885 -0.06342 -0.11200 1.732 3.43e-03 0.1746    \n13  0.16193 -0.199660 -0.212315  0.49942  0.64667 0.973 9.83e-02 0.1974    \n14  0.26625  0.608514 -0.271751 -0.52792 -0.84604 0.860 1.61e-01 0.2419    \n15  1.01486 -0.118139 -0.865466 -0.02127  1.25657 0.874 3.41e-01 0.3651   *\n\n\n\nDFFITS\n\n\ndffits(model2) \n\n10.087892379093318521.757610700782963-0.9662689176736724-0.12563105544856850.28547271433999960.4240688812545357-0.2944193130156078-0.005915653038725999-0.358654177245365100.024046553927341211-0.40259490099549812-0.111997181258085130.64667374817515514-0.846036927571321151.25657461898781\n\n\n\nwhich(abs(dffits(model2)) &gt; 2*sqrt(2/(15-3-1)))\n\n22331515\n\n\n\nCook’s distance\n\n\ncooks.distance(model2)\n\n10.0021188984102373920.52799976057204930.21304869765318640.0043258179635064750.021844203945393860.048460261704198570.022912209221435389.62351672584315e-0690.0347416842517463100.000158976058928977110.0369801047117493120.00342909608500354130.0982906080447196140.160777911639787150.340678864434841\n\n\n\nqf(0.5,3+1,15-3-1)\n\n0.893156955577709\n\n\n\nwhich(cooks.distance(model2) &gt;qf(0.5,4,11))\n\n\n\n\n없다\n\ncovratio\n\n\ncovratio(model2)\n\n11.8805693554821220.3292995127988631.009842768207742.1223430695868251.7365333282902462.5331162822626771.5177704688720481.9843334119922392.32487827053021101.80699058897987110.748453434660152121.73240614043161130.973451691520289140.859642643137438150.873805339356644\n\n\n\nwhich(abs(covratio(model2)-1) &gt; 3*(3+1)/15)\n\n11446688991010\n\n\n\n\n정규성\n\nshapiro.test(model2$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  model2$residuals\nW = 0.95818, p-value = 0.6608\n\n\n\n\n회귀진단 그림\n\npar(mfrow = c(2, 2))\nplot(model2, pch=16)"
  },
  {
    "objectID": "posts/AS3.html#section-2",
    "href": "posts/AS3.html#section-2",
    "title": "AS HW3",
    "section": "(1)",
    "text": "(1)\n중회귀모형\n\\(y_i = β_0 + β_1x_{i1} + β_2x_{i2} + β_3x_{i3} + ϵ_i\\)\n\\(i = 1, 2, · · · , 21\\)\n$ϵ_i \\(∼\\)N(0, σ^2_ϵ)$\n이 데이터간의 관계를 설명하는 데 충분하다고 가정하고 다음의 물음에 답하시오. 유의수준 α = 0.05를 사용하여라.\n\npairs(dt2, pch=16)\ncor(dt2)\n\n\nA matrix: 4 × 4 of type dbl\n\n\n\nx1\nx2\nx3\ny\n\n\n\n\nx1\n1.0000000\n0.7818523\n0.4885669\n0.9196635\n\n\nx2\n0.7818523\n1.0000000\n0.3078454\n0.8755044\n\n\nx3\n0.4885669\n0.3078454\n1.0000000\n0.3776617\n\n\ny\n0.9196635\n0.8755044\n0.3776617\n1.0000000\n\n\n\n\n\n\n\n\n\ny와 x1의 상관관계가 높아보인다. y와 x2도 높아보이낟.\nx1과 x2도 높다!!\n\n\nm &lt;- lm(y~., dt2) ##FM\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4829 -1.7449 -0.4688  2.3497  5.7224 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -42.5557    12.7798  -3.330 0.003965 ** \nx1            0.7107     0.1416   5.018 0.000106 ***\nx2            1.2610     0.3768   3.347 0.003823 ** \nx3           -0.1092     0.1632  -0.669 0.512537    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.289 on 17 degrees of freedom\nMultiple R-squared:  0.9111,    Adjusted R-squared:  0.8954 \nF-statistic: 58.08 on 3 and 17 DF,  p-value: 3.83e-09\n\n\n\n(a)\nadd1/drop1 함수를 이용하여, 부분F검정통계량 값을 통한 후진제거법을 시행하여 가장 적절한 회귀모형을 구하여라.\n\ndrop1(m,test=\"F\")\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n183.9537\n53.57340\nNA\nNA\n\n\nx1\n1\n272.464536\n456.4183\n70.65663\n25.1796878\n0.0001055439\n\n\nx2\n1\n121.206721\n305.1604\n62.20262\n11.2012647\n0.0038231136\n\n\nx3\n1\n4.841619\n188.7953\n52.11896\n0.4474361\n0.5125370733\n\n\n\n\n\n\nqf(0.95,1,21-3-1)\n\n4.45132177246813\n\n\n\\(F_L &lt; F_{\\alpha B}\\) 이므로 설명변수 x3제거\n\nm1 &lt;- update(m, ~ . -x3)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5290 -1.7505  0.1894  2.1156  5.6588 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.3588     5.1383  -9.801 1.22e-08 ***\nx1            0.6712     0.1267   5.298 4.90e-05 ***\nx2            1.2954     0.3675   3.525  0.00242 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.239 on 18 degrees of freedom\nMultiple R-squared:  0.9088,    Adjusted R-squared:  0.8986 \nF-statistic: 89.64 on 2 and 18 DF,  p-value: 4.382e-10\n\n\n\npvalue의 값도 유의하고 \\(R^2\\)도 약 90%의 설명력을 가진다. 각각의 회귀계수도 유의하다.\n\\(y=-50.3588 + 0.6712 x_1 + 1.2954 x_2\\)\n\n\ndrop1(m1, test = \"F\")\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n188.7953\n52.11896\nNA\nNA\n\n\nx1\n1\n294.3553\n483.1506\n69.85193\n28.06423\n0.0000489797\n\n\nx2\n1\n130.3208\n319.1161\n61.14168\n12.42496\n0.0024191459\n\n\n\n\n\n\nqf(0.95,1,21-2-1)\n\n4.41387341917057\n\n\n\nF-value값이 작은 x2를 제거하려고 봤더니 pr값이 0.002419로 유의하므로 제거하지 않는다.\n\n\n\n(b)\nadd1/drop1 함수를 이용하여, 부분F검정통계량 값을 통한 전진선택법을 시행하여 가장 적절한 회귀모형을 구하여라.\n\nm0 = lm(y ~ 1, data = dt2)\nadd1(m0, scope = y ~ x1 + x2 + x3, test = \"F\")\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n2069.2381\n98.39868\nNA\nNA\n\n\nx1\n1\n1750.1220\n319.1161\n61.14168\n104.201315\n3.774296e-09\n\n\nx2\n1\n1586.0875\n483.1506\n69.85193\n62.373224\n2.028017e-07\n\n\nx3\n1\n295.1321\n1774.1060\n97.16712\n3.160752\n9.143800e-02\n\n\n\n\n\n\nF값이 큰 x1을 추가하자. 심지어 유의하당\n\n\nqf(0.95,1,21-1-1)\n\n4.3807496923318\n\n\n\nm1 &lt;- update(m0, ~.+x1)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1, data = dt2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.2896  -1.1272  -0.0459   1.1166   8.8728 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -44.13202    6.10586  -7.228 7.31e-07 ***\nx1            1.02031    0.09995  10.208 3.77e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.098 on 19 degrees of freedom\nMultiple R-squared:  0.8458,    Adjusted R-squared:  0.8377 \nF-statistic: 104.2 on 1 and 19 DF,  p-value: 3.774e-09\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3,\n test = \"F\")\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n319.1161\n61.14168\nNA\nNA\n\n\nx2\n1\n130.32077\n188.7953\n52.11896\n12.4249569\n0.002419146\n\n\nx3\n1\n13.95567\n305.1604\n62.20262\n0.8231803\n0.376238733\n\n\n\n\n\n\nx2의 Fvalue가 크고 유의하므로 추가하자.\n\n\nm2 &lt;- update(m1, ~ . +x2)\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5290 -1.7505  0.1894  2.1156  5.6588 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.3588     5.1383  -9.801 1.22e-08 ***\nx1            0.6712     0.1267   5.298 4.90e-05 ***\nx2            1.2954     0.3675   3.525  0.00242 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.239 on 18 degrees of freedom\nMultiple R-squared:  0.9088,    Adjusted R-squared:  0.8986 \nF-statistic: 89.64 on 2 and 18 DF,  p-value: 4.382e-10\n\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3,\n test = \"F\") \n\n\nA anova: 2 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n188.7953\n52.11896\nNA\nNA\n\n\nx3\n1\n4.841619\n183.9537\n53.57340\n0.4474361\n0.5125371\n\n\n\n\n\n\nx3의 pr값도 유의수준 0.05에서 유의하지 않다. 모형에 포함될 수 없으므로 멈춘다. 최종모형은 x1과 x2를 포함한 모형이다.\n\n\n\n(c)\nadd1/drop1 함수를 이용하여, 부분F검정통계량 값을 통한 단계적 전진선택법을 시행하여 가장 적절한 회귀모형을 구하여라.\n\nm0 = lm(y ~ 1, data = dt2)\n\n\nadd1(m0,\n scope = y ~ x1 + x2 + x3,\n test = \"F\") \n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n2069.2381\n98.39868\nNA\nNA\n\n\nx1\n1\n1750.1220\n319.1161\n61.14168\n104.201315\n3.774296e-09\n\n\nx2\n1\n1586.0875\n483.1506\n69.85193\n62.373224\n2.028017e-07\n\n\nx3\n1\n295.1321\n1774.1060\n97.16712\n3.160752\n9.143800e-02\n\n\n\n\n\n\nFvalue가 크고 pr값이 유의하 x1을 추가하자.\n\n\nm1 &lt;- update(m0, ~ . +x1)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1, data = dt2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.2896  -1.1272  -0.0459   1.1166   8.8728 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -44.13202    6.10586  -7.228 7.31e-07 ***\nx1            1.02031    0.09995  10.208 3.77e-09 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 4.098 on 19 degrees of freedom\nMultiple R-squared:  0.8458,    Adjusted R-squared:  0.8377 \nF-statistic: 104.2 on 1 and 19 DF,  p-value: 3.774e-09\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3,\n test = \"F\") \n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n319.1161\n61.14168\nNA\nNA\n\n\nx2\n1\n130.32077\n188.7953\n52.11896\n12.4249569\n0.002419146\n\n\nx3\n1\n13.95567\n305.1604\n62.20262\n0.8231803\n0.376238733\n\n\n\n\n\n\nx2의 Fvalue가 더 크고 pr값이 유의하므로 x2선택\n\n\nm2 &lt;- update(m1, ~ . +x2)\n\n\nx1이 있는 모형에 x2를 추가함\n\n\ndrop1(m2, test = \"F\")\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n188.7953\n52.11896\nNA\nNA\n\n\nx1\n1\n294.3553\n483.1506\n69.85193\n28.06423\n0.0000489797\n\n\nx2\n1\n130.3208\n319.1161\n61.14168\n12.42496\n0.0024191459\n\n\n\n\n\n\nx1을 보자. 제거할까? 유의하므로 제거하지 말자.\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3,\n test = \"F\")\n\n\nA anova: 2 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n188.7953\n52.11896\nNA\nNA\n\n\nx3\n1\n4.841619\n183.9537\n53.57340\n0.4474361\n0.5125371\n\n\n\n\n\n\n유의수준 0.05에서는 유의하지 않으므로 x3를 추가하지 않는다.\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5290 -1.7505  0.1894  2.1156  5.6588 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -50.3588     5.1383  -9.801 1.22e-08 ***\nx1            0.6712     0.1267   5.298 4.90e-05 ***\nx2            1.2954     0.3675   3.525  0.00242 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 3.239 on 18 degrees of freedom\nMultiple R-squared:  0.9088,    Adjusted R-squared:  0.8986 \nF-statistic: 89.64 on 2 and 18 DF,  p-value: 4.382e-10"
  },
  {
    "objectID": "posts/AS3.html#section-3",
    "href": "posts/AS3.html#section-3",
    "title": "AS HW3",
    "section": "(2)",
    "text": "(2)\nleaps 패키지의 regsubsets 함수를 이용하여, 다음의 물음에 답하여라.\n\nlibrary(leaps)\n\n\n(a)\n전역탐색법을 사용하여, 가능한 모든 회귀모형에 대하여 \\(MSE_p, R^2_p, R^2_{adj,p}, C_p\\) 를 구하여라. (regsubsets 사용시, 옵션에서 nbest=1 대신에 적당한 숫자를 입력하면 모든 회귀모형에 대한 결과를 확인할 수 있다.)\n\nfit&lt;-regsubsets(y~., data=dt2, nbest=6,\nmethod='exhaustive',\n)\nsummary(fit)\nwith(summary(fit),\n     round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt2, nbest = 6, method = \"exhaustive\", \n    )\n3 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\n6 subsets of each size up to 3\nSelection Algorithm: exhaustive\n         x1  x2  x3 \n1  ( 1 ) \"*\" \" \" \" \"\n1  ( 2 ) \" \" \"*\" \" \"\n1  ( 3 ) \" \" \" \" \"*\"\n2  ( 1 ) \"*\" \"*\" \" \"\n2  ( 2 ) \"*\" \" \" \"*\"\n2  ( 3 ) \" \" \"*\" \"*\"\n3  ( 1 ) \"*\" \"*\" \"*\"\n\n\n\nA matrix: 7 × 9 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n1\n0\n0\n319.116\n0.846\n0.838\n12.491\n-33.168\n\n\n1\n1\n0\n1\n0\n483.151\n0.767\n0.754\n27.650\n-24.458\n\n\n1\n1\n0\n0\n1\n1774.106\n0.143\n0.098\n146.953\n2.857\n\n\n2\n1\n1\n1\n0\n188.795\n0.909\n0.899\n2.447\n-41.146\n\n\n2\n1\n1\n0\n1\n305.160\n0.853\n0.836\n13.201\n-31.062\n\n\n2\n1\n0\n1\n1\n456.418\n0.779\n0.755\n27.180\n-22.608\n\n\n3\n1\n1\n1\n1\n183.954\n0.911\n0.895\n4.000\n-38.647\n\n\n\n\n\n\n\n(b)\n\\(p = 1, p = 2, p = 3\\)에서 각각 가장 적절한 회귀모형을 구하여라. (기준 : \\(R^2_p\\))\np=1인 경우에 \\(R_2\\) 값이 가장 큰(0.846) 모형을 선택한다. 즉 \\(y=\\beta_0+\\beta_1x_1\\)\np=2인 경우에는 \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2\\)\np=3인 경우에는 \\(y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3\\)\n\nfit&lt;-regsubsets(y~., data=dt2, nbest=1,\nmethod='exhaustive',\n)\nsummary(fit)\nwith(summary(fit),\n     round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt2, nbest = 1, method = \"exhaustive\", \n    )\n3 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\n1 subsets of each size up to 3\nSelection Algorithm: exhaustive\n         x1  x2  x3 \n1  ( 1 ) \"*\" \" \" \" \"\n2  ( 1 ) \"*\" \"*\" \" \"\n3  ( 1 ) \"*\" \"*\" \"*\"\n\n\n\nA matrix: 3 × 9 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n1\n0\n0\n319.116\n0.846\n0.838\n12.491\n-33.168\n\n\n2\n1\n1\n1\n0\n188.795\n0.909\n0.899\n2.447\n-41.146\n\n\n3\n1\n1\n1\n1\n183.954\n0.911\n0.895\n4.000\n-38.647\n\n\n\n\n\n\n\n(c)\n위에서 구한 모형 중 가장 적절한 회귀모형을 선택하여라.\nadjr이 제일 높은 모형-&gt; y=x1+x2\nrss는 작은것이 제일 좋은데 3개를 선택한 모형은 원래 제일 작음.. x1 1개만 선택한 모형의 rss는 319이고 x1과 x2를 선택한 모형의 rss는 188로 급격히 감소함. x3까ㅣ 추가한것은 183으로 별차이가 없으므로 x1+x2를 선택한 모형을 고르는 것이 좋아보인다. bic값도 제일 낮음\n\n\n(d)\n\\(C_p\\)의 경우 \\(C_p ≤ p + 1\\) 이면 좋은 모형으로 판정하고, 이를 만족할 때 변수의 수가 가장 적은 모형을 선택하는 것이 좋다고 알려져 있다. \\(C_p\\) 를 이용했을 때, \\(p = 1, p = 2, p = 3\\)에서 각각 가장 좋은 모형을 선택하여라.\np=1일때는 x1선택한 모형이 제일작지만, cp&gt;2이므로 좋은 모형이 아니다.\np=2일때는 x1+x2\np=3일때는 x1+x2+x3\n\nfit&lt;-regsubsets(y~., data=dt2, nbest=6,\nmethod='exhaustive',\n)\nsummary(fit)\nwith(summary(fit),\n     round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt2, nbest = 6, method = \"exhaustive\", \n    )\n3 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\n6 subsets of each size up to 3\nSelection Algorithm: exhaustive\n         x1  x2  x3 \n1  ( 1 ) \"*\" \" \" \" \"\n1  ( 2 ) \" \" \"*\" \" \"\n1  ( 3 ) \" \" \" \" \"*\"\n2  ( 1 ) \"*\" \"*\" \" \"\n2  ( 2 ) \"*\" \" \" \"*\"\n2  ( 3 ) \" \" \"*\" \"*\"\n3  ( 1 ) \"*\" \"*\" \"*\"\n\n\n\nA matrix: 7 × 9 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n1\n0\n0\n319.116\n0.846\n0.838\n12.491\n-33.168\n\n\n1\n1\n0\n1\n0\n483.151\n0.767\n0.754\n27.650\n-24.458\n\n\n1\n1\n0\n0\n1\n1774.106\n0.143\n0.098\n146.953\n2.857\n\n\n2\n1\n1\n1\n0\n188.795\n0.909\n0.899\n2.447\n-41.146\n\n\n2\n1\n1\n0\n1\n305.160\n0.853\n0.836\n13.201\n-31.062\n\n\n2\n1\n0\n1\n1\n456.418\n0.779\n0.755\n27.180\n-22.608\n\n\n3\n1\n1\n1\n1\n183.954\n0.911\n0.895\n4.000\n-38.647\n\n\n\n\n\n\n\n(e)\n위에서 선택한 모형 중 \\(C_p\\)를 기준으로 가장 적절한 회귀모형을 선택하여라\np=2일때 x1+x2를 선택한 모형이 가장 적절하다. p=1인 경우에는 cp&lt;p+1이므로 좋은 모형이라고 할 수 없다."
  },
  {
    "objectID": "posts/AS3.html#section-4",
    "href": "posts/AS3.html#section-4",
    "title": "AS HW3",
    "section": "(3)",
    "text": "(3)\n일차 선형항(lenear terms)만으로는 충분하지 않다고 생각하고 이차다항회귀모형\n\\(y = β_0 + β_1x_1 + β_2x_2 + β_3x_3 + β_4x^2_1 + β_5x^2_2 + β_6x^2_3 + β_7x_1x_2 + β_8x_1x_3 + β_9x_2x_3 + ϵ\\)을 가정하였다. 다음 물음에 답하여라.\n\ndt2$x_1 &lt;- dt2$x1^2\ndt2$x_2 &lt;- dt2$x2^2\ndt2$x_3 &lt;- dt2$x3^2\ndt2$x1x2 &lt;- dt2$x1 * dt2$x2\ndt2$x1x3 &lt;- dt2$x1 * dt2$x1\ndt2$x2x3 &lt;- dt2$x2 * dt2$x3\nhead(dt2)\n\n\nA data.frame: 6 × 10\n\n\n\nx1\nx2\nx3\ny\nx_1\nx_2\nx_3\nx1x2\nx1x3\nx2x3\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n80\n27\n89\n42\n6400\n729\n7921\n2160\n6400\n2403\n\n\n2\n80\n27\n88\n37\n6400\n729\n7744\n2160\n6400\n2376\n\n\n3\n75\n25\n90\n37\n5625\n625\n8100\n1875\n5625\n2250\n\n\n4\n62\n24\n87\n28\n3844\n576\n7569\n1488\n3844\n2088\n\n\n5\n62\n22\n87\n18\n3844\n484\n7569\n1364\n3844\n1914\n\n\n6\n62\n23\n87\n18\n3844\n529\n7569\n1426\n3844\n2001\n\n\n\n\n\n\n(a)\n변수간 상관계수를 구하여라.\n\ncor(dt2)\n\n\nA matrix: 10 × 10 of type dbl\n\n\n\nx1\nx2\nx3\ny\nx_1\nx_2\nx_3\nx1x2\nx1x3\nx2x3\n\n\n\n\nx1\n1.0000000\n0.7818523\n0.4885669\n0.9196635\n0.9969709\n0.8002266\n0.4861833\n0.9452829\n0.9969709\n0.8221139\n\n\nx2\n0.7818523\n1.0000000\n0.3078454\n0.8755044\n0.7846889\n0.9982761\n0.3079752\n0.9377909\n0.7846889\n0.9542580\n\n\nx3\n0.4885669\n0.3078454\n1.0000000\n0.3776617\n0.4524089\n0.3130881\n0.9991718\n0.4004805\n0.4524089\n0.5773637\n\n\ny\n0.9196635\n0.8755044\n0.3776617\n1.0000000\n0.9251379\n0.8933751\n0.3735385\n0.9588371\n0.9251379\n0.8672805\n\n\nx_1\n0.9969709\n0.7846889\n0.4524089\n0.9251379\n1.0000000\n0.8053207\n0.4499860\n0.9500453\n1.0000000\n0.8131868\n\n\nx_2\n0.8002266\n0.9982761\n0.3130881\n0.8933751\n0.8053207\n1.0000000\n0.3129003\n0.9498899\n0.8053207\n0.9544366\n\n\nx_3\n0.4861833\n0.3079752\n0.9991718\n0.3735385\n0.4499860\n0.3129003\n1.0000000\n0.3990206\n0.4499860\n0.5777549\n\n\nx1x2\n0.9452829\n0.9377909\n0.4004805\n0.9588371\n0.9500453\n0.9498899\n0.3990206\n1.0000000\n0.9500453\n0.9290098\n\n\nx1x3\n0.9969709\n0.7846889\n0.4524089\n0.9251379\n1.0000000\n0.8053207\n0.4499860\n0.9500453\n1.0000000\n0.8131868\n\n\nx2x3\n0.8221139\n0.9542580\n0.5773637\n0.8672805\n0.8131868\n0.9544366\n0.5777549\n0.9290098\n0.8131868\n1.0000000\n\n\n\n\n\n\n\n(b)\n설명변수 9개를 모두 사용한 완전모형(full model)에 대한 분산분석표를 작성하고, α = 0.05에서 선형회귀모형의 유의성을 검정하시오.\n\nmodel3 &lt;- lm(y~.,data=dt2)\nsummary(model3)\n\n\nCall:\nlm(formula = y ~ ., data = dt2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.265 -1.249  0.005  1.177  5.382 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -95.454579 171.941017  -0.555    0.589\nx1            1.918799   1.503723   1.276    0.226\nx2           -4.101841  10.326442  -0.397    0.698\nx3            1.727460   3.273516   0.528    0.607\nx_1          -0.040497   0.026368  -1.536    0.151\nx_2          -0.007079   0.265375  -0.027    0.979\nx_3          -0.004630   0.023903  -0.194    0.850\nx1x2          0.172153   0.128263   1.342    0.204\nx1x3                NA         NA      NA       NA\nx2x3         -0.054589   0.112622  -0.485    0.637\n\nResidual standard error: 3.037 on 12 degrees of freedom\nMultiple R-squared:  0.9465,    Adjusted R-squared:  0.9109 \nF-statistic: 26.55 on 8 and 12 DF,  p-value: 1.706e-06\n\n\n\nanova(model3)\n\n\nA anova: 9 × 5\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nx1\n1\n1750.121989\n1750.121989\n189.7977744\n1.023932e-08\n\n\nx2\n1\n130.320772\n130.320772\n14.1330676\n2.723908e-03\n\n\nx3\n1\n4.841619\n4.841619\n0.5250654\n4.825789e-01\n\n\nx_1\n1\n8.498097\n8.498097\n0.9216043\n3.559986e-01\n\n\nx_2\n1\n39.533776\n39.533776\n4.2873712\n6.062395e-02\n\n\nx_3\n1\n5.206534\n5.206534\n0.5646399\n4.668786e-01\n\n\nx1x2\n1\n17.897053\n17.897053\n1.9409052\n1.888401e-01\n\n\nx2x3\n1\n2.166462\n2.166462\n0.2349492\n6.366054e-01\n\n\nResiduals\n12\n110.651792\n9.220983\nNA\nNA\n\n\n\n\n\n\nnull_model &lt;- lm(y~1, data=dt2)  #H0\nmodel3&lt;- lm(y~., data=dt2)  #H1\n\nanova(null_model, model3)\n\n\nA anova: 2 × 6\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n20\n2069.2381\nNA\nNA\nNA\nNA\n\n\n2\n12\n110.6518\n8\n1958.586\n26.55067\n1.706475e-06\n\n\n\n\n\n\nF=26.55067\n\n\n\n(c)\n상관계수들을 보고 변수를 하나만 선택할 때에는 어떤 변수가 선택될 것인지를 말하시오.\nx1x3와 y의 상관계수가 0.9588371로 가장 높으므로 선택\n\n\n(d)\n상관계수들을 보고 변수를 하나만 제거시키려고 할 때, 어떤 변수가 제거될 것인가를 말할 수 있는가?\nx3^2와 y의 상관계수가 0.3735385로 가장 작으므로 제거\n\n\n(e)\nadd1/drop1 함수를 이용하여, \\(MSE_p\\) 기준으로 단계적 전진선택법을 시행하여 가장 적절한 회귀모형을 구하여라.\n\nmodel_step = step(\n m0,\n scope =y ~ x1 + x2 + x3+ I(x1^2)+I(x2^2)+I(x3^2)+I(x1*x2)+I(x1*x3)+I(x2*x3),\n direction = \"both\")\nsummary(model_step)\n\nStart:  AIC=98.4\ny ~ 1\n\n             Df Sum of Sq     RSS    AIC\n+ I(x1 * x2)  1   1902.39  166.85 47.523\n+ I(x1^2)     1   1771.02  298.22 59.719\n+ x1          1   1750.12  319.12 61.142\n+ I(x2^2)     1   1651.50  417.74 66.797\n+ x2          1   1586.09  483.15 69.852\n+ I(x1 * x3)  1   1563.93  505.31 70.794\n+ I(x2 * x3)  1   1556.43  512.81 71.103\n+ x3          1    295.13 1774.11 97.167\n+ I(x3^2)     1    288.72 1780.52 97.243\n&lt;none&gt;                    2069.24 98.399\n\nStep:  AIC=47.52\ny ~ I(x1 * x2)\n\n             Df Sum of Sq     RSS    AIC\n&lt;none&gt;                     166.85 47.523\n+ x2          1      9.63  157.22 48.275\n+ I(x2 * x3)  1      8.34  158.51 48.447\n+ I(x2^2)     1      6.42  160.42 48.699\n+ I(x1^2)     1      4.28  162.56 48.977\n+ x1          1      3.43  163.41 49.087\n+ I(x1 * x3)  1      0.61  166.23 49.446\n+ I(x3^2)     1      0.20  166.64 49.498\n+ x3          1      0.10  166.75 49.511\n- I(x1 * x2)  1   1902.39 2069.24 98.399\n\n\n\nCall:\nlm(formula = y ~ I(x1 * x2), data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1481 -2.3759 -0.6042  2.6123  5.6241 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -15.29308    2.32149  -6.588 2.64e-06 ***\nI(x1 * x2)    0.02532    0.00172  14.719 7.68e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.963 on 19 degrees of freedom\nMultiple R-squared:  0.9194,    Adjusted R-squared:  0.9151 \nF-statistic: 216.6 on 1 and 19 DF,  p-value: 7.675e-12\n\n\n\nAIC로 확인해보니까 \\(y\\)~\\(I(x1*x2)\\)가 최종모형\n\n\nm0 = lm(y ~ 1, data = dt2)\nadd1(m0,\n scope = y ~ x1 + x2 + x3+ I(x1^2)+I(x2^2)+I(x3^2)+I(x1*x2)+I(x1*x3)+I(x2*x3),\n test = \"F\") \n\n\nA anova: 10 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n2069.2381\n98.39868\nNA\nNA\n\n\nx1\n1\n1750.1220\n319.1161\n61.14168\n104.201315\n3.774296e-09\n\n\nx2\n1\n1586.0875\n483.1506\n69.85193\n62.373224\n2.028017e-07\n\n\nx3\n1\n295.1321\n1774.1060\n97.16712\n3.160752\n9.143800e-02\n\n\nI(x1^2)\n1\n1771.0198\n298.2182\n59.71937\n112.834735\n1.972872e-09\n\n\nI(x2^2)\n1\n1651.4985\n417.7396\n66.79705\n75.114902\n5.001792e-08\n\n\nI(x3^2)\n1\n288.7229\n1780.5152\n97.24285\n3.080982\n9.532366e-02\n\n\nI(x1 * x2)\n1\n1902.3924\n166.8457\n47.52349\n216.639964\n7.675449e-12\n\n\nI(x1 * x3)\n1\n1563.9280\n505.3101\n70.79365\n58.804744\n3.124530e-07\n\n\nI(x2 * x3)\n1\n1556.4302\n512.8079\n71.10295\n57.667163\n3.601417e-07\n\n\n\n\n\n\nqf(0.95,1,21-9-1)\n\n4.84433567494361\n\n\n\nRSS의 값이 가장 작은 x1*x2 변수를 선택\n\n\nm1 &lt;- update(m0, ~ . +I(x1*x2))\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ I(x1 * x2), data = dt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1481 -2.3759 -0.6042  2.6123  5.6241 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -15.29308    2.32149  -6.588 2.64e-06 ***\nI(x1 * x2)    0.02532    0.00172  14.719 7.68e-12 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.963 on 19 degrees of freedom\nMultiple R-squared:  0.9194,    Adjusted R-squared:  0.9151 \nF-statistic: 216.6 on 1 and 19 DF,  p-value: 7.675e-12\n\n\n\nadd1(m1, \n     scope = y ~ x1 + x2 + x3+ I(x1^2)+I(x2^2)+I(x3^2)+I(x1*x2)+I(x1*x3)+I(x2*x3),\n test = \"F\") \n\n\nA anova: 9 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n166.8457\n47.52349\nNA\nNA\n\n\nx1\n1\n3.43425436\n163.4115\n49.08673\n0.37828785\n0.5462188\n\n\nx2\n1\n9.62876230\n157.2170\n48.27519\n1.10241098\n0.3076309\n\n\nx3\n1\n0.09886847\n166.7469\n49.51104\n0.01067266\n0.9188604\n\n\nI(x1^2)\n1\n4.28275838\n162.5630\n48.97740\n0.47421406\n0.4998351\n\n\nI(x2^2)\n1\n6.42241880\n160.4233\n48.69917\n0.72061555\n0.4070905\n\n\nI(x3^2)\n1\n0.20188867\n166.6438\n49.49807\n0.02180696\n0.8842434\n\n\nI(x1 * x3)\n1\n0.61479960\n166.2309\n49.44597\n0.06657240\n0.7993213\n\n\nI(x2 * x3)\n1\n8.33654778\n158.5092\n48.44709\n0.94668239\n0.3434600\n\n\n\n\n\n\nqf(0.95,1,21-8-1)\n\n4.74722534672251\n\n\n\nX2의 RSS값이 157.2170으로 가장 낮지만 pr값이 유의하지 않으므로 선택하지 않는다.\n\n\n\n(f)\nregsubsets 함수를 이용하여 전역탐색법을 시행하여라. \\(C_p, MSE_p, R^2_{adj,p}\\) 기준으로 가장 적절한 모형을 선택한 후 위의 결과와 비교하시오\n\nfit2 &lt;- regsubsets(y~., data=dt2, nbest=20, method='exhaustive',)\nsummary(fit2)\n\nWarning message in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in = force.in, :\n“1  linear dependencies found”\n\n\nReordering variables and trying again:\n\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt2, nbest = 20, method = \"exhaustive\", \n    )\n9 Variables  (and intercept)\n     Forced in Forced out\nx1       FALSE      FALSE\nx2       FALSE      FALSE\nx3       FALSE      FALSE\nx_1      FALSE      FALSE\nx_2      FALSE      FALSE\nx_3      FALSE      FALSE\nx1x2     FALSE      FALSE\nx2x3     FALSE      FALSE\nx1x3     FALSE      FALSE\n20 subsets of each size up to 8\nSelection Algorithm: exhaustive\n          x1  x2  x3  x_1 x_2 x_3 x1x2 x1x3 x2x3\n1  ( 1 )  \" \" \" \" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n1  ( 2 )  \" \" \" \" \" \" \" \" \" \" \" \" \" \"  \"*\"  \" \" \n1  ( 3 )  \" \" \" \" \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \n1  ( 4 )  \"*\" \" \" \" \" \" \" \" \" \" \" \" \"  \" \"  \" \" \n1  ( 5 )  \" \" \" \" \" \" \" \" \"*\" \" \" \" \"  \" \"  \" \" \n1  ( 6 )  \" \" \"*\" \" \" \" \" \" \" \" \" \" \"  \" \"  \" \" \n1  ( 7 )  \" \" \" \" \" \" \" \" \" \" \" \" \" \"  \" \"  \"*\" \n1  ( 8 )  \" \" \" \" \"*\" \" \" \" \" \" \" \" \"  \" \"  \" \" \n1  ( 9 )  \" \" \" \" \" \" \" \" \" \" \"*\" \" \"  \" \"  \" \" \n2  ( 1 )  \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n2  ( 2 )  \" \" \" \" \" \" \" \" \" \" \" \" \"*\"  \" \"  \"*\" \n2  ( 3 )  \" \" \" \" \" \" \" \" \"*\" \" \" \"*\"  \" \"  \" \" \n2  ( 4 )  \" \" \" \" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n2  ( 5 )  \" \" \" \" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \" \" \n2  ( 6 )  \"*\" \" \" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n2  ( 7 )  \" \" \" \" \" \" \" \" \" \" \"*\" \"*\"  \" \"  \" \" \n2  ( 8 )  \" \" \" \" \"*\" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n2  ( 9 )  \" \" \" \" \" \" \" \" \"*\" \" \" \" \"  \"*\"  \" \" \n2  ( 10 ) \" \" \" \" \" \" \"*\" \"*\" \" \" \" \"  \" \"  \" \" \n2  ( 11 ) \"*\" \" \" \" \" \" \" \"*\" \" \" \" \"  \" \"  \" \" \n2  ( 12 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \"  \"*\"  \" \" \n2  ( 13 ) \" \" \"*\" \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \n2  ( 14 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \" \"  \" \"  \" \" \n2  ( 15 ) \" \" \" \" \" \" \" \" \" \" \" \" \" \"  \"*\"  \"*\" \n2  ( 16 ) \" \" \" \" \" \" \"*\" \" \" \" \" \" \"  \" \"  \"*\" \n2  ( 17 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"  \" \"  \"*\" \n2  ( 18 ) \" \" \"*\" \" \" \" \" \"*\" \" \" \" \"  \" \"  \" \" \n2  ( 19 ) \" \" \" \" \" \" \" \" \" \" \"*\" \" \"  \"*\"  \" \" \n2  ( 20 ) \" \" \" \" \" \" \"*\" \" \" \"*\" \" \"  \" \"  \" \" \n3  ( 1 )  \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \" \" \n3  ( 2 )  \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n3  ( 3 )  \" \" \" \" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \" \" \n3  ( 4 )  \" \" \" \" \" \" \" \" \"*\" \" \" \"*\"  \"*\"  \" \" \n3  ( 5 )  \"*\" \"*\" \" \" \" \" \"*\" \" \" \" \"  \" \"  \" \" \n3  ( 6 )  \" \" \"*\" \" \" \" \" \"*\" \" \" \"*\"  \" \"  \" \" \n3  ( 7 )  \" \" \"*\" \" \" \"*\" \"*\" \" \" \" \"  \" \"  \" \" \n3  ( 8 )  \" \" \"*\" \" \" \" \" \"*\" \" \" \" \"  \"*\"  \" \" \n3  ( 9 )  \"*\" \"*\" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n3  ( 10 ) \" \" \" \" \"*\" \" \" \" \" \" \" \"*\"  \" \"  \"*\" \n3  ( 11 ) \" \" \" \" \" \" \" \" \" \" \"*\" \"*\"  \" \"  \"*\" \n3  ( 12 ) \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \" \"  \"*\" \n3  ( 13 ) \" \" \"*\" \" \" \" \" \" \" \"*\" \"*\"  \" \"  \" \" \n3  ( 14 ) \" \" \"*\" \"*\" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n3  ( 15 ) \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\"  \" \"  \" \" \n3  ( 16 ) \" \" \" \" \"*\" \" \" \" \" \"*\" \"*\"  \" \"  \" \" \n3  ( 17 ) \" \" \" \" \" \" \" \" \"*\" \" \" \"*\"  \" \"  \"*\" \n3  ( 18 ) \"*\" \" \" \" \" \" \" \" \" \" \" \"*\"  \" \"  \"*\" \n3  ( 19 ) \" \" \" \" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n3  ( 20 ) \" \" \" \" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n4  ( 1 )  \"*\" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n4  ( 2 )  \"*\" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \" \" \n4  ( 3 )  \" \" \" \" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n4  ( 4 )  \" \" \" \" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n4  ( 5 )  \" \" \"*\" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \" \" \n4  ( 6 )  \" \" \"*\" \" \" \" \" \"*\" \" \" \"*\"  \"*\"  \" \" \n4  ( 7 )  \" \" \" \" \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n4  ( 8 )  \" \" \" \" \" \" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n4  ( 9 )  \" \" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \" \" \n4  ( 10 ) \" \" \"*\" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n4  ( 11 ) \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n4  ( 12 ) \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n4  ( 13 ) \" \" \"*\" \" \" \" \" \" \" \"*\" \"*\"  \"*\"  \" \" \n4  ( 14 ) \" \" \"*\" \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \" \" \n4  ( 15 ) \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \"*\"  \" \" \n4  ( 16 ) \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \"  \" \"  \"*\" \n4  ( 17 ) \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\"  \"*\"  \" \" \n4  ( 18 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \" \" \n4  ( 19 ) \" \" \" \" \"*\" \" \" \"*\" \" \" \"*\"  \" \"  \"*\" \n4  ( 20 ) \" \" \" \" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \"*\" \n5  ( 1 )  \"*\" \" \" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n5  ( 2 )  \"*\" \" \" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n5  ( 3 )  \"*\" \" \" \" \" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n5  ( 4 )  \"*\" \" \" \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n5  ( 5 )  \"*\" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n5  ( 6 )  \"*\" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n5  ( 7 )  \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \" \" \n5  ( 8 )  \"*\" \"*\" \" \" \" \" \" \" \"*\" \"*\"  \"*\"  \" \" \n5  ( 9 )  \"*\" \"*\" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n5  ( 10 ) \"*\" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \" \" \n5  ( 11 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \" \" \n5  ( 12 ) \"*\" \"*\" \" \" \" \" \"*\" \" \" \"*\"  \"*\"  \" \" \n5  ( 13 ) \"*\" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \"*\"  \" \" \n5  ( 14 ) \" \" \" \" \"*\" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n5  ( 15 ) \" \" \" \" \"*\" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n5  ( 16 ) \" \" \" \" \"*\" \" \" \"*\" \" \" \"*\"  \"*\"  \"*\" \n5  ( 17 ) \" \" \" \" \"*\" \"*\" \"*\" \" \" \"*\"  \" \"  \"*\" \n5  ( 18 ) \" \" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n5  ( 19 ) \" \" \"*\" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n5  ( 20 ) \" \" \" \" \"*\" \"*\" \" \" \" \" \"*\"  \"*\"  \"*\" \n6  ( 1 )  \"*\" \"*\" \"*\" \" \" \" \" \" \" \"*\"  \"*\"  \"*\" \n6  ( 2 )  \"*\" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n6  ( 3 )  \"*\" \" \" \"*\" \" \" \"*\" \" \" \"*\"  \"*\"  \"*\" \n6  ( 4 )  \"*\" \" \" \"*\" \"*\" \"*\" \" \" \"*\"  \" \"  \"*\" \n6  ( 5 )  \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n6  ( 6 )  \"*\" \" \" \"*\" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n6  ( 7 )  \"*\" \" \" \"*\" \"*\" \" \" \" \" \"*\"  \"*\"  \"*\" \n6  ( 8 )  \"*\" \"*\" \"*\" \" \" \" \" \"*\" \"*\"  \"*\"  \" \" \n6  ( 9 )  \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\"  \" \"  \" \" \n6  ( 10 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n6  ( 11 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n6  ( 12 ) \"*\" \" \" \" \" \" \" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n6  ( 13 ) \"*\" \" \" \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \n6  ( 14 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \"*\"  \"*\"  \"*\" \n6  ( 15 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\"  \" \"  \"*\" \n6  ( 16 ) \"*\" \"*\" \" \" \" \" \"*\" \" \" \"*\"  \"*\"  \"*\" \n6  ( 17 ) \"*\" \"*\" \" \" \"*\" \" \" \" \" \"*\"  \"*\"  \"*\" \n6  ( 18 ) \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \" \" \n6  ( 19 ) \"*\" \"*\" \" \" \" \" \"*\" \"*\" \"*\"  \"*\"  \" \" \n6  ( 20 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\"  \"*\"  \" \" \n7  ( 1 )  \"*\" \"*\" \"*\" \" \" \" \" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 2 )  \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n7  ( 3 )  \"*\" \"*\" \"*\" \" \" \"*\" \" \" \"*\"  \"*\"  \"*\" \n7  ( 4 )  \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\"  \" \"  \"*\" \n7  ( 5 )  \"*\" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \"*\"  \"*\" \n7  ( 6 )  \"*\" \" \" \"*\" \" \" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 7 )  \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \n7  ( 8 )  \"*\" \" \" \"*\" \"*\" \"*\" \" \" \"*\"  \"*\"  \"*\" \n7  ( 9 )  \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 10 ) \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \"*\"  \"*\"  \" \" \n7  ( 11 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \" \" \n7  ( 12 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\"  \"*\"  \" \" \n7  ( 13 ) \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \n7  ( 14 ) \"*\" \"*\" \" \" \" \" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 15 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 16 ) \"*\" \" \" \" \" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n7  ( 17 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\"  \"*\"  \"*\" \n7  ( 18 ) \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \"*\"  \"*\"  \" \" \n7  ( 19 ) \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\"  \"*\"  \" \" \n7  ( 20 ) \" \" \"*\" \"*\" \" \" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 1 )  \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 2 )  \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \n8  ( 3 )  \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 4 )  \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\"  \"*\"  \"*\" \n8  ( 5 )  \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 6 )  \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 7 )  \" \" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \n8  ( 8 )  \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \"  \"*\"  \"*\" \n\n\n\nwith(summary(fit2),\n     round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\n\nA matrix: 137 × 15 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nx_1\nx_2\nx_3\nx1x2\nx1x3\nx2x3\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n166.846\n0.919\n0.915\n-0.414\n-46.786\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n298.218\n0.856\n0.848\n12.646\n-34.590\n\n\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n298.218\n0.856\n0.848\n12.646\n-34.590\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n319.116\n0.846\n0.838\n14.724\n-33.168\n\n\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n417.740\n0.798\n0.787\n24.528\n-27.513\n\n\n1\n1\n0\n1\n0\n0\n0\n0\n0\n0\n0\n483.151\n0.767\n0.754\n31.030\n-24.458\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n512.808\n0.752\n0.739\n33.979\n-23.207\n\n\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1774.106\n0.143\n0.098\n159.366\n2.857\n\n\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1780.515\n0.140\n0.094\n160.003\n2.933\n\n\n2\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n157.217\n0.924\n0.916\n0.629\n-44.990\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n158.509\n0.923\n0.915\n0.758\n-44.818\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n160.423\n0.922\n0.914\n0.948\n-44.566\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n1\n1\n0\n162.563\n0.921\n0.913\n1.161\n-44.288\n\n\n2\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n162.563\n0.921\n0.913\n1.161\n-44.288\n\n\n2\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n163.411\n0.921\n0.912\n1.245\n-44.178\n\n\n2\n1\n0\n0\n0\n0\n0\n1\n1\n0\n0\n166.644\n0.919\n0.911\n1.566\n-43.767\n\n\n2\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n166.747\n0.919\n0.910\n1.576\n-43.754\n\n\n2\n1\n0\n0\n0\n0\n1\n0\n0\n1\n0\n168.659\n0.918\n0.909\n1.767\n-43.515\n\n\n2\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n168.659\n0.918\n0.909\n1.767\n-43.515\n\n\n2\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n176.505\n0.915\n0.905\n2.547\n-42.560\n\n\n2\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n177.768\n0.914\n0.905\n2.672\n-42.410\n\n\n2\n1\n0\n1\n0\n1\n0\n0\n0\n0\n0\n177.768\n0.914\n0.905\n2.672\n-42.410\n\n\n2\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n188.795\n0.909\n0.899\n3.768\n-41.146\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1\n1\n217.470\n0.895\n0.883\n6.619\n-38.177\n\n\n2\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n217.470\n0.895\n0.883\n6.619\n-38.177\n\n\n2\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1\n240.158\n0.884\n0.871\n8.874\n-36.093\n\n\n2\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n257.547\n0.876\n0.862\n10.603\n-34.625\n\n\n2\n1\n0\n0\n0\n0\n0\n1\n0\n1\n0\n293.474\n0.858\n0.842\n14.175\n-31.883\n\n\n2\n1\n0\n0\n0\n1\n0\n1\n0\n0\n0\n293.474\n0.858\n0.842\n14.175\n-31.883\n\n\n3\n1\n0\n1\n0\n1\n0\n0\n1\n0\n0\n132.668\n0.936\n0.925\n0.189\n-45.511\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n6\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\n115.569\n0.944\n0.920\n4.489\n-39.275\n\n\n6\n1\n1\n1\n0\n1\n0\n1\n1\n1\n0\n115.767\n0.944\n0.920\n4.508\n-39.239\n\n\n7\n1\n1\n1\n1\n0\n0\n1\n1\n1\n1\n110.658\n0.947\n0.918\n6.001\n-37.142\n\n\n7\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n110.658\n0.947\n0.918\n6.001\n-37.142\n\n\n7\n1\n1\n1\n1\n0\n1\n0\n1\n1\n1\n110.998\n0.946\n0.917\n6.034\n-37.078\n\n\n7\n1\n1\n1\n1\n1\n1\n0\n1\n0\n1\n110.998\n0.946\n0.917\n6.034\n-37.078\n\n\n7\n1\n1\n1\n1\n1\n0\n0\n1\n1\n1\n110.998\n0.946\n0.917\n6.034\n-37.078\n\n\n7\n1\n1\n0\n1\n0\n1\n1\n1\n1\n1\n112.107\n0.946\n0.917\n6.145\n-36.869\n\n\n7\n1\n1\n0\n1\n1\n1\n1\n1\n0\n1\n112.107\n0.946\n0.917\n6.145\n-36.869\n\n\n7\n1\n1\n0\n1\n1\n1\n0\n1\n1\n1\n112.112\n0.946\n0.917\n6.145\n-36.868\n\n\n7\n1\n1\n0\n1\n1\n0\n1\n1\n1\n1\n112.246\n0.946\n0.917\n6.158\n-36.843\n\n\n7\n1\n1\n1\n1\n0\n1\n1\n1\n1\n0\n112.818\n0.945\n0.916\n6.215\n-36.736\n\n\n7\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n112.818\n0.945\n0.916\n6.215\n-36.736\n\n\n7\n1\n1\n1\n1\n1\n0\n1\n1\n1\n0\n112.974\n0.945\n0.916\n6.231\n-36.707\n\n\n7\n1\n1\n1\n0\n1\n1\n1\n1\n0\n1\n113.220\n0.945\n0.916\n6.255\n-36.662\n\n\n7\n1\n1\n1\n0\n0\n1\n1\n1\n1\n1\n113.220\n0.945\n0.916\n6.255\n-36.662\n\n\n7\n1\n1\n1\n0\n1\n0\n1\n1\n1\n1\n113.233\n0.945\n0.916\n6.257\n-36.659\n\n\n7\n1\n1\n0\n0\n1\n1\n1\n1\n1\n1\n114.787\n0.945\n0.915\n6.411\n-36.373\n\n\n7\n1\n1\n1\n0\n1\n1\n0\n1\n1\n1\n115.049\n0.944\n0.914\n6.437\n-36.325\n\n\n7\n1\n1\n1\n0\n1\n1\n1\n1\n1\n0\n115.569\n0.944\n0.914\n6.489\n-36.230\n\n\n7\n1\n1\n1\n1\n1\n1\n0\n1\n1\n0\n115.969\n0.944\n0.914\n6.529\n-36.158\n\n\n7\n1\n0\n1\n1\n0\n1\n1\n1\n1\n1\n125.666\n0.939\n0.907\n7.493\n-34.471\n\n\n8\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n110.652\n0.947\n0.911\n8.000\n-34.099\n\n\n8\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n110.652\n0.947\n0.911\n8.000\n-34.099\n\n\n8\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n110.658\n0.947\n0.911\n8.001\n-34.098\n\n\n8\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n110.998\n0.946\n0.911\n8.034\n-34.033\n\n\n8\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n112.107\n0.946\n0.910\n8.145\n-33.824\n\n\n8\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n113.220\n0.945\n0.909\n8.255\n-33.617\n\n\n8\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n125.666\n0.939\n0.899\n9.493\n-31.427\n\n\n8\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n127.263\n0.938\n0.897\n9.651\n-31.162\n\n\n\n\n\n\ncp기준으로는 x1x2를 선택한 변수 1개가 가장 적절하다.\n\n\nfit3 &lt;- regsubsets(y~., data=dt2, nbest=1, method='exhaustive',)\nsummary(fit3)\nwith(summary(fit3),\n     round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\nWarning message in leaps.setup(x, y, wt = wt, nbest = nbest, nvmax = nvmax, force.in = force.in, :\n“1  linear dependencies found”\n\n\nReordering variables and trying again:\n\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt2, nbest = 1, method = \"exhaustive\", \n    )\n9 Variables  (and intercept)\n     Forced in Forced out\nx1       FALSE      FALSE\nx2       FALSE      FALSE\nx3       FALSE      FALSE\nx_1      FALSE      FALSE\nx_2      FALSE      FALSE\nx_3      FALSE      FALSE\nx1x2     FALSE      FALSE\nx2x3     FALSE      FALSE\nx1x3     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         x1  x2  x3  x_1 x_2 x_3 x1x2 x1x3 x2x3\n1  ( 1 ) \" \" \" \" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n2  ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \" \"  \" \" \n3  ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n4  ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \"*\"  \"*\"  \" \" \n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n6  ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \" \" \"*\"  \" \"  \"*\" \n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \n8  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \n\n\n\nA matrix: 8 × 15 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nx_1\nx_2\nx_3\nx1x2\nx1x3\nx2x3\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n166.846\n0.919\n0.915\n-0.414\n-46.786\n\n\n2\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n157.217\n0.924\n0.916\n0.629\n-44.990\n\n\n3\n1\n0\n1\n0\n0\n0\n0\n1\n1\n0\n132.668\n0.936\n0.925\n0.189\n-45.511\n\n\n4\n1\n1\n1\n0\n0\n0\n0\n1\n1\n0\n120.807\n0.942\n0.927\n1.009\n-44.433\n\n\n5\n1\n1\n0\n1\n1\n0\n0\n1\n0\n1\n112.246\n0.946\n0.928\n2.158\n-42.932\n\n\n6\n1\n1\n1\n1\n1\n0\n0\n1\n0\n1\n110.998\n0.946\n0.923\n4.034\n-40.122\n\n\n7\n1\n1\n1\n1\n1\n0\n1\n1\n0\n1\n110.658\n0.947\n0.918\n6.001\n-37.142\n\n\n8\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n110.652\n0.947\n0.911\n8.000\n-34.099"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html",
    "href": "posts/AS4_5-Copy1.html",
    "title": "AS HW4_5(2)",
    "section": "",
    "text": "python-data-analysis data\nData Source"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#model1원본",
    "href": "posts/AS4_5-Copy1.html#model1원본",
    "title": "AS HW4_5(2)",
    "section": "model1(원본)",
    "text": "model1(원본)\n\nmodel1 &lt;- lm(연봉.2018. ~ ., dt)\nsummary(model1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ ., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46529  -2418    424   2649  47773 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.513e+04  1.826e+04   0.829   0.4087    \n승           1.004e+03  5.375e+02   1.869   0.0639 .  \n패          -1.836e+02  5.504e+02  -0.334   0.7392    \n세          -2.112e+01  2.713e+02  -0.078   0.9381    \n홀드        -1.817e+01  3.161e+02  -0.057   0.9542    \n블론         4.535e+02  7.610e+02   0.596   0.5522    \n경기        -1.760e+02  1.456e+02  -1.209   0.2289    \n선발        -6.719e+02  4.616e+02  -1.456   0.1479    \n이닝         7.425e+01  1.156e+02   0.642   0.5217    \n삼진.9      -4.603e+02  2.349e+03  -0.196   0.8449    \n볼넷.9       1.194e+03  2.256e+03   0.529   0.5976    \n홈런.9       4.874e+03  1.413e+04   0.345   0.7306    \nBABIP       -9.997e+03  1.486e+04  -0.673   0.5022    \nLOB.        -4.350e+01  1.299e+02  -0.335   0.7382    \nERA         -7.413e+01  5.693e+02  -0.130   0.8966    \nRA9.WAR     -7.584e+02  1.487e+03  -0.510   0.6109    \nFIP         -6.436e+03  4.477e+04  -0.144   0.8859    \nkFIP         3.805e+03  3.593e+04   0.106   0.9158    \nWAR          8.559e+03  1.789e+03   4.783 4.55e-06 ***\n연봉.2017.   8.755e-01  4.444e-02  19.698  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9198 on 132 degrees of freedom\nMultiple R-squared:  0.9228,    Adjusted R-squared:  0.9116 \nF-statistic: 82.99 on 19 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodel1 &lt;- lm(연봉.2018. ~ +WAR+연봉.2017., dt)\nsummary(model1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ +WAR + 연봉.2017., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-50442  -1849    758   2050  56166 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -576.58811  889.09610  -0.649    0.518    \nWAR         7007.17364  761.83979   9.198 3.03e-16 ***\n연봉.2017.     0.89926    0.04022  22.360  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9124 on 149 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.913 \nF-statistic: 793.8 on 2 and 149 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model1)\n\nWAR1.84059685810784연봉.2017.1.84059685810784\n\n\n\nthreshold &lt;- 10\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\ncharacter(0)\n\n\n\nthreshold &lt;- 15\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\ncharacter(0)\n\n\n\npairs(dt,panel=panel.smooth)\n\n\n\n\n\n수치형 데이터들끼리의 상관계수 확인..\n\n\ndt_numeric &lt;- dt[, sapply(dt, is.numeric)]\ncor_matrix &lt;- cor(dt_numeric)\nprint(round(cor_matrix,2))\n\n              승    패    세  홀드  블론  경기  선발  이닝 삼진.9 볼넷.9 홈런.9\n승          1.00  0.71  0.05  0.09  0.11  0.40  0.77  0.91   0.08  -0.40  -0.12\n패          0.71  1.00  0.07  0.10  0.12  0.34  0.77  0.83   0.03  -0.39  -0.06\n세          0.05  0.07  1.00  0.11  0.61  0.43 -0.18  0.02   0.17  -0.13  -0.07\n홀드        0.09  0.10  0.11  1.00  0.49  0.72 -0.29  0.02   0.19  -0.15  -0.08\n블론        0.11  0.12  0.61  0.49  1.00  0.63 -0.26  0.01   0.19  -0.14  -0.06\n경기        0.40  0.34  0.43  0.72  0.63  1.00 -0.04  0.38   0.19  -0.36  -0.11\n선발        0.77  0.77 -0.18 -0.29 -0.26 -0.04  1.00  0.89  -0.06  -0.31  -0.06\n이닝        0.91  0.83  0.02  0.02  0.01  0.38  0.89  1.00   0.04  -0.45  -0.11\n삼진.9      0.08  0.03  0.17  0.19  0.19  0.19 -0.06  0.04   1.00   0.11   0.22\n볼넷.9     -0.40 -0.39 -0.13 -0.15 -0.14 -0.36 -0.31 -0.45   0.11   1.00   0.30\n홈런.9     -0.12 -0.06 -0.07 -0.08 -0.06 -0.11 -0.06 -0.11   0.22   0.30   1.00\nBABIP      -0.17 -0.13 -0.09 -0.10 -0.11 -0.24 -0.10 -0.19   0.46   0.28   0.36\nLOB.        0.13 -0.02  0.17  0.05  0.10  0.11  0.04  0.10  -0.07  -0.15  -0.27\nERA        -0.27 -0.19 -0.15 -0.16 -0.16 -0.32 -0.16 -0.29   0.26   0.52   0.63\nRA9.WAR     0.85  0.60  0.17  0.00  0.01  0.28  0.74  0.85   0.10  -0.40  -0.19\nFIP        -0.30 -0.23 -0.20 -0.21 -0.21 -0.35 -0.15 -0.30  -0.15   0.63   0.83\nkFIP       -0.31 -0.24 -0.23 -0.24 -0.24 -0.37 -0.14 -0.30  -0.32   0.61   0.74\nWAR         0.82  0.63  0.08 -0.04 -0.06  0.20  0.76  0.83   0.15  -0.39  -0.21\n연봉.2017.  0.63  0.43  0.26  0.00  0.15  0.23  0.49  0.59   0.10  -0.33  -0.10\n연봉.2018.  0.71  0.47  0.21 -0.02  0.10  0.21  0.56  0.66   0.10  -0.33  -0.12\n           BABIP  LOB.   ERA RA9.WAR   FIP  kFIP   WAR 연봉.2017. 연봉.2018.\n승         -0.17  0.13 -0.27    0.85 -0.30 -0.31  0.82       0.63       0.71\n패         -0.13 -0.02 -0.19    0.60 -0.23 -0.24  0.63       0.43       0.47\n세         -0.09  0.17 -0.15    0.17 -0.20 -0.23  0.08       0.26       0.21\n홀드       -0.10  0.05 -0.16    0.00 -0.21 -0.24 -0.04       0.00      -0.02\n블론       -0.11  0.10 -0.16    0.01 -0.21 -0.24 -0.06       0.15       0.10\n경기       -0.24  0.11 -0.32    0.28 -0.35 -0.37  0.20       0.23       0.21\n선발       -0.10  0.04 -0.16    0.74 -0.15 -0.14  0.76       0.49       0.56\n이닝       -0.19  0.10 -0.29    0.85 -0.30 -0.30  0.83       0.59       0.66\n삼진.9      0.46 -0.07  0.26    0.10 -0.15 -0.32  0.15       0.10       0.10\n볼넷.9      0.28 -0.15  0.52   -0.40  0.63  0.61 -0.39      -0.33      -0.33\n홈런.9      0.36 -0.27  0.63   -0.19  0.83  0.74 -0.21      -0.10      -0.12\nBABIP       1.00 -0.51  0.73   -0.19  0.25  0.17 -0.08      -0.09      -0.10\nLOB.       -0.51  1.00 -0.72    0.29 -0.29 -0.27  0.14       0.11       0.13\nERA         0.73 -0.72  1.00   -0.34  0.65  0.58 -0.26      -0.20      -0.22\nRA9.WAR    -0.19  0.29 -0.34    1.00 -0.37 -0.38  0.92       0.64       0.74\nFIP         0.25 -0.29  0.65   -0.37  1.00  0.98 -0.39      -0.27      -0.28\nkFIP        0.17 -0.27  0.58   -0.38  0.98  1.00 -0.41      -0.28      -0.30\nWAR        -0.08  0.14 -0.26    0.92 -0.39 -0.41  1.00       0.68       0.79\n연봉.2017. -0.09  0.11 -0.20    0.64 -0.27 -0.28  0.68       1.00       0.93\n연봉.2018. -0.10  0.13 -0.22    0.74 -0.28 -0.30  0.79       0.93       1.00"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#vif계수가-높은-변수-제거",
    "href": "posts/AS4_5-Copy1.html#vif계수가-높은-변수-제거",
    "title": "AS HW4_5(2)",
    "section": "VIF계수가 높은 변수 제거",
    "text": "VIF계수가 높은 변수 제거\n\nmodel2(Vif 10 이상인 변수 제거)\n\nmodel2 &lt;- lm(연봉.2018. ~ .-경기-선발-이닝-삼진.9-볼넷.9-홈런.9-ERA-RA9.WAR-FIP-kFIP, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - 경기 - 선발 - 이닝 - 삼진.9 - \n    볼넷.9 - 홈런.9 - ERA - RA9.WAR - FIP - kFIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48657  -1981    511   2303  51073 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.432e+03  7.893e+03   0.815   0.4165    \n승           4.770e+02  4.061e+02   1.175   0.2421    \n패          -7.851e+02  3.525e+02  -2.227   0.0275 *  \n세          -1.172e+02  2.150e+02  -0.545   0.5865    \n홀드        -1.229e+02  1.973e+02  -0.623   0.5344    \n블론         6.340e+02  7.188e+02   0.882   0.3792    \nBABIP       -7.810e+03  9.994e+03  -0.781   0.4358    \nLOB.        -4.979e+01  7.793e+01  -0.639   0.5239    \nWAR          7.298e+03  1.169e+03   6.243 4.67e-09 ***\n연봉.2017.   8.846e-01  4.322e-02  20.469  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9149 on 142 degrees of freedom\nMultiple R-squared:  0.9178,    Adjusted R-squared:  0.9126 \nF-statistic: 176.1 on 9 and 142 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model2)\n\n승4.44133840452701패2.19787784118271세1.9291576101908홀드1.43155944990414블론2.48814143828698BABIP1.42670331782564LOB.1.47225296358887WAR4.30937396392511연봉.2017.2.11357639082776\n\n\n\nmodel1에서 다중공산성이 높았던 변수들을 제외하고 lm을 돌렸더니, 회귀모형은 유의하게 나왔고 R^2값도 91%로 높게 나왔지만 model1보다는 R^2값이 조금 적게 나왔다.\n다중공산성이 높은 변수를 제외하는 것은 다른 것들도 확인을 해보아야 한다.\n\n\n\nVIF제거시 고려사항\n\n\nVIF계수가 높은 피처 우선 제거하되, FIP, kFIP와 같이 유사한 변수들은 두개 중에서 하나만 제거해보자.\n\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP, dt)\nsummary(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46688  -2466    423   2597  47710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.406e+04  1.660e+04   0.847    0.399    \n승           1.007e+03  5.352e+02   1.882    0.062 .  \n패          -1.723e+02  5.427e+02  -0.317    0.751    \n세          -2.263e+01  2.701e+02  -0.084    0.933    \n홀드        -1.779e+01  3.149e+02  -0.056    0.955    \n블론         4.563e+02  7.579e+02   0.602    0.548    \n경기        -1.738e+02  1.443e+02  -1.205    0.230    \n선발        -6.701e+02  4.598e+02  -1.458    0.147    \n이닝         7.216e+01  1.142e+02   0.632    0.529    \n삼진.9      -7.714e+02  9.085e+02  -0.849    0.397    \n볼넷.9       8.998e+02  9.504e+02   0.947    0.346    \n홈런.9       2.904e+03  3.404e+03   0.853    0.395    \nBABIP       -9.797e+03  1.474e+04  -0.665    0.507    \nLOB.        -4.465e+01  1.292e+02  -0.346    0.730    \nERA         -8.076e+01  5.654e+02  -0.143    0.887    \nRA9.WAR     -7.473e+02  1.480e+03  -0.505    0.614    \nkFIP        -1.347e+03  2.371e+03  -0.568    0.571    \nWAR          8.560e+03  1.783e+03   4.802 4.17e-06 ***\n연봉.2017.   8.757e-01  4.426e-02  19.787  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9164 on 133 degrees of freedom\nMultiple R-squared:  0.9227,    Adjusted R-squared:  0.9123 \nF-statistic: 88.25 on 18 and 133 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model3)\n\n승7.68840921316788패5.19140274673014세3.03263975401923홀드3.63578951116975블론2.75775305712261경기14.0426530671348선발36.1331990754777이닝59.3709458069269삼진.911.8666574529657볼넷.99.0682604275144홈런.921.5595297493918BABIP3.09205217740503LOB.4.03056643053091ERA9.9781711774582RA9.WAR13.3837520395074kFIP39.6977412189025WAR9.99134587181084연봉.2017.2.20945320867407\n\n\n\nVIF계수가 가장 높았떤 FIP를 제거하니 전체적으로 VIF값들이 많이 감소했다. 볼넷의 경우 50에서 9로 감소함\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47170  -2539    292   2603  47529 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.425e+04  1.656e+04   0.860   0.3912    \n승           1.053e+03  5.292e+02   1.989   0.0487 *  \n패          -1.258e+02  5.365e+02  -0.234   0.8150    \n세          -7.264e+01  2.576e+02  -0.282   0.7784    \n홀드        -7.025e+01  3.031e+02  -0.232   0.8171    \n블론         4.745e+02  7.557e+02   0.628   0.5312    \n경기        -1.021e+02  8.877e+01  -1.150   0.2523    \n선발        -4.306e+02  2.595e+02  -1.659   0.0994 .  \n삼진.9      -7.892e+02  9.060e+02  -0.871   0.3853    \n볼넷.9       8.829e+02  9.479e+02   0.931   0.3533    \n홈런.9       2.956e+03  3.396e+03   0.871   0.3855    \nBABIP       -1.004e+04  1.470e+04  -0.683   0.4957    \nLOB.        -4.506e+01  1.289e+02  -0.350   0.7272    \nERA         -6.838e+01  5.637e+02  -0.121   0.9036    \nRA9.WAR     -4.551e+02  1.402e+03  -0.325   0.7460    \nkFIP        -1.349e+03  2.366e+03  -0.570   0.5696    \nWAR          8.733e+03  1.758e+03   4.968 2.03e-06 ***\n연봉.2017.   8.784e-01  4.395e-02  19.984  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9143 on 134 degrees of freedom\nMultiple R-squared:  0.9225,    Adjusted R-squared:  0.9127 \nF-statistic: 93.84 on 17 and 134 DF,  p-value: &lt; 2.2e-16\n\n\n승7.54995904402493패5.09588128549648세2.77188079102657홀드3.38288542173628블론2.75379743632109경기5.34096225137479선발11.5652288817222삼진.911.8553164987874볼넷.99.06115155090058홈런.921.5464901061541BABIP3.0899457520436LOB.4.03046844648891ERA9.96618149145026RA9.WAR12.0759322801392kFIP39.6977185280435WAR9.75717291579296연봉.2017.2.18906326524158\n\n\n\n그 다음 vif계수값이 높은 ’이닝’을 제거했다.\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47261  -2379    309   2742  47813 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.978e+03  1.054e+04   0.662   0.5090    \n승           1.055e+03  5.278e+02   1.999   0.0476 *  \n패          -1.135e+02  5.347e+02  -0.212   0.8323    \n세          -7.382e+01  2.569e+02  -0.287   0.7743    \n홀드        -7.661e+01  3.021e+02  -0.254   0.8002    \n블론         5.038e+02  7.521e+02   0.670   0.5040    \n경기        -9.923e+01  8.841e+01  -1.122   0.2637    \n선발        -4.402e+02  2.583e+02  -1.704   0.0906 .  \n삼진.9      -3.109e+02  3.413e+02  -0.911   0.3639    \n볼넷.9       4.082e+02  4.514e+02   0.904   0.3675    \n홈런.9       1.129e+03  1.118e+03   1.010   0.3143    \nBABIP       -9.576e+03  1.464e+04  -0.654   0.5141    \nLOB.        -3.779e+01  1.279e+02  -0.295   0.7681    \nERA         -8.963e+01  5.611e+02  -0.160   0.8733    \nRA9.WAR     -4.669e+02  1.399e+03  -0.334   0.7390    \nWAR          8.800e+03  1.749e+03   5.030 1.53e-06 ***\n연봉.2017.   8.779e-01  4.384e-02  20.027  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9120 on 135 degrees of freedom\nMultiple R-squared:  0.9223,    Adjusted R-squared:  0.9131 \nF-statistic: 100.2 on 16 and 135 DF,  p-value: &lt; 2.2e-16\n\n\n승7.54945354936935패5.08758327243023세2.77170233465349홀드3.37829494203411블론2.74099514800148경기5.32417495720081선발11.5165551404735삼진.91.691074402967볼넷.92.06526070907551홈런.92.34713984614222BABIP3.08046557773165LOB.3.99101474806593ERA9.92261602645989RA9.WAR12.0732896169765WAR9.71340804685137연봉.2017.2.18834997395971\n\n\n\nKFIP제거\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP-RA9.WAR, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP - RA9.WAR, \n    data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47256  -2340    228   2820  48394 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.002e+03  1.005e+04   0.796   0.4273    \n승           1.005e+03  5.044e+02   1.993   0.0483 *  \n패          -6.188e+01  5.102e+02  -0.121   0.9036    \n세          -1.005e+02  2.434e+02  -0.413   0.6805    \n홀드        -8.969e+01  2.986e+02  -0.300   0.7643    \n블론         5.293e+02  7.457e+02   0.710   0.4790    \n경기        -1.027e+02  8.749e+01  -1.174   0.2424    \n선발        -4.671e+02  2.447e+02  -1.909   0.0584 .  \n삼진.9      -3.052e+02  3.398e+02  -0.898   0.3707    \n볼넷.9       4.218e+02  4.481e+02   0.941   0.3482    \n홈런.9       1.154e+03  1.112e+03   1.037   0.3013    \nBABIP       -9.059e+03  1.451e+04  -0.624   0.5334    \nLOB.        -5.310e+01  1.190e+02  -0.446   0.6562    \nERA         -1.249e+02  5.493e+02  -0.227   0.8205    \nWAR          8.406e+03  1.289e+03   6.523 1.25e-09 ***\n연봉.2017.   8.790e-01  4.358e-02  20.168  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9090 on 136 degrees of freedom\nMultiple R-squared:  0.9223,    Adjusted R-squared:  0.9137 \nF-statistic: 107.6 on 15 and 136 DF,  p-value: &lt; 2.2e-16\n\n\n승6.94042688100102패4.66294914095306세2.50425253772256홀드3.32146243586634블론2.71278267026247경기5.24904766467952선발10.3991198289636삼진.91.68672132907842볼넷.92.04837389574494홈런.92.33709850623971BABIP3.046073880605LOB.3.47761526760098ERA9.57142193166993WAR5.30623103354661연봉.2017.2.17720905024618\n\n\n\nmodel3 &lt;- lm(연봉.2018. ~ .-FIP-이닝-kFIP-RA9.WAR-선발, dt)\nsummary(model3)\nvif(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - FIP - 이닝 - kFIP - RA9.WAR - \n    선발, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46776  -2395    374   2597  50018 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7221.2085 10138.7800   0.712   0.4775    \n승            558.2422   451.0986   1.238   0.2180    \n패           -758.5773   359.9930  -2.107   0.0369 *  \n세            -12.1963   241.3026  -0.051   0.9598    \n홀드          106.5326   283.0186   0.376   0.7072    \n블론          843.4665   734.3349   1.149   0.2527    \n경기          -69.6278    86.5803  -0.804   0.4227    \n삼진.9       -270.1732   342.5480  -0.789   0.4316    \n볼넷.9        431.0859   452.3679   0.953   0.3423    \n홈런.9        983.4449  1119.0004   0.879   0.3810    \nBABIP       -8863.3423 14648.1787  -0.605   0.5461    \nLOB.          -57.5239   120.1320  -0.479   0.6328    \nERA           -88.2397   554.2171  -0.159   0.8737    \nWAR          7825.6419  1264.4051   6.189 6.57e-09 ***\n연봉.2017.      0.8792     0.0440  19.981  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9178 on 137 degrees of freedom\nMultiple R-squared:  0.9202,    Adjusted R-squared:  0.912 \nF-statistic: 112.8 on 14 and 137 DF,  p-value: &lt; 2.2e-16\n\n\n승5.44563629770577패2.27744620091112세2.41391273581397홀드2.9278580003899블론2.58069224802437경기5.04287223750904삼진.91.68181308100372볼넷.92.04813245831198홈런.92.32207970272157BABIP3.04592151858475LOB.3.47629848428834ERA9.55974332977644WAR5.01053061622992연봉.2017.2.17719632600132\n\n\n\n유의미한 변수는 ’WAR’과 ’연봉(2017)’이다."
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#정규화",
    "href": "posts/AS4_5-Copy1.html#정규화",
    "title": "AS HW4_5(2)",
    "section": "정규화",
    "text": "정규화\n\nnormalize\n\nnormalize &lt;- function(x) {\n  return((x - mean(x)) / sd(x))\n}\n\n\ndf_normalized &lt;- as.data.frame(lapply(dt, normalize))\n\n\nhead(df_normalized)\n\n\nA data.frame: 6 × 20\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2017.\n연봉.2018.\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n3.313623\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n0.05943348\n2.452068\n2.645175\n0.6720988\n-0.8689998\n-0.44238194\n0.01678276\n0.4466146\n-0.5870557\n3.174630\n-0.9710297\n-1.0581252\n4.503142\n2.7347053\n3.912893\n\n\n2\n2.019505\n2.5047212\n-0.0985024\n-0.5857052\n-0.5435919\n0.05943348\n2.349505\n2.547755\n0.1345315\n-0.9875023\n-0.66852133\n-0.24168646\n-0.1227637\n-0.5198553\n3.114968\n-1.0618879\n-1.0732645\n4.094734\n1.3373033\n3.266495\n\n\n3\n4.348918\n0.9077513\n-0.3064519\n-0.5857052\n-0.5435919\n0.11105570\n2.554632\n2.706808\n0.1097751\n-0.8859287\n-0.41288550\n-0.09559517\n0.3085835\n-0.6254559\n2.973948\n-0.8374147\n-0.8663606\n3.761956\n5.3298806\n6.821679\n\n\n4\n1.760682\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n-0.04381097\n2.246942\n2.350927\n0.3502657\n-0.9451800\n-0.18674611\n-0.47768010\n0.5587649\n-0.6278559\n2.740722\n-0.6984550\n-0.7603854\n2.998081\n3.3335919\n2.620098\n\n\n5\n2.537153\n1.2271453\n-0.3064519\n-0.5857052\n-0.5435919\n0.05943348\n2.452068\n2.587518\n0.1557512\n-0.8774643\n-0.29489973\n-0.19673529\n0.4811224\n-0.5390554\n2.751570\n-0.6129414\n-0.6190851\n2.809003\n2.7347053\n2.975617\n\n\n6\n1.243035\n2.1853272\n-0.3064519\n-0.5857052\n-0.5435919\n-0.14705541\n2.041816\n2.048726\n0.1309948\n-1.0340569\n-0.08842464\n-0.57882022\n0.6536613\n-0.7214564\n2.963100\n-0.5808738\n-0.6140386\n2.476226\n0.7384167\n2.135301\n\n\n\n\n\n\nmodel4 &lt;- lm(연봉.2018. ~ ., df_normalized)\nsummary(model4)\n\n\nCall:\nlm(formula = 연봉.2018. ~ ., data = df_normalized)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.50382 -0.07816  0.01372  0.08561  1.54402 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5.061e-16  2.411e-02   0.000   1.0000    \n승           1.254e-01  6.712e-02   1.869   0.0639 .  \n패          -1.858e-02  5.569e-02  -0.334   0.7392    \n세          -3.282e-03  4.216e-02  -0.078   0.9381    \n홀드        -2.652e-03  4.613e-02  -0.057   0.9542    \n블론         2.395e-02  4.019e-02   0.596   0.5522    \n경기        -1.102e-01  9.116e-02  -1.209   0.2289    \n선발        -2.117e-01  1.455e-01  -1.456   0.1479    \n이닝         1.207e-01  1.879e-01   0.642   0.5217    \n삼진.9      -4.207e-02  2.146e-01  -0.196   0.8449    \n볼넷.9       9.115e-02  1.723e-01   0.529   0.5976    \n홈런.9       1.602e-01  4.643e-01   0.345   0.7306    \nBABIP       -2.875e-02  4.273e-02  -0.673   0.5022    \nLOB.        -1.630e-02  4.866e-02  -0.335   0.7382    \nERA         -9.982e-03  7.667e-02  -0.130   0.8966    \nRA9.WAR     -4.519e-02  8.862e-02  -0.510   0.6109    \nFIP         -3.892e-01  2.707e+00  -0.144   0.8859    \nkFIP         2.437e-01  2.301e+00   0.106   0.9158    \nWAR          3.657e-01  7.647e-02   4.783 4.55e-06 ***\n연봉.2017.   7.087e-01  3.598e-02  19.698  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.2973 on 132 degrees of freedom\nMultiple R-squared:  0.9228,    Adjusted R-squared:  0.9116 \nF-statistic: 82.99 on 19 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n\nthreshold &lt;- 10\n\n\nhigh_vif_vars &lt;- ㅍget_high_vif_variables(model4, threshold)\nprint(high_vif_vars)\n\n [1] \"경기\"    \"선발\"    \"이닝\"    \"삼진.9\"  \"볼넷.9\"  \"홈런.9\"  \"ERA\"    \n [8] \"RA9.WAR\" \"FIP\"     \"kFIP\"   \n\n\n\nvif(model4)\n\n승7.6993466963604패5.30032799820294세3.03718565967898홀드3.63605161357941블론2.75956034802382경기14.2011271199764선발36.160187873294이닝60.3244538179135삼진.978.7161704890434볼넷.950.7257985014939홈런.9368.399308167005BABIP3.11936893312485LOB.4.04602533224442ERA10.044173823258RA9.WAR13.4198973514472FIP12525.2424058262kFIP9046.04880481494WAR9.99177856815799연봉.2017.2.2118694256429\n\n\n\n###"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#변수선택",
    "href": "posts/AS4_5-Copy1.html#변수선택",
    "title": "AS HW4_5(2)",
    "section": "변수선택",
    "text": "변수선택\n\nmodel3(AIC)\n- AIC(Step)\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n\n\nmodel3 = step(\n m0,\n scope = 연봉.2018. ~연봉.2017.+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"both\")\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n             Df  Sum of Sq        RSS    AIC\n+ 연봉.2017.  1 1.2511e+11 1.9445e+10 2841.4\n+ WAR         1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR     1 7.9230e+10 6.5326e+10 3025.6\n+ 승          1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝        1 6.2759e+10 8.1797e+10 3059.8\n+ 선발        1 4.5409e+10 9.9147e+10 3089.0\n+ 패          1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9      1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP        1 1.2591e+10 1.3197e+11 3132.4\n+ FIP         1 1.1403e+10 1.3315e+11 3133.8\n+ ERA         1 6.7332e+09 1.3782e+11 3139.1\n+ 세          1 6.4461e+09 1.3811e+11 3139.4\n+ 경기        1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.        1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9      1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                     1.4456e+11 3144.3\n+ 삼진.9      1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP       1 1.5139e+09 1.4304e+11 3144.7\n+ 블론        1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드        1 4.3499e+07 1.4451e+11 3146.3\n\nStep:  AIC=2841.38\n연봉.2018. ~ 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n+ WAR         1 7.0421e+09 1.2403e+10 2775.0\n+ RA9.WAR     1 4.9589e+09 1.4486e+10 2798.6\n+ 승          1 3.8414e+09 1.5604e+10 2809.9\n+ 이닝        1 2.8118e+09 1.6633e+10 2819.6\n+ 선발        1 2.1318e+09 1.7313e+10 2825.7\n+ 패          1 8.8114e+08 1.8564e+10 2836.3\n&lt;none&gt;                     1.9445e+10 2841.4\n+ 블론        1 2.2022e+08 1.9225e+10 2841.7\n+ 세          1 1.7105e+08 1.9274e+10 2842.0\n+ kFIP        1 1.6254e+08 1.9283e+10 2842.1\n+ FIP         1 1.5483e+08 1.9290e+10 2842.2\n+ ERA         1 1.0735e+08 1.9338e+10 2842.5\n+ LOB.        1 7.7049e+07 1.9368e+10 2842.8\n+ 홈런.9      1 7.3957e+07 1.9371e+10 2842.8\n+ 볼넷.9      1 6.4565e+07 1.9381e+10 2842.9\n+ BABIP       1 5.6938e+07 1.9388e+10 2842.9\n+ 홀드        1 3.8024e+07 1.9407e+10 2843.1\n+ 삼진.9      1 5.5081e+06 1.9440e+10 2843.3\n+ 경기        1 1.2651e+04 1.9445e+10 2843.4\n- 연봉.2017.  1 1.2511e+11 1.4456e+11 3144.3\n\nStep:  AIC=2775.03\n연봉.2018. ~ 연봉.2017. + WAR\n\n             Df  Sum of Sq        RSS    AIC\n+ 패          1 2.1336e+08 1.2190e+10 2774.4\n+ kFIP        1 1.8769e+08 1.2215e+10 2774.7\n+ 선발        1 1.7153e+08 1.2232e+10 2774.9\n+ FIP         1 1.6877e+08 1.2234e+10 2774.9\n+ 볼넷.9      1 1.6419e+08 1.2239e+10 2775.0\n&lt;none&gt;                     1.2403e+10 2775.0\n+ 이닝        1 1.4704e+08 1.2256e+10 2775.2\n+ 홈런.9      1 5.1612e+07 1.2351e+10 2776.4\n+ 삼진.9      1 4.8349e+07 1.2355e+10 2776.4\n+ 승          1 3.0076e+07 1.2373e+10 2776.7\n+ 경기        1 2.7246e+07 1.2376e+10 2776.7\n+ BABIP       1 2.4182e+07 1.2379e+10 2776.7\n+ ERA         1 1.7077e+07 1.2386e+10 2776.8\n+ 블론        1 1.1153e+07 1.2392e+10 2776.9\n+ RA9.WAR     1 6.6509e+06 1.2396e+10 2776.9\n+ 세          1 4.3325e+06 1.2399e+10 2777.0\n+ 홀드        1 3.4824e+06 1.2400e+10 2777.0\n+ LOB.        1 6.6018e+05 1.2402e+10 2777.0\n- WAR         1 7.0421e+09 1.9445e+10 2841.4\n- 연봉.2017.  1 4.1619e+10 5.4022e+10 2996.7\n\nStep:  AIC=2774.4\n연봉.2018. ~ 연봉.2017. + WAR + 패\n\n             Df  Sum of Sq        RSS    AIC\n+ kFIP        1 1.9738e+08 1.1992e+10 2773.9\n+ 승          1 1.8072e+08 1.2009e+10 2774.1\n+ FIP         1 1.7496e+08 1.2015e+10 2774.2\n&lt;none&gt;                     1.2190e+10 2774.4\n- 패          1 2.1336e+08 1.2403e+10 2775.0\n+ 볼넷.9      1 1.0330e+08 1.2086e+10 2775.1\n+ 홈런.9      1 7.1015e+07 1.2119e+10 2775.5\n+ 삼진.9      1 6.6895e+07 1.2123e+10 2775.6\n+ 블론        1 4.2173e+07 1.2148e+10 2775.9\n+ BABIP       1 4.1954e+07 1.2148e+10 2775.9\n+ 선발        1 3.1474e+07 1.2158e+10 2776.0\n+ ERA         1 1.3441e+07 1.2176e+10 2776.2\n+ 이닝        1 5.8966e+06 1.2184e+10 2776.3\n+ 세          1 3.4705e+06 1.2186e+10 2776.3\n+ RA9.WAR     1 2.4143e+06 1.2187e+10 2776.4\n+ LOB.        1 1.7129e+06 1.2188e+10 2776.4\n+ 경기        1 1.1252e+06 1.2189e+10 2776.4\n+ 홀드        1 1.8992e+05 1.2190e+10 2776.4\n- WAR         1 6.3743e+09 1.8564e+10 2836.3\n- 연봉.2017.  1 4.1680e+10 5.3870e+10 2998.3\n\nStep:  AIC=2773.92\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP\n\n             Df  Sum of Sq        RSS    AIC\n+ 승          1 1.6741e+08 1.1825e+10 2773.8\n&lt;none&gt;                     1.1992e+10 2773.9\n+ 블론        1 1.2836e+08 1.1864e+10 2774.3\n- kFIP        1 1.9738e+08 1.2190e+10 2774.4\n+ 선발        1 1.1764e+08 1.1875e+10 2774.4\n- 패          1 2.2305e+08 1.2215e+10 2774.7\n+ BABIP       1 7.5190e+07 1.1917e+10 2775.0\n+ ERA         1 2.1818e+07 1.1971e+10 2775.6\n+ 홀드        1 2.1404e+07 1.1971e+10 2775.6\n+ 삼진.9      1 1.9275e+07 1.1973e+10 2775.7\n+ 경기        1 1.7028e+07 1.1975e+10 2775.7\n+ 이닝        1 1.3041e+07 1.1979e+10 2775.8\n+ FIP         1 9.3610e+06 1.1983e+10 2775.8\n+ 볼넷.9      1 8.8432e+06 1.1983e+10 2775.8\n+ 홈런.9      1 8.7223e+06 1.1984e+10 2775.8\n+ LOB.        1 4.0316e+06 1.1988e+10 2775.9\n+ RA9.WAR     1 2.0131e+06 1.1990e+10 2775.9\n+ 세          1 1.4454e+06 1.1991e+10 2775.9\n- WAR         1 6.4941e+09 1.8486e+10 2837.7\n- 연봉.2017.  1 4.1735e+10 5.3727e+10 2999.9\n\nStep:  AIC=2773.78\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승\n\n             Df  Sum of Sq        RSS    AIC\n+ 이닝        1 2.1565e+08 1.1609e+10 2773.0\n+ 선발        1 1.9668e+08 1.1628e+10 2773.2\n&lt;none&gt;                     1.1825e+10 2773.8\n- 승          1 1.6741e+08 1.1992e+10 2773.9\n- kFIP        1 1.8408e+08 1.2009e+10 2774.1\n+ 블론        1 8.3012e+07 1.1742e+10 2774.7\n+ RA9.WAR     1 6.3182e+07 1.1762e+10 2775.0\n+ BABIP       1 4.5875e+07 1.1779e+10 2775.2\n+ 볼넷.9      1 1.7921e+07 1.1807e+10 2775.6\n+ 삼진.9      1 1.4564e+07 1.1810e+10 2775.6\n+ 홈런.9      1 1.2160e+07 1.1813e+10 2775.6\n+ ERA         1 8.8026e+06 1.1816e+10 2775.7\n+ FIP         1 8.1221e+06 1.1817e+10 2775.7\n+ 세          1 5.8214e+06 1.1819e+10 2775.7\n+ 홀드        1 5.2671e+06 1.1820e+10 2775.7\n+ LOB.        1 3.9758e+05 1.1825e+10 2775.8\n+ 경기        1 3.3176e+05 1.1825e+10 2775.8\n- 패          1 3.6648e+08 1.2191e+10 2776.4\n- WAR         1 3.6353e+09 1.5460e+10 2812.5\n- 연봉.2017.  1 3.9188e+10 5.1013e+10 2994.0\n\nStep:  AIC=2772.98\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n- 패          1 3.1923e+07 1.1641e+10 2771.4\n&lt;none&gt;                     1.1609e+10 2773.0\n- kFIP        1 2.1496e+08 1.1824e+10 2773.8\n- 이닝        1 2.1565e+08 1.1825e+10 2773.8\n+ BABIP       1 8.7592e+07 1.1522e+10 2773.8\n+ 선발        1 5.0414e+07 1.1559e+10 2774.3\n+ 블론        1 3.9472e+07 1.1570e+10 2774.5\n+ 삼진.9      1 3.3863e+07 1.1575e+10 2774.5\n+ ERA         1 3.3525e+07 1.1576e+10 2774.5\n+ FIP         1 1.8310e+07 1.1591e+10 2774.7\n+ 홈런.9      1 1.2031e+07 1.1597e+10 2774.8\n+ RA9.WAR     1 1.0398e+07 1.1599e+10 2774.8\n+ LOB.        1 3.1362e+06 1.1606e+10 2774.9\n+ 경기        1 1.9500e+06 1.1607e+10 2775.0\n+ 볼넷.9      1 1.2880e+06 1.1608e+10 2775.0\n+ 세          1 2.2726e+05 1.1609e+10 2775.0\n+ 홀드        1 9.3003e+04 1.1609e+10 2775.0\n- 승          1 3.7002e+08 1.1979e+10 2775.8\n- WAR         1 3.7546e+09 1.5364e+10 2813.6\n- 연봉.2017.  1 3.8723e+10 5.0333e+10 2993.9\n\nStep:  AIC=2771.4\n연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                     1.1641e+10 2771.4\n+ BABIP       1 9.5915e+07 1.1545e+10 2772.1\n- kFIP        1 2.2291e+08 1.1864e+10 2772.3\n+ 선발        1 6.0820e+07 1.1580e+10 2772.6\n+ ERA         1 4.1760e+07 1.1599e+10 2772.8\n+ 삼진.9      1 3.6788e+07 1.1604e+10 2772.9\n+ 패          1 3.1923e+07 1.1609e+10 2773.0\n+ 블론        1 2.3680e+07 1.1618e+10 2773.1\n+ FIP         1 2.0239e+07 1.1621e+10 2773.1\n+ 홈런.9      1 1.4166e+07 1.1627e+10 2773.2\n+ LOB.        1 7.7349e+06 1.1633e+10 2773.3\n+ 경기        1 1.5351e+06 1.1640e+10 2773.4\n+ RA9.WAR     1 1.4350e+06 1.1640e+10 2773.4\n+ 볼넷.9      1 1.4095e+06 1.1640e+10 2773.4\n+ 홀드        1 2.5525e+05 1.1641e+10 2773.4\n+ 세          1 6.8181e+04 1.1641e+10 2773.4\n- 승          1 4.0011e+08 1.2041e+10 2774.5\n- 이닝        1 5.5021e+08 1.2191e+10 2776.4\n- WAR         1 3.9604e+09 1.5602e+10 2813.9\n- 연봉.2017.  1 3.8795e+10 5.0436e+10 2992.2\n\n\n\nsummary\n\nsummary(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + \n    이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48717  -2879    204   3083  48961 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.691e+03  2.658e+03  -1.012  0.31310    \n연봉.2017.   8.862e-01  4.018e-02  22.058  &lt; 2e-16 ***\nWAR          8.118e+03  1.152e+03   7.048 6.68e-11 ***\nkFIP         6.737e+02  4.029e+02   1.672  0.09666 .  \n승           1.059e+03  4.727e+02   2.240  0.02659 *  \n이닝        -9.701e+01  3.693e+01  -2.627  0.00954 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8929 on 146 degrees of freedom\nMultiple R-squared:  0.9195,    Adjusted R-squared:  0.9167 \nF-statistic: 333.4 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\nAIC를 이용하면 최종 모형은 “연봉.2018. ~ 연봉.2017. + WAR + kFIP+승+이닝” 이다.\n\n\nvif(model3)\n\n연봉.2017.1.91752518192932WAR4.3927072113068kFIP1.20722030237251승6.3165168650018이닝6.53436057823665\n\n\n연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + 이닝\n\n\n\n후진\n\nmodel_back = step(model1, direction = \"backward\")\nsummary(model_back)\n\nStart:  AIC=2775.03\n연봉.2018. ~ +WAR + 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                     1.2403e+10 2775.0\n- WAR         1 7.0421e+09 1.9445e+10 2841.4\n- 연봉.2017.  1 4.1619e+10 5.4022e+10 2996.7\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ +WAR + 연봉.2017., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-50442  -1849    758   2050  56166 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -576.58811  889.09610  -0.649    0.518    \nWAR         7007.17364  761.83979   9.198 3.03e-16 ***\n연봉.2017.     0.89926    0.04022  22.360  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9124 on 149 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.913 \nF-statistic: 793.8 on 2 and 149 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model_back)\n\nWAR1.84059685810784연봉.2017.1.84059685810784\n\n\n연봉.2018. ~ 승 + 경기 + 선발 + WAR + 연봉.2017.\n\n\n전진\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n\n\nmodel_forward = step(\n m0,\n scope = 연봉.2018. ~연봉.2017.+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"forward\")\nsummary(model_forward)\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n             Df  Sum of Sq        RSS    AIC\n+ 연봉.2017.  1 1.2511e+11 1.9445e+10 2841.4\n+ WAR         1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR     1 7.9230e+10 6.5326e+10 3025.6\n+ 승          1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝        1 6.2759e+10 8.1797e+10 3059.8\n+ 선발        1 4.5409e+10 9.9147e+10 3089.0\n+ 패          1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9      1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP        1 1.2591e+10 1.3197e+11 3132.4\n+ FIP         1 1.1403e+10 1.3315e+11 3133.8\n+ ERA         1 6.7332e+09 1.3782e+11 3139.1\n+ 세          1 6.4461e+09 1.3811e+11 3139.4\n+ 경기        1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.        1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9      1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                     1.4456e+11 3144.3\n+ 삼진.9      1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP       1 1.5139e+09 1.4304e+11 3144.7\n+ 블론        1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드        1 4.3499e+07 1.4451e+11 3146.3\n\nStep:  AIC=2841.38\n연봉.2018. ~ 연봉.2017.\n\n          Df  Sum of Sq        RSS    AIC\n+ WAR      1 7042094427 1.2403e+10 2775.0\n+ RA9.WAR  1 4958914952 1.4486e+10 2798.6\n+ 승       1 3841387936 1.5604e+10 2809.9\n+ 이닝     1 2811807174 1.6633e+10 2819.6\n+ 선발     1 2131826098 1.7313e+10 2825.7\n+ 패       1  881138122 1.8564e+10 2836.3\n&lt;none&gt;                  1.9445e+10 2841.4\n+ 블론     1  220224080 1.9225e+10 2841.7\n+ 세       1  171052899 1.9274e+10 2842.0\n+ kFIP     1  162536872 1.9283e+10 2842.1\n+ FIP      1  154825743 1.9290e+10 2842.2\n+ ERA      1  107350094 1.9338e+10 2842.5\n+ LOB.     1   77049296 1.9368e+10 2842.8\n+ 홈런.9   1   73957140 1.9371e+10 2842.8\n+ 볼넷.9   1   64564811 1.9381e+10 2842.9\n+ BABIP    1   56938420 1.9388e+10 2842.9\n+ 홀드     1   38023685 1.9407e+10 2843.1\n+ 삼진.9   1    5508109 1.9440e+10 2843.3\n+ 경기     1      12651 1.9445e+10 2843.4\n\nStep:  AIC=2775.03\n연봉.2018. ~ 연봉.2017. + WAR\n\n          Df Sum of Sq        RSS    AIC\n+ 패       1 213356827 1.2190e+10 2774.4\n+ kFIP     1 187694356 1.2215e+10 2774.7\n+ 선발     1 171531569 1.2232e+10 2774.9\n+ FIP      1 168772833 1.2234e+10 2774.9\n+ 볼넷.9   1 164189202 1.2239e+10 2775.0\n&lt;none&gt;                 1.2403e+10 2775.0\n+ 이닝     1 147039192 1.2256e+10 2775.2\n+ 홈런.9   1  51612430 1.2351e+10 2776.4\n+ 삼진.9   1  48348966 1.2355e+10 2776.4\n+ 승       1  30075743 1.2373e+10 2776.7\n+ 경기     1  27245510 1.2376e+10 2776.7\n+ BABIP    1  24181791 1.2379e+10 2776.7\n+ ERA      1  17077047 1.2386e+10 2776.8\n+ 블론     1  11153112 1.2392e+10 2776.9\n+ RA9.WAR  1   6650871 1.2396e+10 2776.9\n+ 세       1   4332494 1.2399e+10 2777.0\n+ 홀드     1   3482363 1.2400e+10 2777.0\n+ LOB.     1    660176 1.2402e+10 2777.0\n\nStep:  AIC=2774.4\n연봉.2018. ~ 연봉.2017. + WAR + 패\n\n          Df Sum of Sq        RSS    AIC\n+ kFIP     1 197383620 1.1992e+10 2773.9\n+ 승       1 180715640 1.2009e+10 2774.1\n+ FIP      1 174958135 1.2015e+10 2774.2\n&lt;none&gt;                 1.2190e+10 2774.4\n+ 볼넷.9   1 103300993 1.2086e+10 2775.1\n+ 홈런.9   1  71014626 1.2119e+10 2775.5\n+ 삼진.9   1  66895356 1.2123e+10 2775.6\n+ 블론     1  42172679 1.2148e+10 2775.9\n+ BABIP    1  41953578 1.2148e+10 2775.9\n+ 선발     1  31473684 1.2158e+10 2776.0\n+ ERA      1  13441234 1.2176e+10 2776.2\n+ 이닝     1   5896647 1.2184e+10 2776.3\n+ 세       1   3470456 1.2186e+10 2776.3\n+ RA9.WAR  1   2414340 1.2187e+10 2776.4\n+ LOB.     1   1712854 1.2188e+10 2776.4\n+ 경기     1   1125166 1.2189e+10 2776.4\n+ 홀드     1    189917 1.2190e+10 2776.4\n\nStep:  AIC=2773.92\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP\n\n          Df Sum of Sq        RSS    AIC\n+ 승       1 167413120 1.1825e+10 2773.8\n&lt;none&gt;                 1.1992e+10 2773.9\n+ 블론     1 128359041 1.1864e+10 2774.3\n+ 선발     1 117641927 1.1875e+10 2774.4\n+ BABIP    1  75190355 1.1917e+10 2775.0\n+ ERA      1  21818455 1.1971e+10 2775.6\n+ 홀드     1  21403854 1.1971e+10 2775.6\n+ 삼진.9   1  19275489 1.1973e+10 2775.7\n+ 경기     1  17028183 1.1975e+10 2775.7\n+ 이닝     1  13040981 1.1979e+10 2775.8\n+ FIP      1   9361041 1.1983e+10 2775.8\n+ 볼넷.9   1   8843181 1.1983e+10 2775.8\n+ 홈런.9   1   8722328 1.1984e+10 2775.8\n+ LOB.     1   4031616 1.1988e+10 2775.9\n+ RA9.WAR  1   2013063 1.1990e+10 2775.9\n+ 세       1   1445393 1.1991e+10 2775.9\n\nStep:  AIC=2773.78\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승\n\n          Df Sum of Sq        RSS    AIC\n+ 이닝     1 215650124 1.1609e+10 2773.0\n+ 선발     1 196677432 1.1628e+10 2773.2\n&lt;none&gt;                 1.1825e+10 2773.8\n+ 블론     1  83011867 1.1742e+10 2774.7\n+ RA9.WAR  1  63182313 1.1762e+10 2775.0\n+ BABIP    1  45874866 1.1779e+10 2775.2\n+ 볼넷.9   1  17920561 1.1807e+10 2775.6\n+ 삼진.9   1  14563944 1.1810e+10 2775.6\n+ 홈런.9   1  12160231 1.1813e+10 2775.6\n+ ERA      1   8802604 1.1816e+10 2775.7\n+ FIP      1   8122100 1.1817e+10 2775.7\n+ 세       1   5821352 1.1819e+10 2775.7\n+ 홀드     1   5267064 1.1820e+10 2775.7\n+ LOB.     1    397579 1.1825e+10 2775.8\n+ 경기     1    331762 1.1825e+10 2775.8\n\nStep:  AIC=2772.98\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝\n\n          Df Sum of Sq        RSS    AIC\n&lt;none&gt;                 1.1609e+10 2773.0\n+ BABIP    1  87591503 1.1522e+10 2773.8\n+ 선발     1  50413708 1.1559e+10 2774.3\n+ 블론     1  39472232 1.1570e+10 2774.5\n+ 삼진.9   1  33863019 1.1575e+10 2774.5\n+ ERA      1  33524887 1.1576e+10 2774.5\n+ FIP      1  18310110 1.1591e+10 2774.7\n+ 홈런.9   1  12031455 1.1597e+10 2774.8\n+ RA9.WAR  1  10397930 1.1599e+10 2774.8\n+ LOB.     1   3136209 1.1606e+10 2774.9\n+ 경기     1   1950014 1.1607e+10 2775.0\n+ 볼넷.9   1   1288038 1.1608e+10 2775.0\n+ 세       1    227255 1.1609e+10 2775.0\n+ 홀드     1     93003 1.1609e+10 2775.0\n\n\n\nCall:\nlm(formula = 연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + \n    승 + 이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48378  -2526    133   2563  48361 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.653e+03  2.664e+03  -0.996   0.3209    \n연봉.2017.   8.856e-01  4.027e-02  21.992  &lt; 2e-16 ***\nWAR          8.003e+03  1.169e+03   6.848 1.97e-10 ***\n패          -2.708e+02  4.288e+02  -0.631   0.5287    \nkFIP         6.622e+02  4.042e+02   1.639   0.1035    \n승           1.025e+03  4.767e+02   2.150   0.0332 *  \n이닝        -7.811e+01  4.760e+01  -1.641   0.1029    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8948 on 145 degrees of freedom\nMultiple R-squared:  0.9197,    Adjusted R-squared:  0.9164 \nF-statistic: 276.8 on 6 and 145 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model_forward)\n\n연봉.2017.1.9185269678088WAR4.50275401281816패3.39937428417761kFIP1.20965408072733승6.3982769107728이닝10.8086888355271\n\n\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#pca주성분분석",
    "href": "posts/AS4_5-Copy1.html#pca주성분분석",
    "title": "AS HW4_5(2)",
    "section": "PCA(주성분분석)",
    "text": "PCA(주성분분석)\n\n서로 상관성이 높은 변수들의 선형 결합으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법\n\n\ndt2 &lt;- dt[,1:19] # 설명변수\n\n\ndt3 &lt;- dt[,20] # 종속변수\n\n\nprocomp.result2 &lt;- prcomp(dt2, center=T, scale=T)\nsummary(procomp.result2)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.6109 1.8528 1.5587 1.27735 1.07673 0.94635 0.77437\nProportion of Variance 0.3588 0.1807 0.1279 0.08588 0.06102 0.04714 0.03156\nCumulative Proportion  0.3588 0.5395 0.6673 0.75322 0.81424 0.86137 0.89293\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     0.75305 0.60013 0.57118 0.51280 0.43707 0.31874 0.28648\nProportion of Variance 0.02985 0.01896 0.01717 0.01384 0.01005 0.00535 0.00432\nCumulative Proportion  0.92278 0.94173 0.95890 0.97274 0.98280 0.98815 0.99247\n                          PC15    PC16   PC17    PC18     PC19\nStandard deviation     0.27007 0.21161 0.1236 0.10061 0.006742\nProportion of Variance 0.00384 0.00236 0.0008 0.00053 0.000000\nCumulative Proportion  0.99630 0.99866 0.9995 1.00000 1.000000\n\n\n\ndt.pca &lt;- princomp(dt2, cor=TRUE)\n\n\nsummary(dt.pca)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3    Comp.4     Comp.5\nStandard deviation     2.6109027 1.8528422 1.5587351 1.2773530 1.07672785\nProportion of Variance 0.3587796 0.1806855 0.1278766 0.0858753 0.06101805\nCumulative Proportion  0.3587796 0.5394651 0.6673417 0.7532170 0.81423504\n                           Comp.6     Comp.7     Comp.8     Comp.9    Comp.10\nStandard deviation     0.94635475 0.77436931 0.75305028 0.60012648 0.57117946\nProportion of Variance 0.04713617 0.03156041 0.02984656 0.01895536 0.01717084\nCumulative Proportion  0.86137122 0.89293163 0.92277819 0.94173355 0.95890439\n                          Comp.11   Comp.12    Comp.13     Comp.14     Comp.15\nStandard deviation     0.51280325 0.4370697 0.31874103 0.286476368 0.270069813\nProportion of Variance 0.01384038 0.0100542 0.00534715 0.004319406 0.003838827\nCumulative Proportion  0.97274477 0.9827990 0.98814612 0.992465529 0.996304355\n                           Comp.16      Comp.17      Comp.18      Comp.19\nStandard deviation     0.211606675 0.1235839263 0.1006053051 6.741848e-03\nProportion of Variance 0.002356704 0.0008038414 0.0005327067 2.392237e-06\nCumulative Proportion  0.998661060 0.9994649011 0.9999976078 1.000000e+00\n\n\n\n제 1주성분과 제6주성분까지의 누적 분산비율은 대략 85.71%로 6개의 주성분 변수를 활용해 전체 데이터의 85.71%를 설명할 수 있다.\n\n\nscreeplot(dt.pca, npcs=8, type=\"lines\")\n\n\n\n\n\n주성분들에 의해 설명되는 변동 비율\n\n\nloadings(dt.pca)\n\n\nLoadings:\n           Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\n승          0.322  0.201                              0.105              \n패          0.272  0.198         0.108  0.236 -0.108        -0.477 -0.294\n세                -0.202  0.289  0.126 -0.512 -0.426        -0.192  0.360\n홀드              -0.253  0.336  0.141  0.417  0.343  0.154  0.296       \n블론              -0.274  0.396  0.204        -0.208  0.116 -0.183 -0.354\n경기        0.191 -0.220  0.372  0.222  0.227                       0.236\n선발        0.261  0.350 -0.123                             -0.250 -0.105\n이닝        0.329  0.235                0.109               -0.134       \n삼진.9                    0.393 -0.404 -0.229  0.455 -0.171 -0.204 -0.211\n볼넷.9     -0.250  0.116               -0.236  0.289  0.752 -0.178       \n홈런.9     -0.173  0.286  0.276  0.268         0.154 -0.535              \nBABIP      -0.142  0.207  0.292 -0.444                              0.146\nLOB.        0.131 -0.209 -0.230  0.228 -0.416  0.495 -0.174              \nERA        -0.235  0.294  0.288 -0.156        -0.133  0.106         0.107\nRA9.WAR     0.327  0.176               -0.181  0.124         0.119  0.391\nFIP        -0.259  0.284         0.378                                   \nkFIP       -0.257  0.268         0.423                                   \nWAR         0.318  0.213               -0.126         0.113  0.165  0.235\n연봉.2017.  0.252  0.130  0.103        -0.296 -0.181         0.629 -0.535\n           Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\n승                  0.383   0.237   0.542   0.550   0.102                 \n패                 -0.478  -0.137           0.321  -0.270  -0.229         \n세         -0.134  -0.328           0.242           0.206   0.105         \n홀드        0.128  -0.339  -0.335   0.264  -0.113   0.240                 \n블론        0.363   0.513  -0.251  -0.150  -0.121                         \n경기       -0.183           0.560  -0.322          -0.240                 \n선발                                0.154  -0.463   0.328   0.249         \n이닝                        0.213          -0.370           0.151         \n삼진.9     -0.380                                                   0.376 \n볼넷.9     -0.105           0.128                                  -0.359 \n홈런.9                                                             -0.614 \nBABIP       0.665  -0.171   0.296  -0.149           0.187                 \nLOB.        0.414  -0.155           0.112          -0.328   0.216         \nERA                        -0.141   0.336  -0.206  -0.663   0.296         \nRA9.WAR             0.123  -0.243          -0.247  -0.163  -0.683         \nFIP                                                                 0.323 \nkFIP                                                                0.492 \nWAR                        -0.398  -0.499   0.305           0.473         \n연봉.2017.         -0.229   0.174                                         \n           Comp.18 Comp.19\n승                        \n패                        \n세                        \n홀드                      \n블론                      \n경기        0.318         \n선발        0.553         \n이닝       -0.758         \n삼진.9                    \n볼넷.9                    \n홈런.9             -0.125 \nBABIP                     \nLOB.                      \nERA                       \nRA9.WAR                   \nFIP                 0.754 \nkFIP               -0.641 \nWAR                       \n연봉.2017.                \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nSS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000\nProportion Var  0.053  0.053  0.053  0.053  0.053  0.053  0.053  0.053  0.053\nCumulative Var  0.053  0.105  0.158  0.211  0.263  0.316  0.368  0.421  0.474\n               Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\nSS loadings      1.000   1.000   1.000   1.000   1.000   1.000   1.000   1.000\nProportion Var   0.053   0.053   0.053   0.053   0.053   0.053   0.053   0.053\nCumulative Var   0.526   0.579   0.632   0.684   0.737   0.789   0.842   0.895\n               Comp.18 Comp.19\nSS loadings      1.000   1.000\nProportion Var   0.053   0.053\nCumulative Var   0.947   1.000"
  },
  {
    "objectID": "posts/AS4_5-Copy1.html#glm",
    "href": "posts/AS4_5-Copy1.html#glm",
    "title": "AS HW4_5(2)",
    "section": "glm",
    "text": "glm\n\n\nX &lt;- model.matrix(연봉.2018.~., dt)[,-1] \ny &lt;- dt$연봉.2018.\n\n\nhead(X)\n\n\nA matrix: 6 × 19 of type dbl\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2017.\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n85000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n50000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n150000\n\n\n4\n10\n7\n0\n0\n0\n28\n28\n175.2\n8.04\n1.95\n1.02\n0.298\n75.0\n3.43\n6.11\n4.20\n4.03\n4.63\n100000\n\n\n5\n13\n7\n0\n0\n0\n30\n30\n187.1\n7.49\n2.11\n0.91\n0.323\n74.1\n3.80\n6.13\n4.36\n4.31\n4.38\n85000\n\n\n6\n8\n10\n0\n0\n0\n26\n26\n160.0\n7.42\n1.74\n1.12\n0.289\n76.1\n3.04\n6.52\n4.42\n4.32\n3.94\n35000\n\n\n\n\n\n\nridge.fit&lt;-glmnet(X,y,alpha=0, lambda=seq(0,100,10)) ##ridge : alpha=0 \nplot(ridge.fit, label=TRUE)\nabline(h=0, col=\"grey\", lty=2)\n\n\n\n\n\nsummary(ridge.fit)\n\n          Length Class     Mode   \na0         11    -none-    numeric\nbeta      209    dgCMatrix S4     \ndf         11    -none-    numeric\ndim         2    -none-    numeric\nlambda     11    -none-    numeric\ndev.ratio  11    -none-    numeric\nnulldev     1    -none-    numeric\nnpasses     1    -none-    numeric\njerr        1    -none-    numeric\noffset      1    -none-    logical\ncall        5    -none-    call   \nnobs        1    -none-    numeric\n\n\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=length(y))\n\nWarning message:\n“Option grouped=FALSE enforced in cv.glmnet, since &lt; 3 observations per fold”\n\n\n\ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = length(y), alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index   Measure       SE Nonzero\nmin   2869   100 117171048 49876335      19\n1se  12711    84 166795504 78214517      19\n\n\n\nplot(cv.fit)"
  },
  {
    "objectID": "posts/07. 변수선택 실습.html",
    "href": "posts/07. 변수선택 실습.html",
    "title": "07. 변수선택 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/07. 변수선택 실습.html#후진제거법",
    "href": "posts/07. 변수선택 실습.html#후진제거법",
    "title": "07. 변수선택 실습",
    "section": "후진제거법",
    "text": "후진제거법\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n\n부분 F통계량, F통계량 = t^2 (t검정)\n\\(H_0:\\beta_1= 0\\)\n위의 t value 제곱하면 부분 F통계량이 됨\nt-value의 절대값이 가장 작은 값을 찾아주면 됨 -&gt; x3\n\n\ndrop1(m, test = \"F\") #x3 제거\n# m에서 F통계량 사요하여 제거한것 확인\n\n\nA anova: 5 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n47.86364\n26.94429\nNA\nNA\n\n\nx1\n1\n25.9509114\n73.81455\n30.57588\n4.33747400\n0.07082169\n\n\nx2\n1\n2.9724782\n50.83612\n25.72755\n0.49682444\n0.50090110\n\n\nx3\n1\n0.1090900\n47.97273\n24.97388\n0.01823347\n0.89592269\n\n\nx4\n1\n0.2469747\n48.11061\n25.01120\n0.04127972\n0.84407147\n\n\n\n\n\n\nRSS: SSE\nF value : \\(F=\\dfrac{SSE_{RM}-SSE_{FM}}{SSE_{FM}/(n-p-1)}\\) F값이 작을수록 유의하지 않다. 클수록 유의하다.\n3번째값이 가장 작은 값을 갖다. 유의확률이 제일 크니까 변수가 제거된다.\n\n\nm1 &lt;- update(m, ~ . -x3)\nsummary(m1) #x4 제거\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n\\(lm(y\\)~\\(x_1+x_2+x_4, dt)\\) 라고 표현해도 된다.\nx1이 유의해졌다. (x3과 다중공산성이 심했었다)\n\n\ndrop1(m1, test = \"F\")\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n47.97273\n24.97388\nNA\nNA\n\n\nx1\n1\n820.907402\n868.88013\n60.62933\n154.007635\n5.780764e-07\n\n\nx2\n1\n26.789383\n74.76211\n28.74170\n5.025865\n5.168735e-02\n\n\nx4\n1\n9.931754\n57.90448\n25.41999\n1.863262\n2.053954e-01\n\n\n\n\n\n\nFvalue의 x4가 가장 작으므로 제거하자. 유의하지 않다.\n\n\nm2 &lt;- update(m1, ~ . -x4)\nsummary(m2) \n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.893 -1.574 -1.302  1.363  4.048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***\nx1           1.46831    0.12130   12.11 2.69e-07 ***\nx2           0.66225    0.04585   14.44 5.03e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.406 on 10 degrees of freedom\nMultiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 \nF-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09\n\n\n\ndrop1(m2, test = \"F\")\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n57.90448\n25.41999\nNA\nNA\n\n\nx1\n1\n848.4319\n906.33634\n59.17799\n146.5227\n2.692212e-07\n\n\nx2\n1\n1207.7823\n1265.68675\n63.51947\n208.5818\n5.028960e-08\n\n\n\n\n\n\nF value가 작은 x1을 제거할까? 하고 보니까 통계적으로 유의하므로 제거하지 않는다."
  },
  {
    "objectID": "posts/07. 변수선택 실습.html#전진선택법",
    "href": "posts/07. 변수선택 실습.html#전진선택법",
    "title": "07. 변수선택 실습",
    "section": "전진선택법",
    "text": "전진선택법\n\nStart model : \\(𝑦 = 𝛽_0 + \\epsilon\\)\n\n\nm0 = lm(y ~ 1, data = dt)\n\n\nadd1(m0,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x4추가\n\n\nA anova: 5 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n2715.7631\n71.44443\nNA\nNA\n\n\nx1\n1\n1450.0763\n1265.6867\n63.51947\n12.602518\n0.0045520446\n\n\nx2\n1\n1809.4267\n906.3363\n59.17799\n21.960605\n0.0006648249\n\n\nx3\n1\n776.3626\n1939.4005\n69.06740\n4.403417\n0.0597623242\n\n\nx4\n1\n1831.8962\n883.8669\n58.85164\n22.798520\n0.0005762318\n\n\n\n\n\n\nF값이 크면 좋음. x4가 좋으니까 보면 유의하므로 추가하자.\n\n\nm1 &lt;- update(m0, ~ . +x4)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.589  -8.228   1.495   4.726  17.524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 117.5679     5.2622  22.342 1.62e-10 ***\nx4           -0.7382     0.1546  -4.775 0.000576 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.964 on 11 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.645 \nF-statistic:  22.8 on 1 and 11 DF,  p-value: 0.0005762\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x1추가\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n883.86692\n58.85164\nNA\nNA\n\n\nx1\n1\n809.10480\n74.76211\n28.74170\n108.2239093\n1.105281e-06\n\n\nx2\n1\n14.98679\n868.88013\n60.62933\n0.1724839\n6.866842e-01\n\n\nx3\n1\n708.12891\n175.73800\n39.85258\n40.2945802\n8.375467e-05\n\n\n\n\n\n\nx1의 F갑싱 제일 크고 유의하므로 추가\n\n\nm2 &lt;- update(m1, ~ . +x1)\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x4 + x1, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0234 -1.4737  0.1371  1.7305  3.7701 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.09738    2.12398   48.54 3.32e-13 ***\nx4           -0.61395    0.04864  -12.62 1.81e-07 ***\nx1            1.43996    0.13842   10.40 1.11e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.734 on 10 degrees of freedom\nMultiple R-squared:  0.9725,    Adjusted R-squared:  0.967 \nF-statistic: 176.6 on 2 and 10 DF,  p-value: 1.581e-08\n\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## stop\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n74.76211\n28.74170\nNA\nNA\n\n\nx2\n1\n26.78938\n47.97273\n24.97388\n5.025865\n0.05168735\n\n\nx3\n1\n23.92599\n50.83612\n25.72755\n4.235846\n0.06969226\n\n\n\n\n\n\nx2의 F값이 더 크지만, pr값이 애매해. 유의수준 a=0.05면 유의하지 않다. 모형에 포함될 수 없으므로 멈춘다.\n최종 모형은 x1과 x4를 선택한 모형"
  },
  {
    "objectID": "posts/07. 변수선택 실습.html#단계적선택법",
    "href": "posts/07. 변수선택 실습.html#단계적선택법",
    "title": "07. 변수선택 실습",
    "section": "단계적선택법",
    "text": "단계적선택법\n\nm0 = lm(y ~ 1, data = dt)\n# 절편먼저 시작\n\n\nadd1(m0,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x4추가\n\n\nA anova: 5 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n2715.7631\n71.44443\nNA\nNA\n\n\nx1\n1\n1450.0763\n1265.6867\n63.51947\n12.602518\n0.0045520446\n\n\nx2\n1\n1809.4267\n906.3363\n59.17799\n21.960605\n0.0006648249\n\n\nx3\n1\n776.3626\n1939.4005\n69.06740\n4.403417\n0.0597623242\n\n\nx4\n1\n1831.8962\n883.8669\n58.85164\n22.798520\n0.0005762318\n\n\n\n\n\n\nm1 &lt;- update(m0, ~ . +x4)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.589  -8.228   1.495   4.726  17.524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 117.5679     5.2622  22.342 1.62e-10 ***\nx4           -0.7382     0.1546  -4.775 0.000576 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.964 on 11 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.645 \nF-statistic:  22.8 on 1 and 11 DF,  p-value: 0.0005762\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x1추가\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n883.86692\n58.85164\nNA\nNA\n\n\nx1\n1\n809.10480\n74.76211\n28.74170\n108.2239093\n1.105281e-06\n\n\nx2\n1\n14.98679\n868.88013\n60.62933\n0.1724839\n6.866842e-01\n\n\nx3\n1\n708.12891\n175.73800\n39.85258\n40.2945802\n8.375467e-05\n\n\n\n\n\n\nm2 &lt;- update(m1, ~ . +x1)\n\n\ndrop1(m2, test = \"F\") #제거 없음\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n74.76211\n28.74170\nNA\nNA\n\n\nx4\n1\n1190.9246\n1265.68675\n63.51947\n159.2952\n1.814890e-07\n\n\nx1\n1\n809.1048\n883.86692\n58.85164\n108.2239\n1.105281e-06\n\n\n\n\n\n\nx1,x4 유의하니까 그대로 가져가자\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x2추가\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n74.76211\n28.74170\nNA\nNA\n\n\nx2\n1\n26.78938\n47.97273\n24.97388\n5.025865\n0.05168735\n\n\nx3\n1\n23.92599\n50.83612\n25.72755\n4.235846\n0.06969226\n\n\n\n\n\n\n유의수준 a=0.1로 보자\n\n\nm3 &lt;- update(m2, ~ . +x2)\nsummary(m3) \n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\ndrop1(m3, test=\"F\") #x4 제거\n\n\nA anova: 4 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n47.97273\n24.97388\nNA\nNA\n\n\nx4\n1\n9.931754\n57.90448\n25.41999\n1.863262\n2.053954e-01\n\n\nx1\n1\n820.907402\n868.88013\n60.62933\n154.007635\n5.780764e-07\n\n\nx2\n1\n26.789383\n74.76211\n28.74170\n5.025865\n5.168735e-02\n\n\n\n\n\n\nx2를 추가했기 때문에 x2는 보지 않고 x1과 x4만 보면 된다\n\n\nm4 &lt;- update(m3, ~ . -x4)\nsummary(m4)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.893 -1.574 -1.302  1.363  4.048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***\nx1           1.46831    0.12130   12.11 2.69e-07 ***\nx2           0.66225    0.04585   14.44 5.03e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.406 on 10 degrees of freedom\nMultiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 \nF-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09\n\n\n\nadd1(m4,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") #stop\n\n\nA anova: 3 × 6\n\n\n\nDf\nSum of Sq\nRSS\nAIC\nF value\nPr(&gt;F)\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n&lt;none&gt;\nNA\nNA\n57.90448\n25.41999\nNA\nNA\n\n\nx3\n1\n9.793869\n48.11061\n25.01120\n1.832128\n0.2088895\n\n\nx4\n1\n9.931754\n47.97273\n24.97388\n1.863262\n0.2053954"
  },
  {
    "objectID": "posts/07. 변수선택 실습.html#aic를-이용한-변수-선택법",
    "href": "posts/07. 변수선택 실습.html#aic를-이용한-변수-선택법",
    "title": "07. 변수선택 실습",
    "section": "AIC를 이용한 변수 선택법",
    "text": "AIC를 이용한 변수 선택법\n\n모델 선택시 AIC가 작은 모델을 선택\n\n\nBackward - AIC\n\n후진제거법 AIC로 해보기\nstep 함수: AIC가 기준임\n\n\nmodel_back = step(m, direction = \"backward\")\nsummary(model_back)\n\nStart:  AIC=26.94\ny ~ x1 + x2 + x3 + x4\n\n       Df Sum of Sq    RSS    AIC\n- x3    1    0.1091 47.973 24.974\n- x4    1    0.2470 48.111 25.011\n- x2    1    2.9725 50.836 25.728\n&lt;none&gt;              47.864 26.944\n- x1    1   25.9509 73.815 30.576\n\nStep:  AIC=24.97\ny ~ x1 + x2 + x4\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;               47.97 24.974\n- x4    1      9.93  57.90 25.420\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n     Df Sum of Sq    RSS    AIC\n- x3    1    0.1091 47.973 24.974\n- x4    1    0.2470 48.111 25.011\n- x2    1    2.9725 50.836 25.728\n&lt;none&gt;              47.864 26.944\n- x1    1   25.9509 73.815 30.576\n-x3을 뺐을때 aic.. -x4을 뺏을때 aic…\n\nAIC가 작으면 작을수록 좋다.그래서 x3빼자\nx2와 x4가 애매하긴 하지만,,\n\n\n\nForward - AIC\n\n전진선택법\n\n\nmodel_forward = step(\n m0,\n scope = y ~ x1 + x2 + x3+ x4,\n direction = \"forward\")\nsummary(model_forward)\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n&lt;none&gt;              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq    RSS    AIC\n+ x1    1    809.10  74.76 28.742\n+ x3    1    708.13 175.74 39.853\n&lt;none&gt;              883.87 58.852\n+ x2    1     14.99 868.88 60.629\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq    RSS    AIC\n+ x2    1    26.789 47.973 24.974\n+ x3    1    23.926 50.836 25.728\n&lt;none&gt;              74.762 28.742\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;              47.973 24.974\n+ x3    1   0.10909 47.864 26.944\n\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n\nStep - AIC\n\nmodel_step = step(\n m0,\n scope = y ~ x1 + x2 + x3+ x4,\n direction = \"both\")\nsummary(model_step)\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n&lt;none&gt;              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq     RSS    AIC\n+ x1    1    809.10   74.76 28.742\n+ x3    1    708.13  175.74 39.853\n&lt;none&gt;               883.87 58.852\n+ x2    1     14.99  868.88 60.629\n- x4    1   1831.90 2715.76 71.444\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq     RSS    AIC\n+ x2    1     26.79   47.97 24.974\n+ x3    1     23.93   50.84 25.728\n&lt;none&gt;                74.76 28.742\n- x1    1    809.10  883.87 58.852\n- x4    1   1190.92 1265.69 63.519\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n&lt;none&gt;               47.97 24.974\n- x4    1      9.93  57.90 25.420\n+ x3    1      0.11  47.86 26.944\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n추가하는거랑 빼는거랑 동시에 진행"
  },
  {
    "objectID": "posts/07. 변수선택 실습.html#regsubsets",
    "href": "posts/07. 변수선택 실습.html#regsubsets",
    "title": "07. 변수선택 실습",
    "section": "regsubsets",
    "text": "regsubsets\n\nm_full &lt;-lm(y~.,dt)\nsummary(m_full)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n\nnbest=1\n\nfit &lt;- regsubsets(y~., data=dt, nbest=1, nvmax=9, method='exhaustive',)\n\n\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt, nbest = 1, nvmax = 9, method = \"exhaustive\", \n    )\n4 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\n1 subsets of each size up to 4\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4 \n1  ( 1 ) \" \" \" \" \" \" \"*\"\n2  ( 1 ) \"*\" \"*\" \" \" \" \"\n3  ( 1 ) \"*\" \"*\" \" \" \"*\"\n4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n\n\n설명변수 1개, 2개, 3개 , 4개썼을때 제일 좋은 모형을 불러와 :nbest=1\n\n\nwith(summary(fit), round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\n\nA matrix: 4 × 10 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nx4\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n0\n0\n0\n1\n883.867\n0.675\n0.645\n138.731\n-9.463\n\n\n2\n1\n1\n1\n0\n0\n57.904\n0.979\n0.974\n2.678\n-42.330\n\n\n3\n1\n1\n1\n0\n1\n47.973\n0.982\n0.976\n3.018\n-42.211\n\n\n4\n1\n1\n1\n1\n1\n47.864\n0.982\n0.974\n5.000\n-39.675\n\n\n\n\n\n\n\nnbest=2\n\nfit &lt;- regsubsets(y~., data=dt, nbest=2, nvmax=9, method='exhaustive',)\n\n\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt, nbest = 2, nvmax = 9, method = \"exhaustive\", \n    )\n4 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\n2 subsets of each size up to 4\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4 \n1  ( 1 ) \" \" \" \" \" \" \"*\"\n1  ( 2 ) \" \" \"*\" \" \" \" \"\n2  ( 1 ) \"*\" \"*\" \" \" \" \"\n2  ( 2 ) \"*\" \" \" \" \" \"*\"\n3  ( 1 ) \"*\" \"*\" \" \" \"*\"\n3  ( 2 ) \"*\" \"*\" \"*\" \" \"\n4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n\n\n좋았던걸 2개씩 리턴\n\n\n\nnbest=6\n\nfit &lt;- regsubsets(y~., data=dt, nbest=6, nvmax=9, method='exhaustive',)\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt, nbest = 6, nvmax = 9, method = \"exhaustive\", \n    )\n4 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\n6 subsets of each size up to 4\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4 \n1  ( 1 ) \" \" \" \" \" \" \"*\"\n1  ( 2 ) \" \" \"*\" \" \" \" \"\n1  ( 3 ) \"*\" \" \" \" \" \" \"\n1  ( 4 ) \" \" \" \" \"*\" \" \"\n2  ( 1 ) \"*\" \"*\" \" \" \" \"\n2  ( 2 ) \"*\" \" \" \" \" \"*\"\n2  ( 3 ) \" \" \" \" \"*\" \"*\"\n2  ( 4 ) \" \" \"*\" \"*\" \" \"\n2  ( 5 ) \" \" \"*\" \" \" \"*\"\n2  ( 6 ) \"*\" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \"*\" \" \" \"*\"\n3  ( 2 ) \"*\" \"*\" \"*\" \" \"\n3  ( 3 ) \"*\" \" \" \"*\" \"*\"\n3  ( 4 ) \" \" \"*\" \"*\" \"*\"\n4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n\n\nwith(summary(fit), round(cbind(which,rss,rsq,adjr2,cp,bic),3))\n\n\nA matrix: 15 × 10 of type dbl\n\n\n\n(Intercept)\nx1\nx2\nx3\nx4\nrss\nrsq\nadjr2\ncp\nbic\n\n\n\n\n1\n1\n0\n0\n0\n1\n883.867\n0.675\n0.645\n138.731\n-9.463\n\n\n1\n1\n0\n1\n0\n0\n906.336\n0.666\n0.636\n142.486\n-9.137\n\n\n1\n1\n1\n0\n0\n0\n1265.687\n0.534\n0.492\n202.549\n-4.795\n\n\n1\n1\n0\n0\n1\n0\n1939.400\n0.286\n0.221\n315.154\n0.753\n\n\n2\n1\n1\n1\n0\n0\n57.904\n0.979\n0.974\n2.678\n-42.330\n\n\n2\n1\n1\n0\n0\n1\n74.762\n0.972\n0.967\n5.496\n-39.008\n\n\n2\n1\n0\n0\n1\n1\n175.738\n0.935\n0.922\n22.373\n-27.897\n\n\n2\n1\n0\n1\n1\n0\n415.443\n0.847\n0.816\n62.438\n-16.712\n\n\n2\n1\n0\n1\n0\n1\n868.880\n0.680\n0.616\n138.226\n-7.120\n\n\n2\n1\n1\n0\n1\n0\n1227.072\n0.548\n0.458\n198.095\n-2.633\n\n\n3\n1\n1\n1\n0\n1\n47.973\n0.982\n0.976\n3.018\n-42.211\n\n\n3\n1\n1\n1\n1\n0\n48.111\n0.982\n0.976\n3.041\n-42.173\n\n\n3\n1\n1\n0\n1\n1\n50.836\n0.981\n0.975\n3.497\n-41.457\n\n\n3\n1\n0\n1\n1\n1\n73.815\n0.973\n0.964\n7.337\n-36.609\n\n\n4\n1\n1\n1\n1\n1\n47.864\n0.982\n0.974\n5.000\n-39.675\n\n\n\n\n\n\n\nnvmax=2\n\n설명변수 최대 2개까지만\n\n\nfit &lt;- regsubsets(y~., data=dt, nbest=6, nvmax=2, method='exhaustive',)\nsummary(fit)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = dt, nbest = 6, nvmax = 2, method = \"exhaustive\", \n    )\n4 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\n6 subsets of each size up to 2\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4 \n1  ( 1 ) \" \" \" \" \" \" \"*\"\n1  ( 2 ) \" \" \"*\" \" \" \" \"\n1  ( 3 ) \"*\" \" \" \" \" \" \"\n1  ( 4 ) \" \" \" \" \"*\" \" \"\n2  ( 1 ) \"*\" \"*\" \" \" \" \"\n2  ( 2 ) \"*\" \" \" \" \" \"*\"\n2  ( 3 ) \" \" \" \" \"*\" \"*\"\n2  ( 4 ) \" \" \"*\" \"*\" \" \"\n2  ( 5 ) \" \" \"*\" \" \" \"*\"\n2  ( 6 ) \"*\" \" \" \"*\" \" \""
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]